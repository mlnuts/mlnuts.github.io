<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<!--******************************************-->
<!--*  Authored with PreTeXt                 *-->
<!--*  pretextbook.org                       *-->
<!--*  Theme: default-modern                 *-->
<!--*  Palette:                              *-->
<!--******************************************-->
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Basic Probability for Machine Learning</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0">
<link href="_static/pretext/css/theme.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/ol-markers.css" rel="stylesheet" type="text/css">
<script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math"
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "_static/pretext/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="ML Notes">
<meta property="book:author" content="Samuel Ling">
<script src="_static/pretext/js/lib/jquery.min.js"></script><script src="_static/pretext/js/lib/jquery.sticky.js"></script><script src="_static/pretext/js/lib/jquery.espy.min.js"></script><script src="_static/pretext/js/pretext.js"></script><script src="_static/pretext/js/pretext_add_on.js?x=1"></script><script src="_static/pretext/js/user_preferences.js"></script><!--** eBookCongig is necessary to configure interactive       **-->
<!--** Runestone components to run locally in reader's browser **-->
<!--** No external communication:                              **-->
<!--**     log level is 0, Runestone Services are disabled     **-->
<script type="text/javascript">
eBookConfig = {};
eBookConfig.useRunestoneServices = false;
eBookConfig.host = 'http://127.0.0.1:8000';
eBookConfig.course = 'PTX_Course_Title_Here';
eBookConfig.basecourse = 'PTX_Base_Course';
eBookConfig.isLoggedIn = false;
eBookConfig.email = '';
eBookConfig.isInstructor = false;
eBookConfig.logLevel = 0;
eBookConfig.username = '';
eBookConfig.readings = null;
eBookConfig.activities = null;
eBookConfig.downloadsEnabled = false;
eBookConfig.allow_pairs = false;
eBookConfig.enableScratchAC = false;
eBookConfig.build_info = "";
eBookConfig.python3 = null;
eBookConfig.runestone_version = '7.9.8';
eBookConfig.jobehost = '';
eBookConfig.proxyuri_runs = '';
eBookConfig.proxyuri_files = '';
eBookConfig.enable_chatcodes =  false;
</script>
<!--*** Runestone Services ***-->
<script src="_static/prefix-runtime.b215b0da62d655bd.bundle.js"></script><script src="_static/prefix-723.3e6434f80549315a.bundle.js"></script><script src="_static/prefix-runestone.28cc3c4821792be5.bundle.js"></script><link rel="stylesheet" type="text/css" href="_static/prefix-723.3bccd435914aa0ff.css">
<link rel="stylesheet" type="text/css" href="_static/prefix-runestone.8141ae22fd347e48.css">
<script src="_static/pretext/js/lti_iframe_resizer.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-numbers/prism-line-numbers.min.js" integrity="sha512-dubtf8xMHSQlExGRQ5R7toxHLgSDZ0K7AunqPWHXmJQ8XyVIG19S1T95gBxlAeGOK02P4Da2RTnQz0Za0H0ebQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-highlight/prism-line-highlight.min.js" integrity="sha512-93uCmm0q+qO5Lb1huDqr7tywS8A2TFA+1/WHvyiWaK6/pvsFl6USnILagntBx8JnVbQH5s3n0vQZY6xNthNfKA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="_static/pretext/js/pretext_search.js"></script><script src="_static/pretext/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script>
</head>
<body id="ML-Notes" class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner"><div class="title-container">
<h1 class="heading"><a href="my-ML-Notes.html"><span class="title">ML Notes</span> <span class="subtitle">Theoretical and Practical ML Concepts</span></a></h1>
<p class="byline">Samuel Ling</p>
</div></div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><div class="ptx-navbar-contents">
<button class="toc-toggle button" title="Contents"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5d2;</span><span class="name">Contents</span></button><div class="searchbox">
<div class="searchwidget"><button id="searchbutton" class="searchbutton button" type="button" title="Search book"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe8b6;</span><span class="name">Search Book</span></button></div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<div class="search-results-controls">
<input aria-label="Search term" id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search term"><button title="Close search" id="closesearchresults" class="closesearchresults"><span class="material-symbols-outlined">close</span></button>
</div>
<h2 class="search-results-heading">Search Results: </h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div>
<span class="nav-other-controls"><button id="light-dark-button" class="light-dark-button button" title="Dark Mode"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe51c;</span><span class="name">Dark Mode</span></button></span><span class="treebuttons"><a class="previous-button button" href="sec-data-types-for-machine-learning.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="up-button button" href="ch-Essential-Probability-and-Statistics.html" title="Up"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Up</span></a><a class="next-button button" href="sec-Joint-Conditional-and-Marginal-Probabilities.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a></span>
</div></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\N}{\mathbb N} \newcommand{\Z}{\mathbb Z} \newcommand{\Q}{\mathbb Q} \newcommand{\R}{\mathbb R}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2 focused" data-preexpanded-levels="0" data-max-levels="2"><ul class="structural toc-item-list contains-active">
<li class="toc-item toc-frontmatter"><div class="toc-title-box"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div></li>
<li class="toc-item toc-chapter contains-active">
<div class="toc-title-box"><a href="ch-Essential-Probability-and-Statistics.html" class="internal"><span class="codenumber">1</span> <span class="title">Essential Probability and Statistics</span></a></div>
<ul class="structural toc-item-list contains-active">
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Descriptive-Statistics.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Descriptive Statistics</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Central-Tendency" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Measures of Central Tendency</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Dispersion" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Measures of Dispersion</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Distribution-Shape" class="internal"><span class="codenumber">1.1.3</span> <span class="title">Distribution Shape</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Numerical-Summary" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Numerical Summary</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Useful Tools for Descriptive Statistics and Exploratory Data Analysis</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-NumPy" class="internal"><span class="codenumber">1.2.1</span> <span class="title">NumPy: Foundation for Numerical Operations</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-Pandas" class="internal"><span class="codenumber">1.2.2</span> <span class="title">Pandas: Data Manipulation and Analysis</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-Visualization" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Visualization with Matplotlib and Seaborn</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-data-types-for-machine-learning.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Data Types for Machine Learning</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-data-types-for-machine-learning.html#subsec-structured-vs-unstructured" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Structured vs. Unstructured Data</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-data-types-for-machine-learning.html#subsec-sequence-vs-non-sequence" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Sequence vs. Non-Sequence Data</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-data-types-for-machine-learning.html#subsec-numerical-categorical-ordinal" class="internal"><span class="codenumber">1.3.3</span> <span class="title">Numerical, Categorical, and Ordinal Data</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-data-types-for-machine-learning.html#subsec-unstructured-text" class="internal"><span class="codenumber">1.3.4</span> <span class="title">Handling Unstructured Text Data</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section contains-active">
<div class="toc-title-box"><a href="sec-Basic-Probability.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Basic Probability for Machine Learning</span></a></div>
<ul class="structural toc-item-list active">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-Axiomatic-View-of-Probability" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Axiomatic View of Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-sum-product-rules" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Sum and Product Rules for Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-conditional-probability-independence" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Conditional Probability and Independence</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-probability-distributions" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Probability Distributions</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-three-types-of-probabilities" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Three Types of Probabilities</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Joint, Conditional and Marginal Probabilities</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Random-Variables" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Random Variables, Probabilities, and Expections</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Joint-Probability" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Joint ad Marginal Probabilities</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsubsec-Covariance-and-Correlation" class="internal"><span class="codenumber">1.5.2.1</span> <span class="title">Covariance and Correlation</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Conditional-Probability" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Conditional Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-conditional-probability-from-joint-probability" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Bayes’ Rule</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Independent-Variables" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Independent Variables</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Complications-For-Continuous-Variables" class="internal"><span class="codenumber">1.5.6</span> <span class="title">Complications-For-Continuous-Variables</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsubsec-Probability-Density" class="internal"><span class="codenumber">1.5.6.1</span> <span class="title">Probability Density</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsubsec-Cumulative-Distribution-Function" class="internal"><span class="codenumber">1.5.6.2</span> <span class="title">Cumulative Distribution Function (CDF)</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html" class="internal"><span class="codenumber">1.6</span> <span class="title">Example Discrete Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Bernoulli-Distribution" class="internal"><span class="codenumber">1.6.1</span> <span class="title">Bernoulli Distribution</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Binomial-Distribution" class="internal"><span class="codenumber">1.6.2</span> <span class="title">Binomial Distribution</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Poisson-Distribution" class="internal"><span class="codenumber">1.6.3</span> <span class="title">Poisson Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsub-Poisson-Process" class="internal"><span class="codenumber">1.6.3.1</span> <span class="title">Poisson Process</span></a></div></li></ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html" class="internal"><span class="codenumber">1.7</span> <span class="title">Example Continuous Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Uniform-Distribution" class="internal"><span class="codenumber">1.7.1</span> <span class="title">Uniform Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsubsec-Inverse-Uniform-CDF" class="internal"><span class="codenumber">1.7.1.1</span> <span class="title">Inverse Uniform CDF</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Normal-Distribution" class="internal"><span class="codenumber">1.7.2</span> <span class="title">Normal (Gaussian) Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsubsec-Inverse-CDF-and-Sampling" class="internal"><span class="codenumber">1.7.2.1</span> <span class="title">Inverse CDF and Sampling</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Exponential-Distribution" class="internal"><span class="codenumber">1.7.3</span> <span class="title">Exponential Distribution</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-LLN-and-CLT.html" class="internal"><span class="codenumber">1.8</span> <span class="title">Law of Large Numbers and Central Limit Theorem</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Law-of-Large-Numbers" class="internal"><span class="codenumber">1.8.1</span> <span class="title">Law of Large Numbers</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Central-Limit-Theorem" class="internal"><span class="codenumber">1.8.2</span> <span class="title">Central Limit Theorem</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-LLN-vs-CLT" class="internal"><span class="codenumber">1.8.3</span> <span class="title">LLN vs. CLT</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Berry-Esseen" class="internal"><span class="codenumber">1.8.4</span> <span class="title">Berry-Esseen Theorem</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Why-Large-n-Matters" class="internal"><span class="codenumber">1.8.5</span> <span class="title">Why LLN and CLT Matter</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Inferential-Statistics.html" class="internal"><span class="codenumber">1.9</span> <span class="title">Inferential Statistics</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Point-Estimation" class="internal"><span class="codenumber">1.9.1</span> <span class="title">Point Estimation</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Sampling-Distributions" class="internal"><span class="codenumber">1.9.2</span> <span class="title">Sampling Distributions</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Hypothesis-Testing" class="internal"><span class="codenumber">1.9.3</span> <span class="title">Hypothesis Testing</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Confidence-Intervals" class="internal"><span class="codenumber">1.9.4</span> <span class="title">Confidence Intervals</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Types-of-Errors" class="internal"><span class="codenumber">1.9.5</span> <span class="title">Types of Errors</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Power-Effect-Size" class="internal"><span class="codenumber">1.9.6</span> <span class="title">Statistical Power and Effect Size</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Multiple-Testing" class="internal"><span class="codenumber">1.9.7</span> <span class="title">Multiple Testing</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Other-Tests" class="internal"><span class="codenumber">1.9.8</span> <span class="title">Common Statistical Tests</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-backmatter"><div class="toc-title-box"><a href="backmatter.html" class="internal"><span class="title">Backmatter</span></a></div></li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content"><section class="section" id="sec-Basic-Probability"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">1.4</span><span class="space"> </span><span class="title">Basic Probability for Machine Learning</span>
</h2>
<section class="introduction" id="sec-Basic-Probability-2"><div class="para" id="sec-Basic-Probability-2-1">Probability is the backbone of machine learning, helping us model uncertainty in data, predictions, and outcomes. In machine learning, probability underpins tasks like classification (e.g., predicting labels), evaluating model confidence, and handling noisy data. This section introduces probability concepts such as sample spaces, events, and axioms—and connects them to practical machine learning applications using Python. We will use the student dataset from <a href="sec-data-types-for-machine-learning.html" class="internal" title="Section 1.3: Data Types for Machine Learning">Section 1.3</a> to illustrate ideas.<div class="autopermalink" data-description="Paragraph"><a href="#sec-Basic-Probability-2-1" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="sec-Basic-Probability-2-2">An <em class="alert">event</em> is a specific outcome or set of outcomes from an experiment, represented as a set. For a coin toss, "heads" is <span class="process-math">\(\{H\}\text{,}\)</span> "tails" is <span class="process-math">\(\{T\}\text{,}\)</span> and "heads or tails" is <span class="process-math">\(\{H, T\}\text{.}\)</span> Each trial answers whether an event occurred (yes/no). For a die roll yielding 2, events like <span class="process-math">\(\{2\}\)</span> or <span class="process-math">\(\{1,2,3\}\)</span> occur if they include 2. Sets allow combining events via union (<span class="process-math">\(\cup\)</span>) or intersection (<span class="process-math">\(\cap\)</span>), such as <span class="process-math">\(\{H\} \cup \{T\} = \{H, T\}\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#sec-Basic-Probability-2-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div></section><section class="subsection" id="subsec-Axiomatic-View-of-Probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.1</span><span class="space"> </span><span class="title">Axiomatic View of Probability</span>
</h3>
<div class="para" id="subsec-Axiomatic-View-of-Probability-2">In 1933, Andrey Kolmogorov formalized probability with three axioms, providing a mathematical framework. Think of these as rules that ensure probabilities make sense, like ensuring a weather forecast never predicts negative rain or more than 100% chance.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Axiomatic-View-of-Probability-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-Axiomatic-View-of-Probability-3">
<em class="alert">Sample Space <span class="process-math">\(\Omega\)</span></em>: The set of all possible outcomes. For a six-sided die, <span class="process-math">\(\Omega = \{1, 2, 3, 4, 5, 6\}\text{.}\)</span> For a student passing an exam, <span class="process-math">\(\Omega = \{\text{Pass}, \text{Fail}\}\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-Axiomatic-View-of-Probability-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-Axiomatic-View-of-Probability-4">
<em class="alert">Event Space <span class="process-math">\(F\)</span></em>: All possible subsets of <span class="process-math">\(\Omega\text{,}\)</span> including the empty set <span class="process-math">\(\varnothing\)</span> (impossible event) and <span class="process-math">\(\Omega\)</span> (event certain to happen). For a coin toss (<span class="process-math">\(\Omega = \{H, T\}\)</span>), <span class="process-math">\(F = \{\varnothing, \{H\}, \{T\}, \{H, T\}\}\text{.}\)</span> With <span class="process-math">\(N\)</span> outcomes, <span class="process-math">\(F\)</span> has <span class="process-math">\(2^N\)</span> events.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Axiomatic-View-of-Probability-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-Axiomatic-View-of-Probability-5">
<em class="alert">Probability Measure <span class="process-math">\(P\)</span></em>: Assigns a number <span class="process-math">\(P(E)\)</span> to each event <span class="process-math">\(E \in F\text{,}\)</span> representing its likelihood. For example, for a fair die, <span class="process-math">\(P(\{1\}) = 1/6\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-Axiomatic-View-of-Probability-5" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para logical" id="subsec-Axiomatic-View-of-Probability-6">
<div class="para">The probability space is the triplet <span class="process-math">\((\Omega, F, P)\text{.}\)</span> Kolmogorov’s axioms are:</div>
<ol class="decimal ol-marker-1" id="subsec-Axiomatic-View-of-Probability-6-2">
<li id="subsec-Axiomatic-View-of-Probability-6-2-1">
<div class="para" id="p-derived-subsec-Axiomatic-View-of-Probability-6-2-1">
<em class="alert">Non-negativity</em>:<div class="displaymath process-math" id="eqn-first-axiom-non-negativity">
\begin{equation}
P(E) \ge 0\tag{1.4.1}
\end{equation}
</div>for all <span class="process-math">\(E \in F\text{.}\)</span>
</div>
<div class="autopermalink" data-description="Item 1"><a href="#subsec-Axiomatic-View-of-Probability-6-2-1" title="Copy heading and permalink for Item 1" aria-label="Copy heading and permalink for Item 1">🔗</a></div>
</li>
<li id="subsec-Axiomatic-View-of-Probability-6-2-2">
<div class="para" id="p-derived-subsec-Axiomatic-View-of-Probability-6-2-2">
<em class="alert">Normalization</em>:<div class="displaymath process-math" id="eqn-second-axiom-normalization">
\begin{equation}
P(\Omega) = 1\text{,}\tag{1.4.2}
\end{equation}
</div>ensuring total certainty for event <span class="process-math">\(\Omega\text{.}\)</span>
</div>
<div class="autopermalink" data-description="Item 2"><a href="#subsec-Axiomatic-View-of-Probability-6-2-2" title="Copy heading and permalink for Item 2" aria-label="Copy heading and permalink for Item 2">🔗</a></div>
</li>
<li id="subsec-Axiomatic-View-of-Probability-6-2-3">
<div class="para" id="p-derived-subsec-Axiomatic-View-of-Probability-6-2-3">
<em class="alert">Additivity</em>: For disjoint events (<span class="process-math">\(E_1 \cap E_2 = \varnothing\)</span>),<div class="displaymath process-math" id="eqn-third-axiom-additivity">
\begin{equation}
P(E_1 \cup E_2) = P(E_1) + P(E_2)\text{.}\tag{1.4.3}
\end{equation}
</div>
</div>
<div class="autopermalink" data-description="Item 3"><a href="#subsec-Axiomatic-View-of-Probability-6-2-3" title="Copy heading and permalink for Item 3" aria-label="Copy heading and permalink for Item 3">🔗</a></div>
</li>
</ol>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Axiomatic-View-of-Probability-6" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para logical" id="subsec-Axiomatic-View-of-Probability-7">
<div class="para">Derived results:</div>
<div class="displaymath process-math" id="subsec-Axiomatic-View-of-Probability-7-1">
\begin{align}
\amp P(\varnothing) = 0. \tag{1.4.4}\\
\amp P(E^c) = 1 - P(E), \text{ where } E^c \text{ is the complement of } E. \tag{1.4.5}\\
\amp P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2). \tag{1.4.6}
\end{align}
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Axiomatic-View-of-Probability-7" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-Axiomatic-View-of-Probability-8">The union formula accounts for overlap, as shown in <a href="sec-Basic-Probability.html#fig-venn-diagram-E1-E2" class="xref" data-knowl="./knowl/xref/fig-venn-diagram-E1-E2.html" data-reveal-label="Reveal" data-close-label="Close" title="Figure 1.4.1">Figure 1.4.1</a>. For a die, if <span class="process-math">\(E_1 = \{1,3,5\}\)</span> (odd numbers), <span class="process-math">\(E_2 = \{3,5,6\}\text{,}\)</span> then <span class="process-math">\(P(E_1 \cup E_2) = P(\{1,3,5,6\}) = 4/6 = 2/3\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-Axiomatic-View-of-Probability-8" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<figure class="figure figure-like" id="fig-venn-diagram-E1-E2"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/venn-diagram-E1-E2.png" class="contained" alt="Venn diagram of intersecting events."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.1<span class="period">.</span></span><span class="space"> </span>Venn diagram illustrating events <span class="process-math">\(E_1 = \{1,3,5\}\)</span> (odd numbers) and <span class="process-math">\(E_2 = \{3,5,6\}\)</span> for 1,000 simulated rolls of a fair six-sided die. The left region (181) counts rolls of 1 (in <span class="process-math">\(E_1\)</span> only), the right region (326) counts rolls of 6 (in <span class="process-math">\(E_2\)</span> only), and the overlap (155) counts rolls of 3 or 5 (in <span class="process-math">\(E_1 \cap E_2\)</span>). These counts estimate probabilities, with theoretical values <span class="process-math">\(P(E_1) = 3/6\text{,}\)</span> <span class="process-math">\(P(E_2) = 3/6\text{,}\)</span> <span class="process-math">\(P(E_1 \cap E_2) = 2/6\text{,}\)</span> and <span class="process-math">\(P(E_1 \cup E_2) = 4/6\text{.}\)</span> Deviations (e.g., 326 vs. expected 167 for <span class="process-math">\(E_2\)</span> only) reflect random variation. This visualization supports the union formula <span class="process-math">\(P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)\text{,}\)</span> used in machine learning for feature probability calculations.<div class="autopermalink" data-description="Figure 1.4.1"><a href="#fig-venn-diagram-E1-E2" title="Copy heading and permalink for Figure 1.4.1" aria-label="Copy heading and permalink for Figure 1.4.1">🔗</a></div></figcaption></figure><div class="code-box"><pre class="program clipboardable line-numbers"><code class="language-py"># --- DIE ROLL VENN DIAGRAM ---
import numpy as np
from matplotlib_venn import venn2
import matplotlib.pyplot as plt

np.random.seed(42)
n_trials = 1000
rolls = np.random.randint(1, 7, n_trials)

# Events
e1 = np.isin(rolls, [1, 3, 5])  # Odd numbers
e2 = np.isin(rolls, [3, 5, 6])  # 3,5,6
e1_only = np.sum(e1 \amp; ~e2)
e2_only = np.sum(e2 \amp; ~e1)
both = np.sum(e1 \amp; e2)

# Venn diagram
plt.figure(figsize=(6, 4))
venn2(subsets=(e1_only, e2_only, both), set_labels=('E1 (Odd)', 'E2 (3,5,6)'))
plt.title('Venn Diagram of Die Roll Events')
plt.savefig('venn-diagram-E1-E2.png', dpi=300)
plt.show()

# Probabilities
p_e1 = np.mean(e1)
p_e2 = np.mean(e2)
p_inter = np.mean(e1 \amp; e2)
p_union = np.mean(e1 | e2)
print(f"P(E1): {p_e1:.3f}, P(E2): {p_e2:.3f}, P(E1 ∩ E2): {p_inter:.3f}, P(E1 ∪ E2): {p_union:.3f}")
# --- END CODE ---
</code></pre></div>
<div class="autopermalink" data-description="Subsection 1.4.1: Axiomatic View of Probability"><a href="#subsec-Axiomatic-View-of-Probability" title="Copy heading and permalink for Subsection 1.4.1: Axiomatic View of Probability" aria-label="Copy heading and permalink for Subsection 1.4.1: Axiomatic View of Probability">🔗</a></div></section><section class="subsection" id="subsec-sum-product-rules"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.2</span><span class="space"> </span><span class="title">Sum and Product Rules for Probability</span>
</h3>
<div class="para" id="subsec-sum-product-rules-2">The sum and product rules are foundational for computing probabilities of combined events, essential for machine learning tasks like feature engineering and Bayesian inference.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-sum-product-rules-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para logical" id="subsec-sum-product-rules-3">
<div class="para">
<em class="alert">Sum Rule</em>: The probability of the union of two events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> is given by:</div>
<div class="displaymath process-math" id="eqn-sum-rule">
\begin{equation}
P(A \cup B) = P(A) + P(B) - P(A \cap B).\tag{1.4.7}
\end{equation}
</div>
<div class="para">This accounts for overlapping events to avoid double-counting. In machine learning, the sum rule is used to compute marginal probabilities from joint distributions.</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-sum-product-rules-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para logical" id="subsec-sum-product-rules-4">
<div class="para">
<em class="alert">Product Rule</em>: The joint probability of two events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> is:</div>
<div class="displaymath process-math" id="eqn-product-rule">
\begin{equation}
P(A \cap B) = P(A|B)P(B),\tag{1.4.8}
\end{equation}
</div>
<div class="para">where <span class="process-math">\(P(A|B)\)</span> is the conditional probability of <span class="process-math">\(A\)</span> given <span class="process-math">\(B\text{.}\)</span> This rule is key for factoring joint probabilities in models like Naive Bayes.</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-sum-product-rules-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-sum-product-rules-5">Example: For a fair die, let <span class="process-math">\(A = \{1,2,3\}\)</span> (numbers ≤ 3) and <span class="process-math">\(B = \{3,4,5\}\text{.}\)</span> The sum rule gives <span class="process-math">\(P(A \cup B) = P(A) + P(B) - P(A \cap B) = 3/6 + 3/6 - 1/6 = 5/6\text{,}\)</span> as <span class="process-math">\(A \cap B = \{3\}\text{.}\)</span> For the product rule, consider the student dataset from <a href="sec-data-types-for-machine-learning.html" class="internal" title="Section 1.3: Data Types for Machine Learning">Section 1.3</a>: the probability of passing and studying &gt;20 hours is <span class="process-math">\(P(\text{Pass} \cap \text{High Study}) = P(\text{Pass} | \text{High Study})P(\text{High Study})\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-sum-product-rules-5" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="code-box"><pre class="program clipboardable line-numbers"><code class="language-py"># --- SUM AND PRODUCT RULES ---
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Student data
np.random.seed(42)
data = pd.DataFrame({
    'Hours_Studied': np.random.normal(20, 5, 100).clip(0, 40),
    'Passed': np.random.binomial(1, 0.7, 100)
})
data['High_Study'] = data['Hours_Studied'] &gt; 20

# Compute probabilities
p_pass = np.mean(data['Passed'])
p_high_study = np.mean(data['High_Study'])
p_pass_and_high = np.mean(data['Passed'] \amp; data['High_Study'])
p_pass_or_high = p_pass + p_high_study - p_pass_and_high  # Sum rule
p_pass_given_high = p_pass_and_high / p_high_study  # For product rule

print(f"P(Pass): {p_pass:.3f}, P(High Study): {p_high_study:.3f}")
print(f"P(Pass ∩ High Study): {p_pass_and_high:.3f}")
print(f"P(Pass ∪ High Study): {p_pass_or_high:.3f}")
print(f"Product Rule: P(Pass ∩ High Study) = P(Pass | High Study) * P(High Study) = {p_pass_given_high:.3f} * {p_high_study:.3f} = {p_pass_given_high * p_high_study:.3f}")

# Heatmap of joint probabilities
joint_table = pd.crosstab(data['Passed'], data['High_Study'], normalize='all')
sns.heatmap(joint_table, annot=True, cmap='Blues', fmt='.3f')
plt.xlabel('High Study Hours (&gt;20)')
plt.ylabel('Passed')
plt.title('Joint Probability of Pass and High Study Hours')
plt.savefig('joint-probability-heatmap.png', dpi=300)
plt.show()
# --- END CODE ---
</code></pre></div>
<figure class="figure figure-like" id="fig-joint-probability-heatmap"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/joint-probability-heatmap.png" class="contained" alt="Heatmap of joint probabilities for pass and study hours."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.2<span class="period">.</span></span><span class="space"> </span>Heatmap displaying the joint probability distribution of passing an exam and studying more than 20 hours for 100 students, based on synthetic data (Hours_Studied: normal, mean 20, std 5; Passed: Bernoulli, p=0.7). Each cell shows the probability <span class="process-math">\(P(\text{Pass}, \text{High Study})\text{,}\)</span> with darker shades indicating higher probabilities. Marginal probabilities <span class="process-math">\(P(\text{Pass})\)</span> and <span class="process-math">\(P(\text{High Study})\)</span> are derived by summing rows or columns, illustrating the sum rule. The product rule is verified as <span class="process-math">\(P(\text{Pass} \cap \text{High Study}) = P(\text{Pass} | \text{High Study})P(\text{High Study})\text{.}\)</span> This visualization is relevant to machine learning for estimating joint feature probabilities in classification models.<div class="autopermalink" data-description="Figure 1.4.2"><a href="#fig-joint-probability-heatmap" title="Copy heading and permalink for Figure 1.4.2" aria-label="Copy heading and permalink for Figure 1.4.2">🔗</a></div></figcaption></figure><div class="autopermalink" data-description="Subsection 1.4.2: Sum and Product Rules for Probability"><a href="#subsec-sum-product-rules" title="Copy heading and permalink for Subsection 1.4.2: Sum and Product Rules for Probability" aria-label="Copy heading and permalink for Subsection 1.4.2: Sum and Product Rules for Probability">🔗</a></div></section><section class="subsection" id="subsec-conditional-probability-independence"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.3</span><span class="space"> </span><span class="title">Conditional Probability and Independence</span>
</h3>
<div class="para" id="subsec-conditional-probability-independence-2">
<em class="alert">Conditional Probability</em>: The probability of an event <span class="process-math">\(A\)</span> given that <span class="process-math">\(B\)</span> has occurred, denoted <span class="process-math">\(P(A|B) = \frac{P(A \cap B)}{P(B)}\text{,}\)</span> where <span class="process-math">\(P(B) &gt; 0\text{.}\)</span> For example, the probability a student passes given they studied over 20 hours.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-independence-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-conditional-probability-independence-3">
<em class="alert">Independence</em>: Events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> are independent if <span class="process-math">\(P(A \cap B) = P(A)P(B)\text{,}\)</span> meaning one event doesn’t affect the other.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-independence-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-conditional-probability-independence-4">Example: Using the student dataset from <a href="sec-data-types-for-machine-learning.html" class="internal" title="Section 1.3: Data Types for Machine Learning">Section 1.3</a>, estimate the probability of passing given high study hours.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-independence-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="code-box"><pre class="program clipboardable line-numbers"><code class="language-py"># --- CONDITIONAL PROBABILITY ---
import pandas as pd
import numpy as np

# Student data
np.random.seed(42)
data = pd.DataFrame({
    'Hours_Studied': np.random.normal(20, 5, 100).clip(0, 40),
    'Passed': np.random.binomial(1, 0.7, 100)
})
data['High_Study'] = data['Hours_Studied'] &gt; 20

# Conditional probability
p_pass = np.mean(data['Passed'])
p_high_study = np.mean(data['High_Study'])
p_pass_and_high = np.mean(data['Passed'] \amp; data['High_Study'])
p_pass_given_high = p_pass_and_high / p_high_study
print(f"P(Pass | High Study): {p_pass_given_high:.3f}")

# Bar plot
counts = data.groupby(['High_Study', 'Passed']).size().unstack()
counts.plot(kind='bar', stacked=True)
plt.xlabel('High Study Hours (&gt;20)')
plt.ylabel('Count')
plt.title('Pass/Fail by Study Hours')
plt.legend(['Fail', 'Pass'])
plt.savefig('./images/essential-probability-and-statistics/conditional-bar.png', dpi=300)
plt.show()
# --- END CODE ---
</code></pre></div>
<figure class="figure figure-like" id="fig-conditional-bar"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/conditional-bar.png" class="contained" alt="Bar plot of conditional probability."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.3<span class="period">.</span></span><span class="space"> </span>Stacked bar plot showing the distribution of pass/fail outcomes for 100 students, based on whether they studied more than 20 hours (High_Study = True) or not. The dataset is synthetic, with Hours_Studied drawn from a normal distribution (mean 20, std 5) and Passed from a Bernoulli distribution (p=0.7). The plot illustrates conditional probability <span class="process-math">\(P(\text{Pass} | \text{High Study})\text{,}\)</span> showing a higher proportion of passes among students with high study hours. This visualization is relevant to machine learning for feature analysis in classification tasks, such as predicting student success based on study habits.<div class="autopermalink" data-description="Figure 1.4.3"><a href="#fig-conditional-bar" title="Copy heading and permalink for Figure 1.4.3" aria-label="Copy heading and permalink for Figure 1.4.3">🔗</a></div></figcaption></figure><div class="autopermalink" data-description="Subsection 1.4.3: Conditional Probability and Independence"><a href="#subsec-conditional-probability-independence" title="Copy heading and permalink for Subsection 1.4.3: Conditional Probability and Independence" aria-label="Copy heading and permalink for Subsection 1.4.3: Conditional Probability and Independence">🔗</a></div></section><section class="subsection" id="subsec-probability-distributions"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.4</span><span class="space"> </span><span class="title">Probability Distributions</span>
</h3>
<div class="para" id="subsec-probability-distributions-2">Probability distributions describe how probabilities are distributed over outcomes. In machine learning, distributions model data or predictions.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-probability-distributions-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-probability-distributions-3">
<em class="alert">Bernoulli Distribution</em>: Models a binary outcome (e.g., pass/fail) with probability <span class="process-math">\(p\text{.}\)</span> For passing an exam, <span class="process-math">\(P(\text{Pass}) = p\text{,}\)</span> <span class="process-math">\(P(\text{Fail}) = 1-p\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-probability-distributions-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-probability-distributions-4">
<em class="alert">Binomial Distribution</em>: Counts successes in <span class="process-math">\(n\)</span> independent Bernoulli trials. For 10 students, the number who pass follows a binomial distribution.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-probability-distributions-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="code-box"><pre class="program clipboardable line-numbers"><code class="language-py"># --- BINOMIAL DISTRIBUTION ---
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

n, p = 10, 0.7  # 10 students, P(Pass) = 0.7
k = np.arange(0, 11)
pmf = binom.pmf(k, n, p)

plt.bar(k, pmf)
plt.xlabel('Number of Passes')
plt.ylabel('Probability')
plt.title('Binomial Distribution (n=10, p=0.7)')
plt.grid(True, alpha=0.3)
plt.savefig('./images/essential-probability-and-statistics/binomial-dist.png', dpi=300)
plt.show()
# --- END CODE ---
</code></pre></div>
<figure class="figure figure-like" id="fig-binomial-dist"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/binomial-dist.png" class="contained" alt="Bar plot of binomial distribution."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.4<span class="period">.</span></span><span class="space"> </span>Bar plot of the binomial probability mass function (PMF) for the number of students passing an exam out of 10, with a pass probability <span class="process-math">\(p=0.7\text{.}\)</span> Each bar represents the probability of <span class="process-math">\(k\)</span> students passing, calculated as <span class="process-math">\(P(k) = \binom{n}{k} p^k (1-p)^{n-k}\text{.}\)</span> The peak around 7 passes reflects the high likelihood of most students passing given <span class="process-math">\(p=0.7\text{.}\)</span> This distribution is critical in machine learning for modeling binary outcomes, such as predicting the number of successful predictions in a classification task.<div class="autopermalink" data-description="Figure 1.4.4"><a href="#fig-binomial-dist" title="Copy heading and permalink for Figure 1.4.4" aria-label="Copy heading and permalink for Figure 1.4.4">🔗</a></div></figcaption></figure><div class="autopermalink" data-description="Subsection 1.4.4: Probability Distributions"><a href="#subsec-probability-distributions" title="Copy heading and permalink for Subsection 1.4.4: Probability Distributions" aria-label="Copy heading and permalink for Subsection 1.4.4: Probability Distributions">🔗</a></div></section><section class="subsection" id="subsec-three-types-of-probabilities"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.5</span><span class="space"> </span><span class="title">Three Types of Probabilities</span>
</h3>
<div class="para" id="subsec-three-types-of-probabilities-2">Probability can be approached theoretically, empirically (frequentist), or subjectively (Bayesian).<div class="autopermalink" data-description="Paragraph"><a href="#subsec-three-types-of-probabilities-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<ol class="decimal" id="subsec-three-types-of-probabilities-3">
<li id="subsec-three-types-of-probabilities-3-1">
<div class="para" id="subsec-three-types-of-probabilities-3-1-1">
<em class="alert">Theoretical Probability</em>: Uses symmetry. For a fair die, <span class="process-math">\(P(\{1\}) = 1/6\text{.}\)</span> For even numbers, <span class="process-math">\(P(\{2,4,6\}) = 3/6 = 0.5\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-three-types-of-probabilities-3-1-1" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="autopermalink" data-description="Item 1"><a href="#subsec-three-types-of-probabilities-3-1" title="Copy heading and permalink for Item 1" aria-label="Copy heading and permalink for Item 1">🔗</a></div>
</li>
<li id="subsec-three-types-of-probabilities-3-2">
<div class="para logical" id="subsec-three-types-of-probabilities-3-2-1">
<div class="para">
<em class="alert">Frequentist Probability</em>: Estimates probability from trial frequencies:</div>
<div class="displaymath process-math" id="eqn-frequentist-probability-definition">
\begin{equation}
p = \lim_{N \to \infty} \frac{n}{N}\text{.}\tag{1.4.9}
\end{equation}
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-three-types-of-probabilities-3-2-1" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="code-box"><pre class="program clipboardable line-numbers"><code class="language-py"># --- FREQUENTIST SIMULATION ---
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
n_trials = 1000
fair_rolls = np.random.randint(1, 7, n_trials)
biased_rolls = np.random.choice([1, 2, 3, 4, 5, 6], n_trials, 
                                p=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1])

# Cumulative probabilities
cum_fair = np.cumsum(fair_rolls == 1) / np.arange(1, n_trials + 1)
cum_biased = np.cumsum(biased_rolls == 1) / np.arange(1, n_trials + 1)

plt.plot(cum_fair, label='Fair Die (P=1/6)')
plt.plot(cum_biased, label='Biased Die (P=0.2)')
plt.axhline(1/6, color='red', linestyle='--', label='Theoretical P=1/6')
plt.xlabel('Trials')
plt.ylabel('Estimated P(1)')
plt.title('Frequentist Estimates: Fair vs. Biased Die')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('frequentist-convergence.png', dpi=300)
plt.show()
# --- END CODE ---
</code></pre></div>
<figure class="figure figure-like" id="fig-frequentist-convergence"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/frequentist-convergence.png" class="contained" alt="Convergence plot for frequentist estimates."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.5<span class="period">.</span></span><span class="space"> </span>Plot showing the convergence of frequentist probability estimates for rolling a 1 on a fair die (<span class="process-math">\(P(1)=1/6 \approx 0.167\)</span>) and a biased die (<span class="process-math">\(P(1)=0.2\)</span>) over 1,000 trials. The fair die’s estimate (blue) fluctuates but approaches 1/6 (red dashed line), while the biased die’s estimate (orange) converges to 0.2, reflecting the higher probability of rolling a 1. This visualization demonstrates how empirical frequencies approximate true probabilities in large samples, a technique used in machine learning to estimate probabilities from training data.<div class="autopermalink" data-description="Figure 1.4.5"><a href="#fig-frequentist-convergence" title="Copy heading and permalink for Figure 1.4.5" aria-label="Copy heading and permalink for Figure 1.4.5">🔗</a></div></figcaption></figure><div class="autopermalink" data-description="Item 2"><a href="#subsec-three-types-of-probabilities-3-2" title="Copy heading and permalink for Item 2" aria-label="Copy heading and permalink for Item 2">🔗</a></div>
</li>
<li id="subsec-three-types-of-probabilities-3-3">
<div class="para" id="subsec-three-types-of-probabilities-3-3-1">
<em class="alert">Bayesian Probability</em>: Updates prior beliefs with data using Bayes’ theorem: <span class="process-math">\(P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}\text{.}\)</span> For die face 1, use a Beta prior, updated to Beta(<span class="process-math">\(\alpha + n_1, \beta + (N - n_1)\)</span>).<div class="autopermalink" data-description="Paragraph"><a href="#subsec-three-types-of-probabilities-3-3-1" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="para" id="subsec-three-types-of-probabilities-3-3-2">Example: Estimate <span class="process-math">\(P(\text{Pass})\)</span> for students using the dataset, starting with a Beta(1,1) prior.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-three-types-of-probabilities-3-3-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div>
<div class="code-box"><pre class="program clipboardable line-numbers"><code class="language-py"># --- BAYESIAN UPDATE ---
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta
import pandas as pd

# Student data
np.random.seed(42)
data = pd.DataFrame({
    'Passed': np.random.binomial(1, 0.7, 10)
})

# Prior: Beta(1,1)
a, b = 1, 1
n, n1 = len(data), data['Passed'].sum()
a_post, b_post = a + n1, b + n - n1

# Plot prior and posterior
x = np.linspace(0, 1, 1000)
plt.plot(x, beta.pdf(x, a, b), label='Prior Beta(1,1)', color='blue')
plt.plot(x, beta.pdf(x, a + 1, b + 1), label='After 1 Pass', color='orange')
plt.plot(x, beta.pdf(x, a_post, b_post), label=f'Posterior Beta({a_post},{b_post})', color='green')
plt.axvline(n1/n, color='red', linestyle='--', label='Frequentist Est.')
plt.xlabel('P(Pass)')
plt.ylabel('Density')
plt.title('Bayesian Update for P(Pass)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('./images/essential-probability-and-statistics/bayesian-update.png', dpi=300)
plt.show()
# --- END CODE ---
</code></pre></div>
<figure class="figure figure-like" id="fig-bayesian-update"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/bayesian-update.png" class="contained" alt="Bayesian update plot."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.6<span class="period">.</span></span><span class="space"> </span>Plot showing the Bayesian update of the probability of a student passing an exam, starting with a uniform Beta(1,1) prior (blue). After observing one pass (orange) and 10 trials with 3 passes (green, posterior Beta(4,8)), the distribution shifts, with the posterior mean at 4/12 ≈ 0.333. The frequentist estimate (red dashed line, 3/10 = 0.3) is shown for comparison. This visualization illustrates how Bayesian methods incorporate prior beliefs and data to refine probability estimates, a technique used in machine learning for probabilistic models and uncertainty quantification.<div class="autopermalink" data-description="Figure 1.4.6"><a href="#fig-bayesian-update" title="Copy heading and permalink for Figure 1.4.6" aria-label="Copy heading and permalink for Figure 1.4.6">🔗</a></div></figcaption></figure><div class="autopermalink" data-description="Item 3"><a href="#subsec-three-types-of-probabilities-3-3" title="Copy heading and permalink for Item 3" aria-label="Copy heading and permalink for Item 3">🔗</a></div>
</li>
</ol>
<div class="autopermalink" data-description="Subsection 1.4.5: Three Types of Probabilities"><a href="#subsec-three-types-of-probabilities" title="Copy heading and permalink for Subsection 1.4.5: Three Types of Probabilities" aria-label="Copy heading and permalink for Subsection 1.4.5: Three Types of Probabilities">🔗</a></div></section><section class="conclusion" id="sec-Basic-Probability-8"><div class="para" id="sec-Basic-Probability-8-1">Probability provides the foundation for modeling uncertainty in machine learning. Axioms define the rules, while theoretical, frequentist, and Bayesian approaches offer different perspectives. Conditional probability and distributions like binomial are key for models like Naive Bayes. Practice with datasets from <a href="sec-data-types-for-machine-learning.html" class="internal" title="Section 1.3: Data Types for Machine Learning">Section 1.3</a> and libraries from <a href="sec-useful-descriptive-statistics-tools.html" class="internal" title="Section 1.2: Useful Tools for Descriptive Statistics and Exploratory Data Analysis">Section 1.2</a> to apply these concepts. Explore <a class="external" href="https://www.probabilitycourse.com/" target="_blank">Probability Course</a><details class="ptx-footnote" aria-live="polite" id="sec-Basic-Probability-8-1-4"><summary class="ptx-footnote__number" title="Footnote 1.4.1"><sup> 1 </sup></summary><div class="ptx-footnote__contents" id="sec-Basic-Probability-8-1-4"><code class="code-inline tex2jax_ignore">probabilitycourse.com</code></div></details> for further learning.<div class="autopermalink" data-description="Paragraph"><a href="#sec-Basic-Probability-8-1" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">🔗</a></div>
</div></section><div class="autopermalink" data-description="Section 1.4: Basic Probability for Machine Learning"><a href="#sec-Basic-Probability" title="Copy heading and permalink for Section 1.4: Basic Probability for Machine Learning" aria-label="Copy heading and permalink for Section 1.4: Basic Probability for Machine Learning">🔗</a></div></section></div>
<div id="ptx-content-footer" class="ptx-content-footer">
<a class="previous-button button" href="sec-data-types-for-machine-learning.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Top</span></a><a class="next-button button" href="sec-Joint-Conditional-and-Marginal-Probabilities.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" height="100%" viewBox="338 3000 8772 6866" role="img"><title>PreTeXt logo</title><g style="stroke-width:.025in; stroke:currentColor; fill:none"><polyline points="472,3590 472,9732 " style="stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png" alt="Runstone Academy logo"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png" alt="MathJax logo"></a>
</div>
</body>
</html>
