<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<!--******************************************-->
<!--*  Authored with PreTeXt                 *-->
<!--*  pretextbook.org                       *-->
<!--*  Theme: default-modern                 *-->
<!--*  Palette:                              *-->
<!--******************************************-->
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Joint, Conditional and Marginal Probabilities</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0">
<link href="_static/pretext/css/theme.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/ol-markers.css" rel="stylesheet" type="text/css">
<script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math"
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "_static/pretext/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="ML Notes">
<meta property="book:author" content="Samuel Ling">
<script src="_static/pretext/js/lib/jquery.min.js"></script><script src="_static/pretext/js/lib/jquery.sticky.js"></script><script src="_static/pretext/js/lib/jquery.espy.min.js"></script><script src="_static/pretext/js/pretext.js"></script><script src="_static/pretext/js/pretext_add_on.js?x=1"></script><script src="_static/pretext/js/user_preferences.js"></script><!--** eBookCongig is necessary to configure interactive       **-->
<!--** Runestone components to run locally in reader's browser **-->
<!--** No external communication:                              **-->
<!--**     log level is 0, Runestone Services are disabled     **-->
<script type="text/javascript">
eBookConfig = {};
eBookConfig.useRunestoneServices = false;
eBookConfig.host = 'http://127.0.0.1:8000';
eBookConfig.course = 'PTX_Course_Title_Here';
eBookConfig.basecourse = 'PTX_Base_Course';
eBookConfig.isLoggedIn = false;
eBookConfig.email = '';
eBookConfig.isInstructor = false;
eBookConfig.logLevel = 0;
eBookConfig.username = '';
eBookConfig.readings = null;
eBookConfig.activities = null;
eBookConfig.downloadsEnabled = false;
eBookConfig.allow_pairs = false;
eBookConfig.enableScratchAC = false;
eBookConfig.build_info = "";
eBookConfig.python3 = null;
eBookConfig.runestone_version = '7.9.8';
eBookConfig.jobehost = '';
eBookConfig.proxyuri_runs = '';
eBookConfig.proxyuri_files = '';
eBookConfig.enable_chatcodes =  false;
</script>
<!--*** Runestone Services ***-->
<script src="_static/prefix-runtime.b215b0da62d655bd.bundle.js"></script><script src="_static/prefix-723.3e6434f80549315a.bundle.js"></script><script src="_static/prefix-runestone.28cc3c4821792be5.bundle.js"></script><link rel="stylesheet" type="text/css" href="_static/prefix-723.3bccd435914aa0ff.css">
<link rel="stylesheet" type="text/css" href="_static/prefix-runestone.8141ae22fd347e48.css">
<script src="_static/pretext/js/lti_iframe_resizer.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-numbers/prism-line-numbers.min.js" integrity="sha512-dubtf8xMHSQlExGRQ5R7toxHLgSDZ0K7AunqPWHXmJQ8XyVIG19S1T95gBxlAeGOK02P4Da2RTnQz0Za0H0ebQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-highlight/prism-line-highlight.min.js" integrity="sha512-93uCmm0q+qO5Lb1huDqr7tywS8A2TFA+1/WHvyiWaK6/pvsFl6USnILagntBx8JnVbQH5s3n0vQZY6xNthNfKA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="_static/pretext/js/pretext_search.js"></script><script src="_static/pretext/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script>
</head>
<body id="ML-Notes" class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner"><div class="title-container">
<h1 class="heading"><a href="my-ML-Notes.html"><span class="title">ML Notes</span> <span class="subtitle">Theoretical and Practical ML Concepts</span></a></h1>
<p class="byline">Samuel Ling</p>
</div></div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><div class="ptx-navbar-contents">
<button class="toc-toggle button" title="Contents"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5d2;</span><span class="name">Contents</span></button><div class="searchbox">
<div class="searchwidget"><button id="searchbutton" class="searchbutton button" type="button" title="Search book"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe8b6;</span><span class="name">Search Book</span></button></div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<div class="search-results-controls">
<input aria-label="Search term" id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search term"><button title="Close search" id="closesearchresults" class="closesearchresults"><span class="material-symbols-outlined">close</span></button>
</div>
<h2 class="search-results-heading">Search Results: </h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div>
<span class="nav-other-controls"><button id="light-dark-button" class="light-dark-button button" title="Dark Mode"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe51c;</span><span class="name">Dark Mode</span></button></span><span class="treebuttons"><a class="previous-button button" href="sec-Basic-Probability.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="up-button button" href="ch-Essential-Probability-and-Statistics.html" title="Up"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Up</span></a><a class="next-button button" href="sec-Example-Discrete-Probability-Distributions.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a></span>
</div></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\N}{\mathbb N} \newcommand{\Z}{\mathbb Z} \newcommand{\Q}{\mathbb Q} \newcommand{\R}{\mathbb R}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2 focused" data-preexpanded-levels="0" data-max-levels="2"><ul class="structural toc-item-list contains-active">
<li class="toc-item toc-frontmatter"><div class="toc-title-box"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div></li>
<li class="toc-item toc-chapter contains-active">
<div class="toc-title-box"><a href="ch-Essential-Probability-and-Statistics.html" class="internal"><span class="codenumber">1</span> <span class="title">Essential Probability and Statistics</span></a></div>
<ul class="structural toc-item-list contains-active">
<li class="toc-item toc-section"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Descriptive Statistics</span></a></div></li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Useful Descriptive Statistics Tools</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-Pandas" class="internal"><span class="codenumber">1.2.1</span> <span class="title">Pandas</span></a></div></li></ul>
</li>
<li class="toc-item toc-section"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Numerical and Categorical Data</span></a></div></li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Basic-Probability.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Basic Probability</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-Axiomatic-View-of-Probability" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Axiomatic View of Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-three-types-of-probabilities" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Three Types of Probabilities</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section contains-active">
<div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Joint, Conditional and Marginal Probabilities</span></a></div>
<ul class="structural toc-item-list active">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Random-Variables" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Random Variables, Probabilities, and Expections</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Joint-Probability" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Joint ad Marginal Probabilities</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsubsec-Covariance-and-Correlation" class="internal"><span class="codenumber">1.5.2.1</span> <span class="title">Covariance and Correlation</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Conditional-Probability" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Conditional Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-conditional-probability-from-joint-probability" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Bayesâ€™ Rule</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Independent-Variables" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Independent Variables</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Complications-For-Continuous-Variables" class="internal"><span class="codenumber">1.5.6</span> <span class="title">Complications-For-Continuous-Variables</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsubsec-Probability-Density" class="internal"><span class="codenumber">1.5.6.1</span> <span class="title">Probability Density</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#subsubsec-Cumulative-Distribution-Function" class="internal"><span class="codenumber">1.5.6.2</span> <span class="title">Cumulative Distribution Function (CDF)</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html" class="internal"><span class="codenumber">1.6</span> <span class="title">Example Discrete Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Bernoulli-Distribution" class="internal"><span class="codenumber">1.6.1</span> <span class="title">Bernoulli Distribution</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Binomial-Distribution" class="internal"><span class="codenumber">1.6.2</span> <span class="title">Binomial Distribution</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Poisson-Distribution" class="internal"><span class="codenumber">1.6.3</span> <span class="title">Poisson Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsub-Poisson-Process" class="internal"><span class="codenumber">1.6.3.1</span> <span class="title">Poisson Process</span></a></div></li></ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html" class="internal"><span class="codenumber">1.7</span> <span class="title">Example Continuous Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Uniform-Distribution" class="internal"><span class="codenumber">1.7.1</span> <span class="title">Uniform Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsubsec-Inverse-Uniform-CDF" class="internal"><span class="codenumber">1.7.1.1</span> <span class="title">Inverse Uniform CDF</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Normal-Distribution" class="internal"><span class="codenumber">1.7.2</span> <span class="title">Normal (Gaussian) Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsubsec-Inverse-CDF-and-Sampling" class="internal"><span class="codenumber">1.7.2.1</span> <span class="title">Inverse CDF and Sampling</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Exponential-Distribution" class="internal"><span class="codenumber">1.7.3</span> <span class="title">Exponential Distribution</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-LLN-and-CLT.html" class="internal"><span class="codenumber">1.8</span> <span class="title">Law of Large Numbers and Central Limit Theorem</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Law-of-Large-Numbers" class="internal"><span class="codenumber">1.8.1</span> <span class="title">Law of Large Numbers (LLN)</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Central-Limit-Theorem" class="internal"><span class="codenumber">1.8.2</span> <span class="title">Central Limit Theorem</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-LLN-vs-CLT" class="internal"><span class="codenumber">1.8.3</span> <span class="title">LLN vs CLT</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Why-Large-n-Matters" class="internal"><span class="codenumber">1.8.4</span> <span class="title">Why LLN and CLT Matter?</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-backmatter"><div class="toc-title-box"><a href="backmatter.html" class="internal"><span class="title">Backmatter</span></a></div></li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content"><section class="section" id="sec-Joint-Conditional-and-Marginal-Probabilities"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">1.5</span><span class="space"> </span><span class="title">Joint, Conditional and Marginal Probabilities</span>
</h2>
<section class="introduction" id="sec-Joint-Conditional-and-Marginal-Probabilities-2"><div class="para" id="sec-Joint-Conditional-and-Marginal-Probabilities-2-1">In this section, we address probabilities in situation when you have more than one variable. Whether they are independent or not independent makes a difference. Letâ€™s first define a variable in the context of probability and statistics.<div class="autopermalink" data-description="Paragraph"><a href="#sec-Joint-Conditional-and-Marginal-Probabilities-2-1" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div></section><section class="subsection" id="subsec-Random-Variables"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.5.1</span><span class="space"> </span><span class="title">Random Variables, Probabilities, and Expections</span>
</h3>
<div class="para" id="subsec-Random-Variables-2">We will think of variables as something the is observed or measured by experiments. The outcome in any experiment is a real value of the variable. A variable whose value is uncertain or unpredictable or varies from trial to trial, even though measurement conditions havenâ€™t changed, is called a random variable.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Random-Variables-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsec-Random-Variables-3">We tend to use <em class="alert">capital letter for the name</em> of the variable and small letters for its values. Thus, for a variable <span class="process-math">\(X\text{,}\)</span> the values will be denoted by <span class="process-math">\(x_1, x_2, \cdots,\)</span> etc. Sometimes we will use superscripts to denote values, <span class="process-math">\(x^{(1)},x^{(2)}, \cdots \text{.}\)</span> An <em class="alert">event</em> will now refer to the outcome that in a particular trial, variable <span class="process-math">\(X\)</span> has some value or a set of the possible values. Thus, <span class="process-math">\(X=\{x_1\}\)</span> would be an event and so would be <span class="process-math">\(X=\{x_1, x_2\}\text{,}\)</span>  etc. In case of continuous values for <span class="process-math">\(X\text{,}\)</span> and event may even be written as <span class="process-math">\(x_1 \lt X \le x_2\text{,}\)</span> etc. We will speak about probabilities of these events in a later section.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Random-Variables-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsec-Random-Variables-4">In our practice, the values of random variables will all be real numbers. Now, if <span class="process-math">\(X\)</span> can take any real number, whether the entire real line or a finite segment of the real line, then we will call such a variable a <em class="alert">continuous variable</em>. An example of continuous variable will be price of a house, <span class="process-math">\(X=\{x| 0 \le x \lt 1,000,000\}\)</span> if the price cannot be more than <span class="process-math">\(1,000,000\text{.}\)</span> On the other hand, if the variable takes on only discrete values, then it will be called <em class="alert">categorical or discrete variable</em>. An example of discrete variable will fruits of interest in some grocery store <span class="process-math">\(X=\{ \text{'apple'}, \text{'orange'}, \text{'pear'}, \text{'watermelon'}, \text{'grapes'} \}\)</span> which has five categories.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Random-Variables-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-Random-Variables-5">
<div class="para">The probability measure <span class="process-math">\(P\)</span> of a probability space <span class="process-math">\((\Omega, F, P)\)</span> of a categorical variable has to specify probabilities of each of the elements of finite set <span class="process-math">\(\Omega\text{.}\)</span> Let there be <span class="process-math">\(N\)</span> distinct values of of a categorical variable <span class="process-math">\(X\text{,}\)</span> say, <span class="process-math">\(x_1, x_2, \cdots, x_N\text{.}\)</span> Then, all we need to specify <span class="process-math">\(P\)</span> are the numbers <span class="process-math">\(p_i\)</span> for each unique/exclusive outcome.</div>
<div class="displaymath process-math" id="eqn-probability-mass-function">
\begin{equation}
P(X=x_i) = p_i\tag{1.5.1}
\end{equation}
</div>
<div class="para">Note that due to normalization, the probability of event <span class="process-math">\(\Omega\text{,}\)</span> will be 1.</div>
<div class="displaymath process-math" id="eqn-normalization-of-probability">
\begin{equation}
P(\Omega) = \sum_{i=1}^N P(X=x_i) = \sum_{i=1}^N p_1 = 1.\tag{1.5.2}
\end{equation}
</div>
<div class="para">This probability measure is called <em class="alert">probability mass function</em> or <span class="process-math">\(PMF\text{.}\)</span> Given a PMF, it is easy to find the mean value of the random variable <span class="process-math">\(X\text{,}\)</span> which will be denoted by angle brackets as per physics notation.</div>
<div class="displaymath process-math" id="eqn-mean-of-random-variable">
\begin{equation}
\text{Mean}(X) = \langle X \rangle = \sum_{i=1}^N  x_i\, P(X=x_i) = \sum_{i=1}^N\, x_i\, p_i.\tag{1.5.3}
\end{equation}
</div>
<div class="para">The process of taking mean shows that it just sum of values of <span class="process-math">\(X\)</span> weighted according to their probabilities. This weighting according to probabilities is called taking <em class="alert">expectation</em>. Thus, expectation value of any function <span class="process-math">\(f(X)\)</span> is obtained accordingly.</div>
<div class="displaymath process-math" id="eqn-expectation-values">
\begin{equation}
\text{Expectation value of } f(X) \equiv \langle f(X) \rangle = \sum_{i=1}^N\, p_i\, f(x_i).\tag{1.5.4}
\end{equation}
</div>
<div class="para">The variance will be expectation value of another variable obtained by subtracting the mean <span class="process-math">\(\langle X \rangle\)</span> from the random variable <span class="process-math">\(X\)</span> and squaring that. This gives us a measure of the spread of values about the mean.</div>
<div class="displaymath process-math" id="eqn-variance-of-random-variable">
\begin{equation}
\text{Var}(X) = \langle  \left( X - \langle X \rangle\right)^2 \rangle,\tag{1.5.5}
\end{equation}
</div>
<div class="para">where <span class="process-math">\(X\)</span> is the variable but <span class="process-math">\(\langle X \rangle\)</span> is just a number. The following calculation will relate variance to <span class="process-math">\(x_i\)</span> and <span class="process-math">\(p_i\text{.}\)</span>
</div>
<div class="displaymath process-math" id="subsec-Random-Variables-5-27">
\begin{align*}
\text{Var}(X) \amp = \sum_{i=1}^N \left( x_i - \langle X \rangle\right)^2\, P(X=x_i) \\
\amp = \sum_{i=1}^N \left( x_i^2 - 2 x_i \langle X \rangle  + \langle X \rangle^2 \right)\, p_i \\
\amp = \sum_{i=1}^N x_i^2 p_i - 2 \langle X \rangle\sum_{i=1}^N x_i p_i + \langle X \rangle^2\sum_{i=1}^N p_i  \\
\amp = \left( \sum_{i=1}^N p_i\,x_i^2 \right) - \langle X \rangle^2
\end{align*}
</div>
<div class="para">The first term is just expectation value of variable <span class="process-math">\(X^2\text{,}\)</span> i.e., <span class="process-math">\(\langle X^2 \rangle\text{.}\)</span> Therefore, we often see variance in the following simpler-looking formula.</div>
<div class="displaymath process-math" id="eqn-variance-simpler">
\begin{equation}
\text{Var}(X) = \langle X^2 \rangle - \left( \langle X \rangle \right)^2.          \tag{1.5.6}
\end{equation}
</div>
<div class="para">The standard deviation will, of course be, just square root of variance.</div>
<div class="displaymath process-math" id="eqn-standard-deviation-from-variance">
\begin{equation}
\text{Stdev}(X) = \sqrt{ \text{Var}(X) }.\tag{1.5.7}
\end{equation}
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Random-Variables-5" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="autopermalink" data-description="Subsection 1.5.1: Random Variables, Probabilities, and Expections"><a href="#subsec-Random-Variables" title="Copy heading and permalink for Subsection 1.5.1: Random Variables, Probabilities, and Expections" aria-label="Copy heading and permalink for Subsection 1.5.1: Random Variables, Probabilities, and Expections">ðŸ”—</a></div></section><section class="subsection" id="subsec-Joint-Probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.5.2</span><span class="space"> </span><span class="title">Joint ad Marginal Probabilities</span>
</h3>
<section class="introduction" id="subsec-Joint-Probability-2"><div class="para" id="subsec-Joint-Probability-2-1">Suppose you have two discrete random variables, e.g, does the patient have a disease <span class="process-math">\((X)\)</span> and is the diagnosis positive <span class="process-math">\((Y)\text{.}\)</span> Both of these variables have just two values: <span class="process-math">\(\Omega_X = \{ x_1 = \text{Yes}, x_2 = \text{No}\}\)</span> and <span class="process-math">\(\Omega_Y = \{ y_1 = \text{Yes}, y_2 = \text{No}\}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-Joint-Probability-2-1" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-Joint-Probability-2-2">
<div class="para">
<em class="alert">Joint probability</em> is probability measure over the outcome space <span class="process-math">\(\Omega\)</span> that has all the combinations of the elements of <span class="process-math">\(\Omega_X\)</span> and <span class="process-math">\(\Omega_Y\text{.}\)</span> That is, we will specify probability measure for each of the elements of following <span class="process-math">\(\Omega\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
\Omega = \{ (x_1, y_1), (x_1, y_2), (x_2, y_1), (x_2, y_2) \}.
\end{equation*}
</div>
<div class="para">That is we need the following probabilities.</div>
<div class="displaymath process-math" id="subsec-Joint-Probability-2-2-7">
\begin{align*}
p_{11} \amp = P(X=x_1, Y=y_1) \\
p_{12} \amp = P(X=x_1, Y=y_2) \\
p_{21} \amp = P(X=x_2, Y=y_1) \\
p_{22} \amp = P(X=x_2, Y=y_2) 
\end{align*}
</div>
<div class="para">These probabilities are joint probabilities over the joint space of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{.}\)</span> We often write joint probability by just listing the variable names or even a symbol for variable values, e.g., <span class="process-math">\(x\text{,}\)</span> without specifying <span class="process-math">\(x_1, x_2, \text{etc}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
\text{Joint Probability } \equiv P(X,Y) \equiv p(x,y).
\end{equation*}
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Joint-Probability-2-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-Joint-Probability-2-3">
<div class="para">If you ignore <span class="process-math">\(Y\)</span> and just look at the probabilities of patient having the disease and not having the disease, they are called <em class="alert">Marginal Probabilities</em> of <span class="process-math">\(X\text{.}\)</span> If you had more than two variables, say <span class="process-math">\(Y, Z, A, B, \cdots\text{,}\)</span> you would just ignore all the other variables to focus only on <span class="process-math">\(X\)</span> to obtain its marginal probabilities.</div>
<div class="displaymath process-math" id="eqn-marginal-probability-of-X">
\begin{equation}
P(x=x_1) = p_1,\ \ P(X=x_2) = p_2.\tag{1.5.8}
\end{equation}
</div>
<div class="para">Similar to the notation of joint probabilities, we often write the marginal probabilities by simply <span class="process-math">\(P(X)\)</span> or <span class="process-math">\(p(x)\text{.}\)</span>
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Joint-Probability-2-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-Joint-Probability-2-4">
<div class="para">It turns out that the marginal probabilities can be obtained from joint probabilities by just summing out the other variableâ€™s values in the joint probability. Thus, in our example, the p_1 and p_2 of <span class="process-math">\(X\)</span> alone will be</div>
<div class="displaymath process-math" id="subsec-Joint-Probability-2-4-2">
\begin{align*}
p_1 \amp = p_{11} + p_{12} \equiv \sum_{y=1}^2 P(X=1,Y=y) \\
p_2 \amp = p_{21} + p_{22} \equiv \sum_{y=1}^2 P(X=2,Y=y) 
\end{align*}
</div>
<div class="para">Same will be the case if you are interested in the marginal probability of <span class="process-math">\(Y\text{.}\)</span>
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Joint-Probability-2-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div></section><section class="subsubsection" id="subsubsec-Covariance-and-Correlation"><h4 class="heading hide-type">
<span class="type">Subsubsection</span><span class="space"> </span><span class="codenumber">1.5.2.1</span><span class="space"> </span><span class="title">Covariance and Correlation</span>
</h4>
<div class="para" id="subsubsec-Covariance-and-Correlation-2">A very common aspect of dealing with more than one random variable, say <span class="process-math">\(X\)</span> (e.g., height of a man) and <span class="process-math">\(Y\)</span> (e.g., the weight of the man) is to find out to what extent they tend to vary together. <em class="alert">Covariance</em> is a measure of their varying together either in the same direction or in the opposite direction. The normalized version of covariance so that result lies between <span class="process-math">\(-1\)</span> and <span class="process-math">\(+1\)</span> is called <em class="alert">correlation</em>.<div class="autopermalink" data-description="Paragraph"><a href="#subsubsec-Covariance-and-Correlation-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsubsec-Covariance-and-Correlation-3">
<div class="para">Let <span class="process-math">\(P(X,Y)\)</span> denote the joint probability of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{.}\)</span> Then, covariance is the following expectation value computed in this probability distribution.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html" id="eqn-Covariance">
\begin{equation}
\text{Cov}(X,Y) = \langle  \left( X - \mu_X \right)   \left( Y - \mu_Y \right) \rangle,\tag{1.5.9}
\end{equation}
</div>
<div class="para">where to keep the formula simpler, the mean values of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are dented by <span class="process-math">\(\mu_X\)</span> and <span class="process-math">\(\mu_Y\)</span> respectively.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html">
\begin{equation*}
\mu_X = \langle X \rangle,\quad \mu_Y = \langle Y \rangle
\end{equation*}
</div>
<div class="para">By opening the braces inside the angle brackets in Eq. <a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#eqn-Covariance" class="xref" data-knowl="./knowl/xref/eqn-Covariance.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.5.9">(1.5.9)</a>, we can rewrite the Covariance formula in another way.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html" id="eqn-Covariance-2">
\begin{equation}
\text{Cov}(X,Y) = \langle  X\,Y \rangle - \mu_X\,\mu_Y,\tag{1.5.10}
\end{equation}
</div>
<div class="para">where <span class="process-math">\(\langle  X\,Y \rangle\)</span> is to be computed using <span class="process-math">\(P(X,Y)\text{.}\)</span> For <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> discrete variable and <span class="process-math">\(P(X,Y)\)</span> a PMF, the calculation will be</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html">
\begin{equation*}
\langle  X\,Y \rangle = \sum_{x_i} \sum_{y_j} x_i y_j P(x_i, y_j).
\end{equation*}
</div>
<div class="para">Themean values <span class="process-math">\(\mu_X\)</span> and <span class="process-math">\(\mu_Y\)</span> in this case would relate to the marginals <span class="process-math">\(P(X\)</span> and <span class="process-math">\(P(Y)\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html" id="subsubsec-Covariance-and-Correlation-3-22">
\begin{align*}
\mu_X\amp = \sum_{x_i} \sum_{y_j} x_i P(x_i, y_j) = \sum_{x_i} x_i\, P(x_i).\\
\mu_Y\amp = \sum_{x_i} \sum_{y_j} y_j P(x_i, y_j) = \sum_{y_j} y_j\, P(y_j). 
\end{align*}
</div>
<div class="para">Covariance can take any real value, <span class="process-math">\(\text{Cov}(X,Y) \in (-\infty, +\infty)\text{.}\)</span> By dividing the covariance by the standard deviations of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> we get correlations, whose values are in the range <span class="process-math">\(\text{Corr}(X,Y) \in [-1, +]\text{.}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html" id="eqn-Correlation">
\begin{equation}
\text{Corr}(X,Y) = \frac{ \text{Cov}(X,Y) }{ \sigma_X\, \sigma_Y }.\tag{1.5.11}
\end{equation}
</div>
<div class="para">The standard deviations in the joint distribution are the same as the standard deviation in the marginal as was the case for the means <span class="process-math">\(\mu_X\)</span> and <span class="process-math">\(\mu_Y\text{.}\)</span> For instance, in the case of discrete random variables, we will get the following for the variances, which is the square of standard deviations.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html" id="subsubsec-Covariance-and-Correlation-3-30">
\begin{align*}
\sigma_X^2 \amp = \sum_{x_i} \sum_{y_j} \left( x_i -\mu_X \right)^2 P(x_i, y_j) = \sum_{x_i} \left( x_i -\mu_X \right)^2\, P(x_i).\\
\sigma_Y^2\amp = \sum_{x_i} \sum_{y_j}  \left( y_j -\mu_Y \right)^2 P(x_i, y_j) = \sum_{y_j} \left( y_j -\mu_Y \right)^2\, P(y_j). 
\end{align*}
</div>
<div class="para">The positive correlation (or covariance) means <span class="process-math">\(X\)</span> vary in the same direction, i.e., increasing <span class="process-math">\(X\)</span> and increasing <span class="process-math">\(Y\)</span> occur together. The opposite is the case for negative correlation (or covariance).</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsubsec-Covariance-and-Correlation-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsubsec-Covariance-and-Correlation-4">
<em class="alert">Beware, Correlation is not the same thing as Causation!</em> Just because two things move together doesnâ€™t mean one causes the other. For example, suppose you find that ice cream sales and shark attacks may rise together (both happen in hot weather when people flock to the beaches), but it would be ludicurous to suggest that it means ice cream sales cause the shark attack or vice versa! Machine learning models may find patterns, but not always causal ones.<div class="autopermalink" data-description="Paragraph"><a href="#subsubsec-Covariance-and-Correlation-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="autopermalink" data-description="Subsubsection 1.5.2.1: Covariance and Correlation"><a href="#subsubsec-Covariance-and-Correlation" title="Copy heading and permalink for Subsubsection 1.5.2.1: Covariance and Correlation" aria-label="Copy heading and permalink for Subsubsection 1.5.2.1: Covariance and Correlation">ðŸ”—</a></div></section><div class="autopermalink" data-description="Subsection 1.5.2: Joint ad Marginal Probabilities"><a href="#subsec-Joint-Probability" title="Copy heading and permalink for Subsection 1.5.2: Joint ad Marginal Probabilities" aria-label="Copy heading and permalink for Subsection 1.5.2: Joint ad Marginal Probabilities">ðŸ”—</a></div></section><section class="subsection" id="subsec-Conditional-Probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.5.3</span><span class="space"> </span><span class="title">Conditional Probability</span>
</h3>
<div class="para" id="subsec-Conditional-Probability-2">Conditional probability is a little tricky and is the most used in Machine Learning. Conditional probability is denoted by <span class="process-math">\(P(X|Y)\text{.}\)</span> But, this is too abstract a notation. I will denote it by <span class="process-math">\(P(X|Y=y)\)</span> to be a conditional probability  of <span class="process-math">\(X\)</span> given that the random variable <span class="process-math">\(Y\)</span> HAS A PARTICULAR VALUE <span class="process-math">\(y\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-Conditional-Probability-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsec-Conditional-Probability-3">It is very important to notice here that <span class="process-math">\(P(X|Y=y)\)</span> IS A PROBABILITY OF X, just under some condition. That is, in the joint space <span class="process-math">\(\Omega\)</span> of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{,}\)</span> you will collect all the points that correspond to a particular value of <span class="process-math">\(Y\text{,}\)</span> say <span class="process-math">\(Y=y_1\text{.}\)</span> Now, looking at only those, what can you say about the chances for different values of <span class="process-math">\(X\text{.}\)</span> Clearly, conditional probability is trying to capture using some knowledge about the world that comes after you have known something about the world and thus, reducing the uncertainty.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Conditional-Probability-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsec-Conditional-Probability-4">
<em class="alert">Let us look at a numerical example:</em> Suppose 1000 patients presented symptoms of a disease A. All patients were given a test. Only 200 of the tests came out positive. When later on, we find out which of the patients had the disease and whether test came out positive or not, we construct a table given below. Use it to figure out joint probabilities, marginal probabilities, and conditional probabilities.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Conditional-Probability-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-Conditional-Probability-5">
<div class="para">Before, we start writing formulas, it is best to pick a simple notation to reresent various events.</div>
<div class="displaymath process-math" id="subsec-Conditional-Probability-5-1">
\begin{align*}
D \amp = \text{'Has disease'} \\
N \amp = \text{'Does not have disease'} \\
+ \amp = \text{'Test Positive'} \\
- \amp = \text{'Test Negative'} 
\end{align*}
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Conditional-Probability-5" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<figure class="table table-like" id="tab-patient-data-for-jt-and-conditional-probs"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">1.5.1<span class="period">.</span></span><span class="space"> </span>Patient Data<div class="autopermalink" data-description="Table 1.5.1: Patient Data"><a href="#tab-patient-data-for-jt-and-conditional-probs" title="Copy heading and permalink for Table 1.5.1: Patient Data" aria-label="Copy heading and permalink for Table 1.5.1: Patient Data">ðŸ”—</a></div></figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b0 r0 l0 t0 lines"></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(+\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(-\)</span></td>
<td class="l m b0 r0 l0 t0 lines"></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(D\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(150\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(50\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_D = 200\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(N\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(300\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(500\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_N = 800\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_+ = 450\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_{-}= 550\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_T = 1000\)</span></td>
</tr>
</table></div></figure><div class="para" id="subsec-Conditional-Probability-7">For Joint probabilities, we can just divide number in each square by the total number of patients, which <span class="process-math">\(1000\)</span> here. By dividing the totals of rows <span class="process-math">\(D\)</span> and <span class="process-math">\(N\text{,}\)</span> we will get the marginals <span class="process-math">\(P(D)\)</span> and <span class="process-math">\(P(N)\text{.}\)</span> By dividing the column totals, we get the marginals <span class="process-math">\(P(+)\)</span> and <span class="process-math">\(P(-)\text{.}\)</span> <figure class="table table-like" id="tab-jt-probs-disease-test"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">1.5.2<span class="period">.</span></span><span class="space"> </span>Joint Probabilities<div class="autopermalink" data-description="Table 1.5.2: Joint Probabilities"><a href="#tab-jt-probs-disease-test" title="Copy heading and permalink for Table 1.5.2: Joint Probabilities" aria-label="Copy heading and permalink for Table 1.5.2: Joint Probabilities">ðŸ”—</a></div></figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b0 r0 l0 t0 lines"></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(+\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(-\)</span></td>
<td class="l m b0 r0 l0 t0 lines"></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(D\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(0.150\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(0.050\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(P(D) = 0.200\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(N\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(0.300\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(0.500\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(P(N) = 800\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(P(+) = 450\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(P(-) = 550\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(P(\Omega) = 1.000\)</span></td>
</tr>
</table></div></figure><div class="autopermalink" data-description="Paragraph"><a href="#subsec-Conditional-Probability-7" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-Conditional-Probability-8">
<div class="para">We use <em class="alert">conditional probability</em> answer questions like: what  is the probability of a patient having the disease if he has tested negative? When we look at <a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#tab-patient-data-for-jt-and-conditional-probs" class="xref" data-knowl="./knowl/xref/tab-patient-data-for-jt-and-conditional-probs.html" data-reveal-label="Reveal" data-close-label="Close" title="Table 1.5.1: Patient Data">TableÂ 1.5.1</a>, look at only those cases where people test negative - that is, look at numbers in the â€™Test Negativeâ€™ column in the table. We work out other conditional probabilities similarly. <figure class="table table-like" id="subsec-Conditional-Probability-8-3"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">1.5.3<span class="period">.</span></span><span class="space"> </span>Patients that tested negative<div class="autopermalink" data-description="Table 1.5.3: Patients that tested negative"><a href="#subsec-Conditional-Probability-8-3" title="Copy heading and permalink for Table 1.5.3: Patients that tested negative" aria-label="Copy heading and permalink for Table 1.5.3: Patients that tested negative">ðŸ”—</a></div></figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b0 r0 l0 t0 lines">D</td>
<td class="l m b0 r0 l0 t0 lines">50</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">N</td>
<td class="l m b0 r0 l0 t0 lines">500</td>
</tr>
</table></div></figure> Now, we have a total of <span class="process-math">\(50+500 = 550\)</span> cases of which <span class="process-math">\(50\)</span> have disease. So, the <em class="alert">conditional probability</em> of a patient having the disease even when he has test negative will be</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-patient-data-for-jt-and-conditional-probs.html">
\begin{equation*}
p(D\ |\ -) = \frac{50}{550} = \frac{1}{11}.
\end{equation*}
</div>
<div class="para">That is <span class="process-math">\(1\)</span> in <span class="process-math">\(11\)</span> chance he has disease. Try to answer the question: what if a patient has tested positive, what is the probability that he doesnâ€™t have the disease, i.e., <span class="process-math">\(P(N\ |\ +)\text{?}\)</span> Ans: <span class="process-math">\(2/3\text{.}\)</span>
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Conditional-Probability-8" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-Conditional-Probability-9">
<div class="para">You will have eight conditional probabilities in our simple example here:</div>
<div class="displaymath process-math" id="subsec-Conditional-Probability-9-1">
\begin{align*}
\amp p(D\ |\ - ),\qquad p(N\ |\ - ) = 1 - p(D\ |\ - )\\
\amp p(D\ |\ + ),\qquad p(N\ |\ + ) = 1 - p(D\ |\ + )\\
\amp p(+\ |\ D ),\qquad p(-\ |\ D ) = 1 - p(+\ |\ D )\\
\amp p(+\ |\ N ),\qquad p(-\ |\ N ) = 1 - p(+\ |\ N )
\end{align*}
</div>
<div class="para">Try computing all of them! Here, in each row, you have the complementary event, hence you have to compute only four of them. Itâ€™s important to recognize which event are complementary. For instance, <span class="process-math">\((D\ |\ -)\)</span> and <span class="process-math">\((N\ |\ -)\)</span> are complimentary, but <span class="process-math">\((+\ |\ D )\)</span> and <span class="process-math">\((+\ |\ N )\)</span> are not!</div>
<div class="displaymath process-math">
\begin{equation*}
p(+\ |\ D ) \ne 1 -  p(+\ |\ N ). 
\end{equation*}
</div>
<div class="para">That is because conditional probability <span class="process-math">\(P(X|Y)\)</span> is a probability over the <span class="process-math">\(X\)</span> space and not over the <span class="process-math">\(Y\)</span> space, although the value of <span class="process-math">\(Y=y_j\)</span> is important is choosing the slice of the joint probability. That is why, I like the notation <span class="process-math">\(P(X|Y=y_j)\)</span> even though it is cumbersome to write.</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Conditional-Probability-9" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="autopermalink" data-description="Subsection 1.5.3: Conditional Probability"><a href="#subsec-Conditional-Probability" title="Copy heading and permalink for Subsection 1.5.3: Conditional Probability" aria-label="Copy heading and permalink for Subsection 1.5.3: Conditional Probability">ðŸ”—</a></div></section><section class="subsection" id="subsec-conditional-probability-from-joint-probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.5.4</span><span class="space"> </span><span class="title">Bayesâ€™ Rule</span>
</h3>
<div class="para" id="subsec-conditional-probability-from-joint-probability-2">Before we discuss Bayesâ€™ rule, letâ€™s find the relation between Conditional Probability and Joint Probability. Recall that joint probability of two variables <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> is probability over the space of <span class="process-math">\((X,Y)\)</span>-space, but the conditional probability of <span class="process-math">\(X\)</span> given <span class="process-math">\(Y=y_i\text{,}\)</span> <span class="process-math">\(P(X,Y=y_i)\)</span> is over <span class="process-math">\(X\)</span>-space on a lice of <span class="process-math">\(P(X,Y)\)</span> with <span class="process-math">\(Y=y_i\text{,}\)</span> where <span class="process-math">\(y_i\)</span> is a particular value of <span class="process-math">\(Y\text{.}\)</span><div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-from-joint-probability-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-conditional-probability-from-joint-probability-3">
<div class="para">By product rule of probabilities, itâ€™s clear that</div>
<div class="displaymath process-math" id="eqn-joint-prob-and-cond-prob-event">
\begin{equation}
P(X=x_i, Y=y_j) = P(X=x_i|Y=y_j) P(Y=y_j).\tag{1.5.12}
\end{equation}
</div>
<div class="para">You can write this as</div>
<div class="displaymath process-math">
\begin{equation*}
P(X=x_i|Y=y_j) = \frac{P(X=x_i, Y=y_j)}{ P(Y=y_j)}\ \ \left(  P(Y=y_j) \ne 0\right)
\end{equation*}
</div>
<div class="para">Often, we write this relation in a more general language rather then event by event.</div>
<div class="displaymath process-math" id="eqn-joint-prob-and-cond-prob-general">
\begin{equation}
P(X|Y) = \frac{P(X, Y)}{ P(Y)}.\tag{1.5.13}
\end{equation}
</div>
<div class="para">The marginal <span class="process-math">\(P(Y=y_j)\)</span> can, of course be obtained from the joint probability by marginalizing <span class="process-math">\(X\text{,}\)</span> meaning summing over all <span class="process-math">\(X\text{.}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P(Y=y_j) = \sum_{x_i} P(X=x_i, Y=y_j).
\end{equation*}
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-from-joint-probability-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-conditional-probability-from-joint-probability-4">
<div class="para">Notice that on the left side of Eq. <a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#eqn-joint-prob-and-cond-prob-event" class="xref" data-knowl="./knowl/xref/eqn-joint-prob-and-cond-prob-event.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.5.12">(1.5.12)</a>, the order of listing of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> values is arbitrary, i.e.,</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-conditional-prob-second-way.html">
\begin{equation*}
P(X=x_i, Y=y_j) =  P(Y=y_j, X=x_i).
\end{equation*}
</div>
<div class="para">Thus, it is equally possible to work with a given <span class="process-math">\(X=x_i\)</span> and the associated conditional probability of <span class="process-math">\(Y\text{,}\)</span> i.e., <span class="process-math">\(P(Y=y_j | X=x_i)\)</span> and the marginal probability <span class="process-math">\(P(X=x_i)\text{.}\)</span> This will give us</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-conditional-prob-second-way.html" id="eqn-joint-prob-and-conditional-prob-second-way">
\begin{equation}
P(X=x_i, Y=y_j) = P(Y=y_j|X=x_i) P(X=x_i).\tag{1.5.14}
\end{equation}
</div>
<div class="para">From Eqs. <a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#eqn-joint-prob-and-cond-prob-event" class="xref" data-knowl="./knowl/xref/eqn-joint-prob-and-cond-prob-event.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.5.12">(1.5.12)</a> and <a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#eqn-joint-prob-and-conditional-prob-second-way" class="xref" data-knowl="./knowl/xref/eqn-joint-prob-and-conditional-prob-second-way.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.5.14">(1.5.14)</a>, we get the following relation.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-conditional-prob-second-way.html" id="eqn-Bayes-specific">
\begin{equation}
P(X=x_i|Y=y_j) P(Y=y_j) = P(Y=y_j|X=x_i) P(X=x_i).\tag{1.5.15}
\end{equation}
</div>
<div class="para">Writing this in more general notation, we get the relation that is known as <em class="alert">Bayesâ€™ theorem</em> or <em class="alert">Bayesâ€™ Rule</em>.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-cond-prob-event.html ./knowl/xref/eqn-joint-prob-and-conditional-prob-second-way.html" id="eqn-Bayes-general">
\begin{equation}
P(X|Y) P(Y) = P(Y|X) P(X).\tag{1.5.16}
\end{equation}
</div>
<div class="para">It has become very important in the age of ML since many algorithms rely on it.</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-from-joint-probability-4" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsec-conditional-probability-from-joint-probability-5">Let us see <em class="alert">an example of the application of Bayesâ€™ Rule</em>. Suppose, in a population of <span class="process-math">\(1\%\)</span> women in the age 30 to 40 develop breast cancer. Itâ€™s also known that mammogram identifies <span class="process-math">\(80\%\)</span> of cancers accurately and misses <span class="process-math">\(20\%\)</span> of them. This means that if a woman has breast cancer, the test will be positive <span class="process-math">\(80%\)</span> of the time. Furthermore, the test also give negative values <span class="process-math">\(95\%\)</span> of the time correctly, i.e., if a woman doesnâ€™t have cancer, the test would came out negative <span class="process-math">\(95%\)</span> of the times.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-from-joint-probability-5" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsec-conditional-probability-from-joint-probability-6">Now, a new woman in that age group comes in the lab and unfortunately, she is tested positive. So, what would you say about here chances of actually having the cancer? To answer this question, let us first introduce notations to simplify our formulas.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-from-joint-probability-6" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsec-conditional-probability-from-joint-probability-7">
<div class="para">Let</div>
<div class="displaymath process-math" id="subsec-conditional-probability-from-joint-probability-7-1">
\begin{align*}
C \amp = \text{'Has cancer'} \\
N \amp  = \text{'Does not have cancer'} \\
+  \amp = \text{'Test Positive'} \\
-  \amp = \text{'Test Negative'} 
\end{align*}
</div>
<div class="para">Before, her mammogram, you would say that probability that she has cancer is just <span class="process-math">\(1\%\)</span> since you donâ€™t know anything about her case other than her age group and you are just using the general knowledge. This is called  <em class="alert">prior</em> in the context of Bayesâ€™ rule.</div>
<div class="displaymath process-math">
\begin{equation*}
P(C) = 0.01.
\end{equation*}
</div>
<div class="para">This clearly says that the probability of the complement of Cancer is No Cancer. So,</div>
<div class="displaymath process-math">
\begin{equation*}
P(N) = 1 - P(C) = 1 - 0.01 = 0.99.
\end{equation*}
</div>
<div class="para">From the description, we also know the conditional probability that if a woman has breast cancer, her probability of test positive is <span class="process-math">\(80%\text{.}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P(+\ |\ C) = 0.80.
\end{equation*}
</div>
<div class="para">We have one more information in the data provided.</div>
<div class="displaymath process-math">
\begin{equation*}
P(+\ |\ N) = 0.95.
\end{equation*}
</div>
<div class="para">Now, we use Bayesâ€™ rule:</div>
<div class="displaymath process-math">
\begin{equation*}
P(C\ |\ +) = \frac{ P(+\ |\ C) P(C) }{P(+)},
\end{equation*}
</div>
<div class="para">where</div>
<div class="displaymath process-math">
\begin{equation*}
P(+) = P(+\ |\ C) P(C) + P(+\ |\ N)P(N).
\end{equation*}
</div>
<div class="para">Let us use the numerical values now.</div>
<div class="displaymath process-math">
\begin{equation*}
P(+) = 0.80 \times 0.01 + 0.95 \times 0.99 = 0.9485.
\end{equation*}
</div>
<div class="para">Therefore, our desired conditional probability will be</div>
<div class="displaymath process-math">
\begin{equation*}
P(C\ |\ +) = \frac{ P(+\ |\ C) P(C) }{P(+)} = \frac{0.80 \times 0.01}{0.9485} = 0/084.
\end{equation*}
</div>
<div class="para">That is <span class="process-math">\(8.4\%\text{.}\)</span>
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-from-joint-probability-7" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para" id="subsec-conditional-probability-from-joint-probability-8">My example above came from the following good set of example problems from University of Pennsylvania website, where you can find other examples to practice. <a class="external" href="" target="_blank">Practice Bayes Problems</a><details class="ptx-footnote" aria-live="polite" id="subsec-conditional-probability-from-joint-probability-8-2"><summary class="ptx-footnote__number" title="Footnote 1.5.1"><sup>â€‰1â€‰</sup></summary><div class="ptx-footnote__contents" id="subsec-conditional-probability-from-joint-probability-8-2"><code class="code-inline tex2jax_ignore"></code></div></details>.<div class="autopermalink" data-description="Paragraph"><a href="#subsec-conditional-probability-from-joint-probability-8" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="autopermalink" data-description="Subsection 1.5.4: Bayesâ€™ Rule"><a href="#subsec-conditional-probability-from-joint-probability" title="Copy heading and permalink for Subsection 1.5.4: Bayesâ€™ Rule" aria-label="Copy heading and permalink for Subsection 1.5.4: Bayesâ€™ Rule">ðŸ”—</a></div></section><section class="subsection" id="subsec-Independent-Variables"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.5.5</span><span class="space"> </span><span class="title">Independent Variables</span>
</h3>
<div class="para logical" id="subsec-Independent-Variables-2">
<div class="para">Two random variables are said to be independent if their joint probability factors in the product of their marginal probabilities.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html" id="eqn-independent-variables">
\begin{equation}
P(X,Y) = P(X) P(Y)\ \ \text{(Independent variables)}\tag{1.5.17}
\end{equation}
</div>
<div class="para">Keep in mind that behind the scene, probabilities in this equation are over events, i.e., itâ€™s for <span class="process-math">\(X\)</span> having some particular value and <span class="process-math">\(Y\)</span> having its own particular value. If <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are independent variables, we aexpect</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html">
\begin{equation*}
P(X = x_i\, \text{and}\, Y-+y_j) = P(X=x_i) P(Y=y_j)\ \ \text{for all }x_i \text{ and } y_j.
\end{equation*}
</div>
<div class="para">We wouldnâ€™t write our equations in the verbose manner, prefering to keep it simple. But, beware that probabilities are probabilities of events! Now, we write the left side of Eq. <a href="sec-Joint-Conditional-and-Marginal-Probabilities.html#eqn-independent-variables" class="xref" data-knowl="./knowl/xref/eqn-independent-variables.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.5.17">(1.5.17)</a> using marginal and conditional probabilities.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html">
\begin{equation*}
P(X|Y)P(Y) = P(X) P(Y)\ \ \text{(Independent variables)}
\end{equation*}
</div>
<div class="para">Canceling <span class="process-math">\(P(Y)\)</span> from both sides we get</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html">
\begin{equation*}
P(X|Y) = P(X)\ \ \text{(Independent variables)}
\end{equation*}
</div>
<div class="para">That is knowing something about <span class="process-math">\(Y\)</span> does not tell you anything about probability of <span class="process-math">\(X\text{.}\)</span> That is, conditioning on <span class="process-math">\(Y\)</span> is useless when <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are independent.</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsec-Independent-Variables-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="autopermalink" data-description="Subsection 1.5.5: Independent Variables"><a href="#subsec-Independent-Variables" title="Copy heading and permalink for Subsection 1.5.5: Independent Variables" aria-label="Copy heading and permalink for Subsection 1.5.5: Independent Variables">ðŸ”—</a></div></section><section class="subsection" id="subsec-Complications-For-Continuous-Variables"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.5.6</span><span class="space"> </span><span class="title">Complications-For-Continuous-Variables</span>
</h3>
<section class="subsubsection" id="subsubsec-Probability-Density"><h4 class="heading hide-type">
<span class="type">Subsubsection</span><span class="space"> </span><span class="codenumber">1.5.6.1</span><span class="space"> </span><span class="title">Probability Density</span>
</h4>
<div class="para logical" id="subsubsec-Probability-Density-2">
<div class="para">In our discussions above, we spoke about probability space of a random variable <span class="process-math">\(X\)</span> and spoke of probability of <span class="process-math">\(X=x_i\text{,}\)</span> viz. <span class="process-math">\(P(X=X_i)\text{.}\)</span> This works out nicely if <span class="process-math">\(X\)</span> has only a finite number of discrete values, e.g., it is categorical variable. In that case we call <span class="process-math">\(P(X\)</span> a <em class="alert">probability mass function</em>. However, when <span class="process-math">\(X\)</span> is a continuous variable, say <span class="process-math">\(0\le X\le 1\text{,}\)</span> then there are infinitely many values that <span class="process-math">\(X\)</span> can take. In that case, probability for any one of the values will be zero. For instance, probability that <span class="process-math">\(X=0.1\)</span> will be 0</div>
<div class="displaymath process-math">
\begin{equation*}
P(X=0.1) = 0.
\end{equation*}
</div>
<div class="para">So, it is not a useful concept. We replace this probability with probability of <span class="process-math">\(X\)</span> in a range, e.g.</div>
<div class="displaymath process-math">
\begin{equation*}
P( 0.1 \le X \le 0.3 ), \text{Probability that outcome of the experiment will lie in the range of 0.1 to 0.3}.
\end{equation*}
</div>
<div class="para">To help with computing such probabilities, we introduce a concept of probability density <span class="process-math">\(\rho(x)\text{,}\)</span> which will be probability per unit of <span class="process-math">\(x\text{.}\)</span> Now, probability of outcome of an experiment falling in an infinitesimal range <span class="process-math">\(x \le X \le x+dx\)</span> will be</div>
<div class="displaymath process-math" id="eqn-probability-density">
\begin{equation}
P(x \le X \le x+dx) = \rho(x)\,dx.\tag{1.5.18}
\end{equation}
</div>
<div class="para">Then, probability in a range will just be an integral over that range</div>
<div class="displaymath process-math">
\begin{equation*}
P( 0.1 \le X \le 0.3 ) =  \int_{0.1}^{0.3}\, \rho(x)\,dx.
\end{equation*}
</div>
<div class="para">The probability density must be such that when integrated over all possible outcomes, i.e., the set <span class="process-math">\(\Omega\text{,}\)</span> it should give a value of 1. Thus in the case of outcomes being in the range <span class="process-math">\(0 \le X \le 1\text{,}\)</span> we require the following normalization condition.</div>
<div class="displaymath process-math" id="eqn-probability-density-normalized">
\begin{equation}
P( \Omega ) =  \int_{\text{full range}}\, \rho(x)\,dx = 1.\tag{1.5.19}
\end{equation}
</div>
<div class="para">You may have heard of a Gaussian probability density of a variable <span class="process-math">\(X\text{,}\)</span> whose range of possibilities is the entire real line, i.e., <span class="process-math">\(-\infty \lt X \lt +\infty\text{,}\)</span> the probability density  with mean <span class="process-math">\(\mu\)</span> and standard deviation <span class="process-math">\(\sigma\)</span> is</div>
<div class="displaymath process-math" id="eqn-gaussian-probability-density">
\begin{equation}
\rho(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\, e^{ -\frac{(x-\mu)^2}{2\sigma^2} },\tag{1.5.20}
\end{equation}
</div>
<div class="para">where the factor in front of the exponential makes sure that ir is properly normalized to give 1.</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsubsec-Probability-Density-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="autopermalink" data-description="Subsubsection 1.5.6.1: Probability Density"><a href="#subsubsec-Probability-Density" title="Copy heading and permalink for Subsubsection 1.5.6.1: Probability Density" aria-label="Copy heading and permalink for Subsubsection 1.5.6.1: Probability Density">ðŸ”—</a></div></section><section class="subsubsection" id="subsubsec-Cumulative-Distribution-Function"><h4 class="heading hide-type">
<span class="type">Subsubsection</span><span class="space"> </span><span class="codenumber">1.5.6.2</span><span class="space"> </span><span class="title">Cumulative Distribution Function (CDF)</span>
</h4>
<div class="para logical" id="subsubsec-Cumulative-Distribution-Function-2">
<div class="para">Since continuous variables take values on real axis, their values are ordered. Therefore, we define a very important property called cumulative distribution function, often just the distribution function or CDF, to be denoted by <span class="process-math">\(F(x)\)</span> by integrating the probability density function <span class="process-math">\(p(x)\)</span> from <span class="process-math">\(x=-\infty\)</span> to <span class="process-math">\(x\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-distribution-function">
\begin{equation}
F(x) = \int_{-\infty}^x\, \rho(x)\, dx\tag{1.5.21}
\end{equation}
</div>
<div class="para">Clearly, distribution function is probability of the event that the random variable <span class="process-math">\(X\)</span> is in the range <span class="process-math">\(-\infty \lt X \le x\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-distribution-func-as-prob">
\begin{equation}
F(x) = P( \{ -\infty \lt X \le x \} ) \equiv P( -\infty \lt X \le x ).\tag{1.5.22}
\end{equation}
</div>
<div class="para">It is readily seen that probability of <span class="process-math">\(X\)</span> over any finite range <span class="process-math">\(a \le X \le b\)</span> will simply be</div>
<div class="displaymath process-math" id="eqn-probability-over-a-range-and-">
\begin{equation}
P( a \le X \le b ) = F(b) - F(a).\tag{1.5.23}
\end{equation}
</div>
<div class="para">From complementarity of events <span class="process-math">\(\{  -\infty \lt X \le x \}\)</span> and  <span class="process-math">\(\{  X \gt x \}\text{,}\)</span> we immediately know that</div>
<div class="displaymath process-math" id="eqn-rpob-for-X-gt-x">
\begin{equation}
P( X &gt; x) = 1 - P( -\infty \lt X \le x ) = 1 - F(x).\tag{1.5.24}
\end{equation}
</div>
<div class="para">Using the fundamental theorem of Calculus, it is immediately obvious that probability density is just the detivative of the distribution function.</div>
<div class="displaymath process-math" id="eqn-prob-density-as-derivative">
\begin{equation}
\rho(x) = \frac{dF}{dx}.\tag{1.5.25}
\end{equation}
</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsubsec-Cumulative-Distribution-Function-2" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="para logical" id="subsubsec-Cumulative-Distribution-Function-3">
<div class="para">Cumulative distribution function of Gaussian distribution with mean <span class="process-math">\(\mu=0\)</span> and unit standard deviation <span class="process-math">\(\sigma=1\)</span> is highly used in interpretive statistics. There, it is often given a different symbol, <span class="process-math">\(\Phi(x)\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-distribution-function-Normal">
\begin{equation}
\Phi(x) = \int_{-\infty}^{x}\, \frac{1}{\sqrt{2}}\,\exp\left(-x^2/2\right)\, dx.\tag{1.5.26}
\end{equation}
</div>
<div class="para">From complementarity of events <span class="process-math">\(\{  -\infty \lt X \le x \}\)</span> and  <span class="process-math">\(\{  X \gt x \}\text{,}\)</span> it is clear that</div>
<div class="displaymath process-math">
\begin{equation*}
P( X &gt; x) = 1 - \Phi(x).
\end{equation*}
</div>
<div class="para">This is used in finding the remaining probability in the tail part of a Gaussian. For instance, in some application, we may want to know, how much probability is after <span class="process-math">\(x = 2.5\)</span> in a Gaussian variable <span class="process-math">\(X\text{.}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P( X &gt; 2.5) = 1 - \Phi(2.5).
\end{equation*}
</div>
<div class="para">These are easily computed by almost every stat package. For instance in Pythonâ€™s scipy library there is a powerful stats package.</div>
<div class="autopermalink" data-description="Paragraph"><a href="#subsubsec-Cumulative-Distribution-Function-3" title="Copy heading and permalink for Paragraph" aria-label="Copy heading and permalink for Paragraph">ðŸ”—</a></div>
</div>
<div class="code-box"><pre class="program clipboardable"><code class="language-py">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# === PARAMETERS OF THE NORMAL DISTRIBUTION ===
mu = 0       # mean
sigma = 1    # standard deviation
dist = norm(loc=mu, scale=sigma)

# === RANGE OF VALUES FOR PLOTS ===
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)

# === PDF AND CDF CALCULATION ===
pdf_values = dist.pdf(x)
cdf_values = dist.cdf(x)

# === PLOTTING ===
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# --- PDF plot ---
ax[0].plot(x, pdf_values, label=f'N({mu}, {sigma**2}) PDF', color='blue')
ax[0].set_title("Normal Distribution - PDF")
ax[0].set_xlabel("x")
ax[0].set_ylabel("Density")
ax[0].grid(True)
ax[0].legend()

# --- CDF plot ---
ax[1].plot(x, cdf_values, label=f'N({mu}, {sigma**2}) CDF', color='red')
ax[1].set_title("Normal Distribution - CDF")
ax[1].set_xlabel("x")
ax[1].set_ylabel("Cumulative Probability")
ax[1].grid(True)
ax[1].legend()

plt.tight_layout()
plt.show()

# === PROBABILITY FOR GIVEN x ===
x_value = 2.0 # 2 times the standard deviation which is 1 here
prob_less_than = dist.cdf(x_value)
prob_greater_than = 1 - prob_less_than
print(f"P(X \le {x_value}) = {prob_less_than:.4f}") // P(X \le 2.0) = 0.9772
print(f"P(X \gt {x_value}) = {prob_greater_than:.4f}") //P(X \gt 2.0) = 0.0228

# === x-value FOR GIVEN PROBABILITY ===
p_value = 0.99  # 99% quantile
x_at_p = dist.ppf(p_value)
print(f"x such that P(X \le, x) = {p_value} is x = {x_at_p:.4f}") // x such that P(X \lex) = 0.99 is x = 2.3263

# === PROBABILITY BETWEEN TWO VALUES ===
x_low, x_high = -1.0, 1.0
prob_between = dist.cdf(x_high) - dist.cdf(x_low)
print(f"P({x_low} \le X \le {x_high}) = {prob_between:.4f}")// P(-1.0 \le X \le 1.0) = 0.6827
</code></pre></div>
<figure class="figure figure-like" id="fig-psd-anf-cdf-of-normal-function"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/pdf_and_cdf_normal_distribution.png" class="contained" alt="PDF and CDF of Gaussian distribution."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.5.4<span class="period">.</span></span><span class="space"> </span>PDF and CDF of Gaussian distribution.<div class="autopermalink" data-description="Figure 1.5.4"><a href="#fig-psd-anf-cdf-of-normal-function" title="Copy heading and permalink for Figure 1.5.4" aria-label="Copy heading and permalink for Figure 1.5.4">ðŸ”—</a></div></figcaption></figure><div class="autopermalink" data-description="Subsubsection 1.5.6.2: Cumulative Distribution Function (CDF)"><a href="#subsubsec-Cumulative-Distribution-Function" title="Copy heading and permalink for Subsubsection 1.5.6.2: Cumulative Distribution Function (CDF)" aria-label="Copy heading and permalink for Subsubsection 1.5.6.2: Cumulative Distribution Function (CDF)">ðŸ”—</a></div></section><div class="autopermalink" data-description="Subsection 1.5.6: Complications-For-Continuous-Variables"><a href="#subsec-Complications-For-Continuous-Variables" title="Copy heading and permalink for Subsection 1.5.6: Complications-For-Continuous-Variables" aria-label="Copy heading and permalink for Subsection 1.5.6: Complications-For-Continuous-Variables">ðŸ”—</a></div></section><div class="autopermalink" data-description="Section 1.5: Joint, Conditional and Marginal Probabilities"><a href="#sec-Joint-Conditional-and-Marginal-Probabilities" title="Copy heading and permalink for Section 1.5: Joint, Conditional and Marginal Probabilities" aria-label="Copy heading and permalink for Section 1.5: Joint, Conditional and Marginal Probabilities">ðŸ”—</a></div></section></div>
<div id="ptx-content-footer" class="ptx-content-footer">
<a class="previous-button button" href="sec-Basic-Probability.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Top</span></a><a class="next-button button" href="sec-Example-Discrete-Probability-Distributions.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" height="100%" viewBox="338 3000 8772 6866" role="img"><title>PreTeXt logo</title><g style="stroke-width:.025in; stroke:currentColor; fill:none"><polyline points="472,3590 472,9732 " style="stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png" alt="Runstone Academy logo"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png" alt="MathJax logo"></a>
</div>
</body>
</html>
