var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "front-colophon",
  "level": "1",
  "url": "front-colophon.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": "  "
},
{
  "id": "sec-Descriptive-Statistics",
  "level": "1",
  "url": "sec-Descriptive-Statistics.html",
  "type": "Section",
  "number": "1.1",
  "title": "Descriptive Statistics",
  "body": " Descriptive Statistics  Descriptive statistics summarize and describe the main features of a dataset. These numbers are very useful for getting a first look at the patterns in the data and goes by the name Exploratory Data Analysis . The main numbers you would be interested in here are as follows.    Mean (Average) : The mean is the average of all data values. We will usually denote mean by the Greek symbol . So, if your data contains the five values . Then mean will be In general the formula fot a dataset containing values, , will be where I have used a common summation symbol for brevity, which stands for Note also that we are using superscripts to denote individual data points. We will try to use this notation in this book whenever appropriate.     Median : Median is the number below (or above) which 50% of the data resides; it does not have to be any of the data points in the data itself. To calculate median, you would first sort the data . Then, if the data has odd number of data points, then median is the middle data and if the data has even number of points, then median would be the average of the two middle numbers. Thus, suppose the data is , then you would first sort the data, giving you , then the median will be the middle number 3. Suppose you have the data . First sort it to . Now, we have two middle data points, that is 2 and 3. Median will be their average, 2.5.     Mode : Mode is the most frequently occurring value in the dataset. A dataset may have no mode, one mode, two mode, or any number of modes. For instance the dataset does not have a mode, has one mode equal to , the data has one mode equal to and the data has two modes, one at and the other at .     Variance : Variance measures how much the data varies from the mean. The square root of variance is called standard deviation , which is usually denoted by the symbol (pronounced sigma). The variance, itself is denoted by just squaring this symbol, viz., . Suppose we have a dataset containing values, , whose mean is , whose formula is given above in Eq. , then variance will be Let's calculate variance of dataset . We first calculate mean. Then, variance Finally, we can find the standard deviation. Note that the unit of is same as that of the data points themselves, but that of variance will be square of the units of the data points. Thus, if the data were height in , then will be in , but variance will be in .  Standard deviation has considerable importance in understanding data that are nearly Gaussian in their distribution since that distribution is fully specified by only its mean and the standard deviation. Considerable interpretive statistics is based on the data that are distributed approximately Gaussian. We will study this aspect of statistics in a late section.     Histogram : A histogram refers to a frequency distribution of values. It is usually presented as a plot with bins in which data may be distributed. Suppose you have data, whose values denoted by letter , in the range from to . Then, with a bin size of , you can distribute the data, with minimum value and maximum value in bins: , , , , , . Suppose, you have the following dataset Then, you will get the table showing how the data is distributed across the bins.   Histogram    Bin  Range  Data Count Frequency    1  1 0.1    2  4 0.4    3  0 0    4  3 0.3    5  1 0.1    6  1 0.1     Graphically, you would plot the frequency column against the Bin or the Range column. The following Python Code will produce the plot given in . The visual picture obviously gives you a very direct way to see the pattern in the data, including a coarse view of the probability distribution of various values in the data.  import matplotlib.pyplot as plt import numpy as np # Data data = [0, 2, 2, 3, 3, 6, 7, 7, 8, 10] # Desired bin size bin_size = 2 def plot_histogram(data, bin_size=2): # Calculate bin edges min_val = min(data) max_val = max(data) bins = np.arange(min_val, max_val + bin_size, bin_size) # Plotting the histogram plt.hist(data, bins=bins, edgecolor='black', alpha=0.7) # Adding labels and title plt.xlabel('Value') plt.ylabel('Frequency') plt.title(f'Histogram of Data with Bin Size = {bin_size}') # Display the plot plt.xticks(bins) # Optional: to show bin edges as x-axis ticks plt.grid(axis='y', alpha=0.75) # Optional: add a grid for better readability plt.show() return None plot_histogram(data, bin_size)   Histogram with data in range 1 to 10 with bin size 2.   Display of a histogram with data in range 1 to 10 with bin size 2.       Range and Quartiles : First, we sort the data. Range of data is simply Max minus Min of the data. It gives a measure of how widely do the values in the data lie. For quartiles, we then divide data into 4 parts. The first quartile (Q1), also known as the lower quartile, represents the 25th percentile, meaning 25% of the data falls below this value. The second quartile (Q2), also known as the median, represents the 50th percentile, meaning 50% of the data falls below this value. The third quartile (Q3), also known as the upper quartile, represents the 75th percentile, meaning 75% of the data falls below this value. Finally, the fourth quartile (Q4) represents the top 25% of the values of the dataset. for simplicity, suppose the sorted dataset contains Then , , , and .  A boxplot or a box-and-whisker plot is a compact way to display the min, quartiles and the max. We draw a box that spans the range from Q1 to Q3, called the interquartile (IQR) showing the middle of the data. By using IQR with Q1 and Q3, we also define thresholds for spotting outliers in the data. The outliers are shown as points on boxplots as shown in .  Example: Suppose we have the following dataset representing student scores on a quiz: In this data, we can conclude: Median (Q2) = (75 + 80)\/2 = 77.5, Q1 = median of lower half = 60, Q3 = median of upper half = 90. The IQR for the box will be IQR = Q3-Q1 = 30. We also can notice outliers in the data by calculating the lower bound for the data from:  Here, we see that the data of 150 is an outlier.  The following Python program will generate a good looking boxplot of the data. import matplotlib.pyplot as plt import numpy as np # Sample data scores = [20, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 120, 150] def gen_boxplot(data): # Create the boxplot using matplotlib plt.figure(figsize=(8, 4)) plt.boxplot(data, vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'), medianprops=dict(color='red')) plt.title('Boxplot of Student Scores') plt.xlabel('Score') plt.grid(True) plt.savefig(\"boxplot.png\") plt.show() #----------- gen_boxplot(scores)   Illustration of a Boxplot. The box shows the middle 50% of the data and the outliers are shown as point, here the point at 150 is the only outlier. The net range, other than outlier is shown between the two lines.   Display of a boxplot with one outlier.        "
},
{
  "id": "tab-Histogram-Table",
  "level": "2",
  "url": "sec-Descriptive-Statistics.html#tab-Histogram-Table",
  "type": "Table",
  "number": "1.1.1",
  "title": "Histogram",
  "body": " Histogram    Bin  Range  Data Count Frequency    1  1 0.1    2  4 0.4    3  0 0    4  3 0.3    5  1 0.1    6  1 0.1    "
},
{
  "id": "fig-descriptive-statistics-histogram",
  "level": "2",
  "url": "sec-Descriptive-Statistics.html#fig-descriptive-statistics-histogram",
  "type": "Figure",
  "number": "1.1.2",
  "title": "",
  "body": " Histogram with data in range 1 to 10 with bin size 2.   Display of a histogram with data in range 1 to 10 with bin size 2.   "
},
{
  "id": "fig-descriptive-statistics-boxplots",
  "level": "2",
  "url": "sec-Descriptive-Statistics.html#fig-descriptive-statistics-boxplots",
  "type": "Figure",
  "number": "1.1.3",
  "title": "",
  "body": " Illustration of a Boxplot. The box shows the middle 50% of the data and the outliers are shown as point, here the point at 150 is the only outlier. The net range, other than outlier is shown between the two lines.   Display of a boxplot with one outlier.   "
},
{
  "id": "sec-useful-descriptive-statistics-tools",
  "level": "1",
  "url": "sec-useful-descriptive-statistics-tools.html",
  "type": "Section",
  "number": "1.2",
  "title": "Useful Descriptive Statistics Tools",
  "body": " Useful Descriptive Statistics Tools   Here, I want provide a list of data analysis tools that have emerged as important resources for data scientists and can be very useful for machine learning engineer. Among many resources, let us look at just a few important ones.    Pandas   Pandas is a powerful Python library for data manipulation and analysis. It provides two core data structures:    Series : A one-dimensional labeled array.     DataFrame : A two-dimensional labeled table (think Excel or SQL table in Python).     Pandas is designed for cleaning , transforming , analyzing , and visualizing data efficiently. Whether you're exploring a dataset, preprocessing features for machine learning, or analyzing business metrics, Pandas will likely be your go-to tool.    Why Use Pandas?     Handles structured data easily.    Works well with CSV, Excel, JSON, SQL, and more.    Integrates seamlessly with NumPy, Matplotlib, Scikit-learn, and others.    Fast, expressive, and readable.     A Simple Example : Let's walk through a simple example of a data that is presented to us in a table form with each column referring to one property or feature or variables in the data. Each row is one datapoint, with values of different variables across the columns. You can feed that data as a Python dictionay or a csv file or an excel file. Here, we keep it simple and feed the data as a dictionary.   import pandas as pd data = { 'Name': ['Alice', 'Bob', 'Carol', 'Dave'], 'Age': [25, 30, 27, 22], 'Score': [85.5, 90.0, 88.0, 76.5] } df = pd.DataFrame(data) print(df)   This will display the data in a nice table format as in    Printout of the DataFrame df     Name  Age  Score    0  Alice  25  85.5    1  Bob  30  90.0    2  Carol  27  88.0    3  Dave  22  76.5     A great advantage of Pandas DataFrame is that it comes with numerous methods that you can call on a DataFrame to perform various tasks. I will present only a small sample of them and you can go to the Pandas website to expand your knowledge.  One thing you can do is to examine data quickly by following commands:   df.head(n) : diplay n data rows at the top.    df.tail(m) : display m data rows from the bottom.    df.shape : display the dimensions of the DataFrame; note here you do not have () at the end.    df.columns : names of columns of the DataFrame as a list of strings.    df.info() : Summary of data types and memory usage.    df.describe() : Basic statistics for numeric columns.     When I run df.describe() on the DataFrame df we set up above, it produces the descriptive statics of the numerical data, i.e., columns Age and Score.   Output of df.describe()     Age  Score    count  4.000000  4.000000    mean  26.000000  85.000000    std  3.366502  5.958188    min  22.000000  76.500000    25%  24.250000  83.250000    50%  26.000000  86.750000    75%  27.750000  88.500000    max  30.000000  90.000000     Perhaps the strongest case for using Pandas is its ability to manipulate and transform data very rapidly with simple commands which can be piped into each other. Let's see some examples of this power of Pandas.  You can select one column of interest. Or select multiple columns by passing a list of the names of those columns as a list.  score = df['Score']  age_score = df[['Age', 'Score']]   You can filter data based on boolean conditions on the variables.  df1 = df[df['Age'] > 25]  df2 = df[(df['Age'] > 25) & (df['Score'] > 85)] You can create columns for new variables, e.g. Passed, which has 0 or 1 entry based on the values in the Score column. df['Passed'] = df['Score'] >= 80 Of course sort data based on a particular row. df.sort_values(by='Score', ascending=False) >  Here I am trying to give you a sense of the power and flexibility of the Pandas library. A great book to learn this subject is through two free books. Python for Data Analysis by Wes McKinney (the creator of Pandas) at the website and Learning Pandas by Michael Heydt at the website . Additionally, Kaggle offers a free, comprehensive introductory course on Pandas at this website .   Here is a list of useful functions in Pandas. Grouping and Aggregation: grouped = df.groupby('Passed')['Score'].mean(), which groups the data by variable 'Passed' and computes the average score. Handling Missing Data: (1) is_null_mask = df.isnull() ceates a boolean mask of missing values in the data, (2) df.dropna(), which remove rows with missing values, (3) df.fillna(0), which replace missing values with 0 or a value of your choice. Reading\/Writing Files : (1) df = pd.read_csv('data.csv') for reading from a CSV file, (2) df.to_excel('output.xlsx') for writing the DataFrame to an Excel file, (3) df.to_json('output.json') for writing the DataFrame as a JSON file.    Conclusion: Pandas simplifies the data journey: from raw files → clean tables → insightful summaries → model-ready datasets. You don't need to memorize everything. Learn the core workflow: Load data, Inspect and clean, Transform and explore, Export results or feed into models. Once you're comfortable with these steps, you’ll find Pandas becomes second nature.     There are of course many other tools for data cleaning and transformation. For instance, TensorFlow has a very powerful data paipeline that address the transformations of data ad emphasizes consistency across the entire machine learning training and presentation cycle. But, that is a bit too much to master in the beginning. It is best to first start with Pandas or similar easy to use tool. The references given above have many notebook examples for you to work through and develop your own skills. For many years, R programming language, and maybe enven now, R is a go to language for data analysts. But, since we are emphasizing the tools more common in machine learning community, we will not have much to say about R.   "
},
{
  "id": "tab-printout-of-dataframe-as-table",
  "level": "2",
  "url": "sec-useful-descriptive-statistics-tools.html#tab-printout-of-dataframe-as-table",
  "type": "Table",
  "number": "1.2.1",
  "title": "Printout of the DataFrame df",
  "body": " Printout of the DataFrame df     Name  Age  Score    0  Alice  25  85.5    1  Bob  30  90.0    2  Carol  27  88.0    3  Dave  22  76.5    "
},
{
  "id": "subsec-Pandas-12",
  "level": "2",
  "url": "sec-useful-descriptive-statistics-tools.html#subsec-Pandas-12",
  "type": "Table",
  "number": "1.2.2",
  "title": "Output of df.describe()",
  "body": " Output of df.describe()     Age  Score    count  4.000000  4.000000    mean  26.000000  85.000000    std  3.366502  5.958188    min  22.000000  76.500000    25%  24.250000  83.250000    50%  26.000000  86.750000    75%  27.750000  88.500000    max  30.000000  90.000000    "
},
{
  "id": "sec-Numerical-and-Categorical-Data",
  "level": "1",
  "url": "sec-Numerical-and-Categorical-Data.html",
  "type": "Section",
  "number": "1.3",
  "title": "Numerical and Categorical Data",
  "body": " Numerical and Categorical Data   When analyzing data, it is important to know what kind of data you are working with. There are three key data types:     Categorical Data: This type of data represents discrete groups or labels with no inherent order between the values. For example you may have a variable named 'Color', which can take values from the set {\"red\", \"blue\", \"green\"}. Another variable may be 'Animal' with value from the set {\"cat\", \"dog\", \"parrot\"}. In Pandas, you can store as object type or convert to category for efficiency and clarity. For instance,  colors = pd.Series([\"red\", \"blue\", \"red\", \"green\"], dtype=\"category\")  Many ML algorithms require numerical representation of these symbolic\/qualitative values of a categorical variable. A very popular way to represent them is by a one-hot vector.   One hot vector: To get a one hot representation of a categorical variable, suppose there are C different values of a categorial value, e.g., for the 'Color' variable above, we have C=3. Now, we represent ech color by a 4-dimensional vector with 1 at only one entry and zero at other places. Thus That is each value is now represented by a C-dimensional vector. If C becomes large, such as words in a language, it is better to represent each word by a more dense embedding, which we will learn in due course of time.     Ordinal Data: This is a type of categorical data with a defined order. For example, \"small\" < \"medium\" < \"large\" or \"low\" < \"medium\" < \"high\". The order matters, but the difference between categories is not necessarily meaningful. You can define an ordered category in Pandas as follows. sizes = pd.Series([\"medium\", \"small\", \"large\", \"small\"], dtype=pd.CategoricalDtype(categories=[\"small\", \"medium\", \"large\"], ordered=True)) For analysis and processing in machine learning, ordinal numbers have to be mapped to integers. But, if you do it naively, you will mislead the algorithm in learning something wrong about your data. For instance, suppose you have the variable called \"Education_Level\" with order: High School < Bachelor's < Master's < PhD. If you naively map High School 1; Bachelor 2, Master's 3, PhD 4, you would be implying that difference between these levels is 1 for all of them. In reality, the meaning of that step is qualitative, not quantitative.  Despite this difficulties with mapping ordinal numbers, we must find a reasonable way to cast ordinal numbers numerically for many ML algorithms. Ordinal-to-numeric mapping allows models to recognize ordering, but we must avoid letting the model misinterpret the numbers as proportional differences. there are some safe strategies to do that.   How to Map Ordinals Safely:   1. Integer Encoding (Simple Mapping) Use integers to represent order but remember they’re order markers, not measures. Example: education_order = {\"High School\": 1, \"Bachelor's\": 2, \"Master's\": 3, \"PhD\": 4} df['Education_Level_Code'] = df['Education_Level'].map(education_order) This works for tree-based models (e.g., Random Forest, XGBoost) because they split data based on order, not arithmetic differences.  2. Custom Binning or Grouping For wide ranges of categories, group into fewer meaningful bins before mapping.  3. Use of Embeddings In deep learning, treat ordinal categories like tokens and learn their position in a multidimensional space. This captures ordering without assuming equal numeric gaps.  4. Don't One-Hot Encode Ordinal Data One-hot removes the order information entirely. Use it only when the “order” is not meaningful.     Numerical Data: A numerical variable represents quantities that can be measured and compared numerically. The values can be discrete , i.e., countable items such as number of rooms in a house, or continuous , which are measurable quantities with potentially infinite precision (e.g., height, weight, etc). In most algorithms, every variable needs to be cast in some numerical representation for the algorithm to perform calculations on the variables.     "
},
{
  "id": "sec-Basic-Probability",
  "level": "1",
  "url": "sec-Basic-Probability.html",
  "type": "Section",
  "number": "1.4",
  "title": "Basic Probability",
  "body": " Basic Probability   Probability is essential in machine learning to model randomness in data and outcomes. Probability tells us how likely an event is to happen. An event in the context of probability is a description of an outcome of an experiment and is usually written as a set. For instance, if you toss a coin, the outcome Head is an event , Tail is an event , and Either Head Or Tail is the event . When you list multiple elements in an event, all it means that any of those outcomes will constitute this event having occurred.   Every trial of an experiment produces a binary answer yes\/no to the question whether a particular event is the outcome of this trial. For instance, for a roll of a six-sided die suppose you got 2. That would mean any of the following events occurred: , , , , , >m>\\{2,6\\}>\/m>, , , etc.,i.e., all the events that contains in it  An advantage of using sets to represent events is that you can use union and intersections, to build more complex events. For instance event , which must always be true since it represents the event that in a toss, you got either or and those are the only two possibilities.    Axiomatic View of Probability  Andrey Kolmogorov laid the foundations of the subject of probability in 1933 with three fundamental axioms. Although these axioms are fundamental to understanding probability from basic mathematics, we will rarely work in the framework that we will study in this section. First we need definition of some terms in probability space.   Sample Space : The sample space  is the set of all unique outcomes in the experiment of interest. For instance, in the case of a single roll of a six-sided die, you can get only . Therefore, set . For a coin toss, , where is Heads and is Tails.   Event Space : Event Space be the set of all subsets of , including the empty set, . Each event consists of none, one or any number of elementary outcomes listed in and represents all outcomes that would constitute that event if any of those was the result of that trial. Thus, if an event in a six-sided die roll is and if you roll the die and get a or , you will record that event occurred and if neither showed up, i.e., you got , then event dis not occur in that trial.  If has elements, then, will have elements. In the case of the six-sided die, the number of elements in will be . That's too large to list in example here.  Let's look at another experiment: tossing a coin with two possible outcomes in any single toss. So, , where is head and is tail. The subsets will be . All events are sets of elementary outcomes in the experiment! Thus, we can talk about union and intersection of events. For instance,   Notice that and will always be in . The empty set is included for completeness in mathematical calculations and here means either or event, means an event in which it's a head and is an event of tail. Note: you can't have an event in which a single toss will be head AND tail. That is why you don't have that since and are unique exclusive outcomes.   What about two tosses of the same coin? Now, each element of the sample space will be outcomes of the two tosses. Writing the order one next to the other, we can list four possible outcomes: This sample space has elements. Therefore, it will have subsets, each specifying an event. To complete the picture, we also need a Probability Measure : that assigns a real number to each event of the event space . Let us work out for the two toss of a single coin problem we have above.  If the coin was a fair coin, the four elementary outcomes in will all be equally likely, givin probability of the elementary events. Probabilities of other events in. are based on the third axiom given below since they are all (except for the event) just union of the elementary events. Thus, What about the probability of the empty event ? This event is complement of event , i., an event that says any of the possible outcomes of the experiment. So, the probability of the null event is zero.   The trio is called the Probability Space . The three axioms of the probability theory are:    First Axiom: Non-negativity      Second Axiom: Normalization That is, in every trial, there is 100% certainty that one of the elementary events will be the outcome. Together with the first axiom, this implies that probability of any event has to be between and , inclusive.      Third Axiom: Additivity of probabilities of disjoint, i.e., mutually exclusive events.      From these axioms, we can show the following useful results. The last one can be easily seen by drawing a ven diagram with events and overlapping as shown in .   Venn Diagram showing two events and that have an overlap shown by , the intersection of them. When you combine the two events by their union, you need to exclude their intersection since they are counted twice, once in and second time in .   Venn Diagram showing two events E1 and E2 that have an overlap shown by their intersection in the set notation.    . A concrete example may help here. Suppose you have a six-sided die. Let and . The union of the two will be and the intersection will be . Note that the list of faces in is saying that the roll will be either , , or . Similarly for . Then, we have The probability of the union of the two sets is only and the sum of the probabilities of the two events is Thus, we must subtract the probability of the intersection to remove the double counting.    Three Types of Probabilities  There are basically three ways of looking at probability:    Theoretical or Classical Probability . It is based on making use of symmetry and calculating the possible outcomes in an experiment. For instance, if you have a six-sided die, which is not loaded to prefer one outcome or another, the chance of any one face showing up will be . So, we can say that probability of any face as an outcome of a single roll is .     Frequentist Probability . It is an empirical definition of probability by observation of repeated trials of the same experiment. Thus, in the case of a six-sided die, you will roll the die and observe how many times face with 1 dot showed up in how many rolls, each roll of the die being one trial of the experiment. Thus, if of trials had 1 dot face up. Then, we conclude that the ratio is an approximation of the true probability of face 1 in any roll. We say that, in the limit of infinitely many trials, we would get the \"true\" probability. We do not assume that probabilities of every face of the die is same, as we did in using the symmetry argument in the theoretical probability; we rely of the repeated trials to show us any differences among the faces.     Bayesian Probability . This is also an empirical definition of probability. But, rather than give you one number for probability of an event, Bayesian gives you a probability distribution of the values of probability of the event. From that, you can work out the mean value, which you can use as one value for the probability of the event.  It is based on incorporating belief about the probability of an outcome BEFORE we even conduct the experiment and then updated this so-called prior assumption or bias with what we observe in the experiment. The updated belief is the posterior, and improved value of the probability.  Clearly, as we repeat the experiment infinitely many times, the effect of our initial belief would disappear and the answer will match the results of the frequentists' experiments. However, since we can never do infinite number of trials, the Bayesian gives an edge in cases where we have some information about the outcome even before we start the trials.   Example: This example is a little bit ahead of my presentation here as it requres a little bit of math to properly express how th Bayesian probability works. If you feel up to it, you can ahead and read on, but it's okay to skip it for now.  In the case of a six-sided die, suppose we want to estimate the probability for one-dot face up as we illustrated in the frequentist case above. First, we would need to choose a prior belief, i.e., a probability distribution for , i.e., how likely is any value of between its range of values, which will be from to , inclusive, . Since, we do not know which value is right, we might decide that it could be 1\/2 times it will be face up and 1\/2 of the time it will be not face up (I know a fair die will be 1\/6 times face up, but I want to show you how even a very off prior will eventually converge to the proper value). In such cases and our trial each time being either face up true or false ( which is a case of Bernoulli trials ), it is traditional to choose a beta distribution, which has two parameters and , with and . Using symbol for and , probability density of , we will write this as follows where . where is beta function. The mean value of beta distribution is an important result and can be easily found. Here, I have introduced physicists' notation for the mean of a quantity, . Thus, by choosing as the prior distribution, we are assuming that somehow we suspect that is close to . So, we are basically, starting way off in our belief.  Just a side math info: Beta function is usually written in terms of factorial or Gamma function. where, for integer arguments , and in general, To hide all the mathematical details in our work below, we will, as is normally done, just express the probability by a simpler notation and represent Eq.  where instead of lower case variable name , we use the notation of upper case .  Let's get back to our rolling experiment and see how our belief of the true value of evolves with each roll's result. Suppose we roll the die and observe that the up face is not one, then without showing you the calculations here, which will be done later in the chapter, we use Bayes rule, to be discussed later, to show that the probability distribution now shifts to . How did we go from distribution to ? I used Bayesian theorem. We will not show the calculation here but differ to a later section.  Toss second time, let's say the result is a one. Then our belief will be update with this new data to . Toss again, say no one. We keep updating the probability distribution of . At any point, we can take the expectation value of the the variable in the current distribution to give us the \"best current value\" for . Thus, after three trials above, we will say that .  Suppose you continued rolling and you had the following next 7 trials: After these 10 trials in total, the distribution will be This is still far away from that you would expect from a fair die, but you don't know if the die was fair. So, empirical results are all you have to go by.  For the same rolling results, frequentists' probability will give us the following estimate:   They look similar. But, had you expected the die was fair, you would start with a better prior, with say . Then the 10 trials would update to It would have revealed if the die was not a fair die. It's either not a fair die or we have rolled it too few times.      "
},
{
  "id": "fig-venn-diagram-E1-E2",
  "level": "2",
  "url": "sec-Basic-Probability.html#fig-venn-diagram-E1-E2",
  "type": "Figure",
  "number": "1.4.1",
  "title": "",
  "body": " Venn Diagram showing two events and that have an overlap shown by , the intersection of them. When you combine the two events by their union, you need to exclude their intersection since they are counted twice, once in and second time in .   Venn Diagram showing two events E1 and E2 that have an overlap shown by their intersection in the set notation.   "
},
{
  "id": "sec-Joint-Conditional-and-Marginal-Probabilities",
  "level": "1",
  "url": "sec-Joint-Conditional-and-Marginal-Probabilities.html",
  "type": "Section",
  "number": "1.5",
  "title": "Joint, Conditional and Marginal Probabilities",
  "body": " Joint, Conditional and Marginal Probabilities   In this section, we address probabilities in situation when you have more than one variable. Whether they are independent or not independent makes a difference. Let's first define a variable in the context of probability and statistics.    Random Variables, Probabilities, and Expections  We will think of variables as something the is observed or measured by experiments. The outcome in any experiment is a real value of the variable. A variable whose value is uncertain or unpredictable or varies from trial to trial, even though measurement conditions haven't changed, is called a random variable.  We tend to use capital letter for the name of the variable and small letters for its values. Thus, for a variable , the values will be denoted by etc. Sometimes we will use superscripts to denote values, . An event will now refer to the outcome that in a particular trial, variable has some value or a set of the possible values. Thus, would be an event and so would be , etc. In case of continuous values for , and event may even be written as , etc. We will speak about probabilities of these events in a later section.  In our practice, the values of random variables will all be real numbers. Now, if can take any real number, whether the entire real line or a finite segment of the real line, then we will call such a variable a continuous variable . An example of continuous variable will be price of a house, if the price cannot be more than . On the other hand, if the variable takes on only discrete values, then it will be called categorical or discrete variable . An example of discrete variable will fruits of interest in some grocery store which has five categories.  The probability measure of a probability space of a categorical variable has to specify probabilities of each of the elements of finite set . Let there be distinct values of of a categorical variable , say, . Then, all we need to specify are the numbers for each unique\/exclusive outcome. Note that due to normalization, the probability of event , will be 1. This probability measure is called probability mass function or . Given a PMF, it is easy to find the mean value of the random variable , which will be denoted by angle brackets as per physics notation. The process of taking mean shows that it just sum of values of weighted according to their probabilities. This weighting according to probabilities is called taking expectation . Thus, expectation value of any function is obtained accordingly. The variance will be expectation value of another variable obtained by subtracting the mean from the random variable and squaring that. This gives us a measure of the spread of values about the mean. where is the variable but is just a number. The following calculation will relate variance to and . The first term is just expectation value of variable , i.e., . Therefore, we often see variance in the following simpler-looking formula. The standard deviation will, of course be, just square root of variance.     Joint ad Marginal Probabilities   Suppose you have two discrete random variables, e.g, does the patient have a disease and is the diagnosis positive . Both of these variables have just two values: and    Joint probability is probability measure over the outcome space that has all the combinations of the elements of and . That is, we will specify probability measure for each of the elements of following  That is we need the following probabilities. These probabilities are joint probabilities over the joint space of and . We often write joint probability by just listing the variable names or even a symbol for variable values, e.g., , without specifying    If you ignore and just look at the probabilities of patient having the disease and not having the disease, they are called Marginal Probabilities of . If you had more than two variables, say , you would just ignore all the other variables to focus only on to obtain its marginal probabilities. Similar to the notation of joint probabilities, we often write the marginal probabilities by simply or .  It turns out that the marginal probabilities can be obtained from joint probabilities by just summing out the other variable's values in the joint probability. Thus, in our example, the p_1 and p_2 of alone will be Same will be the case if you are interested in the marginal probability of .    Covariance and Correlation  A very common aspect of dealing with more than one random variable, say (e.g., height of a man) and (e.g., the weight of the man) is to find out to what extent they tend to vary together. Covariance is a measure of their varying together either in the same direction or in the opposite direction. The normalized version of covariance so that result lies between and is called correlation .  Let denote the joint probability of and . Then, covariance is the following expectation value computed in this probability distribution. where to keep the formula simpler, the mean values of and are dented by and respectively. By opening the braces inside the angle brackets in Eq. , we can rewrite the Covariance formula in another way. where is to be computed using . For and discrete variable and a PMF, the calculation will be Themean values and in this case would relate to the marginals and  Covariance can take any real value, . By dividing the covariance by the standard deviations of and we get correlations, whose values are in the range . The standard deviations in the joint distribution are the same as the standard deviation in the marginal as was the case for the means and . For instance, in the case of discrete random variables, we will get the following for the variances, which is the square of standard deviations. The positive correlation (or covariance) means vary in the same direction, i.e., increasing and increasing occur together. The opposite is the case for negative correlation (or covariance).     Conditional Probability  Conditional probability is a little tricky and is the most used in Machine Learning. Conditional probability is denoted by . But, this is too abstract a notation. I will denote it by to be a conditional probability of given that the random variable HAS A PARTICULAR VALUE .  It is very important to notice here that IS A PROBABILITY OF X, just under some condition. That is, in the joint space of and , you will collect all the points that correspond to a particular value of , say . Now, looking at only those, what can you say about the chances for different values of . Clearly, conditional probability is trying to capture using some knowledge about the world that comes after you have known something about the world and thus, reducing the uncertainty.    Let us look at a numerical example: Suppose 1000 patients presented symptoms of a disease A. All patients were given a test. Only 200 of the tests came out positive. When later on, we find out which of the patients had the disease and whether test came out positive or not, we construct a table given below. Use it to figure out joint probabilities, marginal probabilities, and conditional probabilities.  Before, we start writing formulas, it is best to pick a simple notation to reresent various events.    Patient Data                      For Joint probabilities, we can just divide number in each square by the total number of patients, which here. By dividing the totals of rows and , we will get the marginals and . By dividing the column totals, we get the marginals and .  Joint Probabilities                      We use conditional probability answer questions like: what is the probability of a patient having the disease if he has tested negative? When we look at , look at only those cases where people test negative - that is, look at numbers in the 'Test Negative' column in the table. We work out other conditional probabilities similarly.  Patients that tested negative    D 50    N 500    Now, we have a total of cases of which have disease. So, the conditional probability of a patient having the disease even when he has test negative will be That is in chance he has disease. Try to answer the question: what if a patient has tested positive, what is the probability that he doesn't have the disease, i.e., ? Ans: .  You will have eight conditional probabilities in our simple example here: Try computing all of them! Here, in each row, you have the complementary event, hence you have to compute only four of them. It's important to recognize which event are complementary. For instance, and are complimentary, but and are not! That is because conditional probability is a probability over the space and not over the space, although the value of is important is choosing the slice of the joint probability. That is why, I like the notation even though it is cumbersome to write.     Bayes' Rule  Before we discuss Bayes' rule, let's find the relation between Conditional Probability and Joint Probability. Recall that joint probability of two variables and is probability over the space of -space, but the conditional probability of given , is over -space on a lice of with , where is a particular value of .  By product rule of probabilities, it's clear that You can write this as Often, we write this relation in a more general language rather then event by event. The marginal can, of course be obtained from the joint probability by marginalizing , meaning summing over all .   Notice that on the left side of Eq. , the order of listing of and values is arbitrary, i.e., Thus, it is equally possible to work with a given and the associated conditional probability of , i.e., and the marginal probability . This will give us From Eqs. and , we get the following relation. Writing this in more general notation, we get the relation that is known as Bayes' theorem or Bayes' Rule . It has become very important in the age of ML since many algorithms rely on it.  Let us see an example of the application of Bayes' Rule . Suppose, in a population of women in the age 30 to 40 develop breast cancer. It's also known that mammogram identifies of cancers accurately and misses of them. This means that if a woman has breast cancer, the test will be positive of the time. Furthermore, the test also give negative values of the time correctly, i.e., if a woman doesn't have cancer, the test would came out negative of the times.  Now, a new woman in that age group comes in the lab and unfortunately, she is tested positive. So, what would you say about here chances of actually having the cancer? To answer this question, let us first introduce notations to simplify our formulas.  Let Before, her mammogram, you would say that probability that she has cancer is just since you don't know anything about her case other than her age group and you are just using the general knowledge. This is called prior in the context of Bayes' rule. This clearly says that the probability of the complement of Cancer is No Cancer. So, From the description, we also know the conditional probability that if a woman has breast cancer, her probability of test positive is . We have one more information in the data provided. Now, we use Bayes' rule: where Let us use the numerical values now. Therefore, our desired conditional probability will be That is .  My example above came from the following good set of example problems from University of Pennsylvania website, where you can find other examples to practice. Practice Bayes Problems .    Independent Variables  Two random variables are said to be independent if their joint probability factors in the product of their marginal probabilities. Keep in mind that behind the scene, probabilities in this equation are over events, i.e., it's for having some particular value and having its own particular value. If and are independent variables, we aexpect We wouldn't write our equations in the verbose manner, prefering to keep it simple. But, beware that probabilities are probabilities of events! Now, we write the left side of Eq. using marginal and conditional probabilities. Canceling from both sides we get That is knowing something about does not tell you anything about probability of . That is, conditioning on is useless when and are independent.    Complications-For-Continuous-Variables   Probability Density  In our discussions above, we spoke about probability space of a random variable and spoke of probability of , viz. . This works out nicely if has only a finite number of discrete values, e.g., it is categorical variable. In that case we call a probability mass function . However, when is a continuous variable, say , then there are infinitely many values that can take. In that case, probability for any one of the values will be zero. For instance, probability that will be 0 So, it is not a useful concept. We replace this probability with probability of in a range, e.g. To help with computing such probabilities, we introduce a concept of probability density , which will be probability per unit of . Now, probability of outcome of an experiment falling in an infinitesimal range will be Then, probability in a range will just be an integral over that range The probability density must be such that when integrated over all possible outcomes, i.e., the set , it should give a value of 1. Thus in the case of outcomes being in the range , we require the following normalization condition. You may have heard of a Gaussian probability density of a variable , whose range of possibilities is the entire real line, i.e., , the probability density with mean and standard deviation is where the factor in front of the exponential makes sure that ir is properly normalized to give 1.    Cumulative Distribution Function (CDF)  Since continuous variables take values on real axis, their values are ordered. Therefore, we define a very important property called cumulative distribution function, often just the distribution function or CDF, to be denoted by by integrating the probability density function from to . Clearly, distribution function is probability of the event that the random variable is in the range . It is readily seen that probability of over any finite range will simply be From complementarity of events and , we immediately know that Using the fundamental theorem of Calculus, it is immediately obvious that probability density is just the detivative of the distribution function.   Cumulative distribution function of Gaussian distribution with mean and unit standard deviation is highly used in interpretive statistics. There, it is often given a different symbol, . From complementarity of events and , it is clear that This is used in finding the remaining probability in the tail part of a Gaussian. For instance, in some application, we may want to know, how much probability is after in a Gaussian variable . These are easily computed by almost every stat package. For instance in Python's scipy library there is a powerful stats package.   import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm # === PARAMETERS OF THE NORMAL DISTRIBUTION === mu = 0 # mean sigma = 1 # standard deviation dist = norm(loc=mu, scale=sigma) # === RANGE OF VALUES FOR PLOTS === x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000) # === PDF AND CDF CALCULATION === pdf_values = dist.pdf(x) cdf_values = dist.cdf(x) # === PLOTTING === fig, ax = plt.subplots(1, 2, figsize=(12, 5)) # --- PDF plot --- ax[0].plot(x, pdf_values, label=f'N({mu}, {sigma**2}) PDF', color='blue') ax[0].set_title(\"Normal Distribution - PDF\") ax[0].set_xlabel(\"x\") ax[0].set_ylabel(\"Density\") ax[0].grid(True) ax[0].legend() # --- CDF plot --- ax[1].plot(x, cdf_values, label=f'N({mu}, {sigma**2}) CDF', color='red') ax[1].set_title(\"Normal Distribution - CDF\") ax[1].set_xlabel(\"x\") ax[1].set_ylabel(\"Cumulative Probability\") ax[1].grid(True) ax[1].legend() plt.tight_layout() plt.show() # === PROBABILITY FOR GIVEN x === x_value = 2.0 # 2 times the standard deviation which is 1 here prob_less_than = dist.cdf(x_value) prob_greater_than = 1 - prob_less_than print(f\"P(X {x_value}) = {prob_less_than:.4f}\") \/\/ P(X 2.0) = 0.9772 print(f\"P(X {x_value}) = {prob_greater_than:.4f}\") \/\/P(X 2.0) = 0.0228 # === x-value FOR GIVEN PROBABILITY === p_value = 0.99 # 99% quantile x_at_p = dist.ppf(p_value) print(f\"x such that P(X x) = {p_value} is x = {x_at_p:.4f}\") \/\/ x such that P(X x) = 0.99 is x = 2.3263 # === PROBABILITY BETWEEN TWO VALUES === x_low, x_high = -1.0, 1.0 prob_between = dist.cdf(x_high) - dist.cdf(x_low) print(f\"P({x_low} X {x_high}) = {prob_between:.4f}\")\/\/ P(-1.0 X 1.0) = 0.6827    PDF and CDF of Gaussian distribution.   PDF and CDF of Gaussian distribution.      "
},
{
  "id": "tab-patient-data-for-jt-and-conditional-probs",
  "level": "2",
  "url": "sec-Joint-Conditional-and-Marginal-Probabilities.html#tab-patient-data-for-jt-and-conditional-probs",
  "type": "Table",
  "number": "1.5.1",
  "title": "Patient Data",
  "body": " Patient Data                    "
},
{
  "id": "tab-jt-probs-disease-test",
  "level": "2",
  "url": "sec-Joint-Conditional-and-Marginal-Probabilities.html#tab-jt-probs-disease-test",
  "type": "Table",
  "number": "1.5.2",
  "title": "Joint Probabilities",
  "body": " Joint Probabilities                    "
},
{
  "id": "subsec-Conditional-Probability-8-3",
  "level": "2",
  "url": "sec-Joint-Conditional-and-Marginal-Probabilities.html#subsec-Conditional-Probability-8-3",
  "type": "Table",
  "number": "1.5.3",
  "title": "Patients that tested negative",
  "body": " Patients that tested negative    D 50    N 500    "
},
{
  "id": "fig-psd-anf-cdf-of-normal-function",
  "level": "2",
  "url": "sec-Joint-Conditional-and-Marginal-Probabilities.html#fig-psd-anf-cdf-of-normal-function",
  "type": "Figure",
  "number": "1.5.4",
  "title": "",
  "body": " PDF and CDF of Gaussian distribution.   PDF and CDF of Gaussian distribution.   "
},
{
  "id": "sec-Example-Discrete-Probability-Distributions",
  "level": "1",
  "url": "sec-Example-Discrete-Probability-Distributions.html",
  "type": "Section",
  "number": "1.6",
  "title": "Example Discrete Probability Distributions",
  "body": " Example Discrete Probability Distributions   Distributions describe how probabilities are spread across values of a random variable. We will given examples of Probability Mass Function (PMF) for the discrete random variables. We will discuss examples of Probability Density Function (PDF) for continuous random variable in the next section.    Bernoulli Distribution  the outcome of each elementary event of a Bernoulli trial is either a failure or success of something 9false or true of some statement, or any myriads of two states problems, which is usually represented by a random variable having values and . That is We call such random variables Bernoulli variables . The values of probability of each value of gives us the Probability Mass Function (PMF) of the Bernoulli distribution. Suppose probability of is . Then, probability of will be . Sometimes is denote by , with .  The separate listing of the probability of the two values of in Eqs. and can actually be written more conveniently in one formula. From this formula, you will get and by substitting appropriate value of .   Graphically, Bernoulli distribution is plotted as bars with a dot at the top of the bar. Figure to the side shows an illustration with for , and of course, for .     Bernoulli distribution with .    For any distribution, we can find the mean of variable by weighing each value of with its probability. Similarly we can find the expectation value of any power of . For instance, the expectation value of the power of will be Thus, variance of a Bernoulli variable will be Therefore, the standard deviation, ,of Bernoulli variable is     Binomial Distribution  Imagine tossing a single coin a fixed number number of times, say times. You might get no Heads at all or 1 Heads and 9 Tails, or 2 Heads and 8 Tails, etc. Record how many Heads you got in this trial, say you got 3 Heads. Now, toss the same coin 10 times again. This will be the second trial of experiment \"tossing a particular coin 10 times\". In the second trial, you might get a different number of Heads, say this time you got 8 Heads.  If you repeated the experiment above hundreds or thousands of times, you can build a table of number of trials that resulted in a total of Heads, Heads, Heads, , Heads, which are all the possibilities. This table, an example shown in for 2000 trials, will be our Frequency Table . By dividing each of these numbers by the total number of trials you performed, you will get an estimate of probabilities of each outcome. The exact formula that gives the distribution you found is called Bernoulli distribution for the -toss experiment .   Binomial Experiment: Frequency and Approximate Probability    Total Number of Heads  Frequency  Approximate Probability    0  8  P(0) \\approx 0.004    1  45  P(1) \\approx 0.0225    2  120  P(2) \\approx 0.06    3  220  P(3) \\approx 0.11    4  300  P(4) \\approx 0.15    5  350  P(5) \\approx 0.175    6  300  P(6) \\approx 0.15    7  250  P(7) \\approx 0.125    8  200  P(8) \\approx 0.10    9  150  P(9) \\approx 0.075    10  57  P(10) \\approx 0.0285    Total Number of Trials:  2000     In general, Binomial distribution gives us the probability of different number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success . In each trial of a Binomial experiments you have a sequence of Bernoulli repeats of (for success) and (for failure). Suppose this sequence has Heads ( ) and Tails ( ). The probability of this sequence of Bernoulli outcomes will be But the and could have occurred in any order.To get the probability of getting a total of Heads in any order, we multiply number of different orders in which we could have got the same total number of Heads. That turns out to be the Binomial coefficient, and hence the name Binomial distribution. Note that this distribution has two fixed parameters , the number of independent Bernoulli trials in each Binomial trial and , the probability of success in each Bernoulli trial. Beware of the importance of ; you can think of there being infinitely many Binomial distributions, each corresponding to different values of . For instance, in Table , if you had conducted the experiment with Bernoulli trials in each Binomial experiment, instead of , you would have gotten much different probabilities for . This is illustrated in the following figure, .   Illustrating that different values in Binomial distribution correspond to different distributions. Here, with and . See that the probabilities for same value are different for the two distributions.   llustrating that different values in Binomial distribution correspond to different distributions. Here, with and . See that the probabilities for same value are different for the two distributions.    Following code was used to create the plot above.   import numpy as np import matplotlib.pyplot as plt from scipy.stats import binom # Parameters p = 0.5 # probability of success N1 = 20 # number of trials for first distribution N2 = 10 # number of trials for second distribution # Support for each distribution x1 = np.arange(0, N1+1) x2 = np.arange(0, N2+1) # PMFs pmf1 = binom.pmf(x1, N1, p) pmf2 = binom.pmf(x2, N2, p) # Plot fig, ax = plt.subplots(figsize=(8,5)) # Binomial N1 ax.vlines(x1, 0, pmf1, colors='blue', lw=2, label=f'Bin{N1}') ax.plot(x1, pmf1, 'o', color='blue') # Binomial N2 ax.vlines(x2, 0, pmf2, colors='orange', lw=2, label=f'Bin{N2}') ax.plot(x2, pmf2, 'o', color='orange') # Labels and grid ax.set_title(f'Binomial Distribution PMFs (p={p})') ax.set_xlabel('Number of Successes') ax.set_ylabel('Probability') ax.grid(axis='y', linestyle='--', alpha=0.6) ax.legend() plt.show()   Another way to improve your intuition about the Binomial distribution is to look at the impact of changing value for the Bernoulli trials themselves - what impact do they have on a -Binomial? It is shown in . These plots show that low skews the PMF toward fewer successes; produces a symmetric distribution centered at ; high skews toward more successes.   Illustrating that different values in Binomial distribution correspond to different distributions but with .   Illustrating that different values in Binomial distribution correspond to different distributions but with .    For doing analytical calculations with the Binomial distribution, it is important to recall the following algebraic identity, called Binomial expansion. Using this it is straightforward to show that Binomial distribution is normalized properly since The mean of the Binomial random variable can be obtained by weighing each value of by the corresponding probability. A simple method of showing the result involves taking an appropriate derivative appropriately. The variance is similarly shown to be And, the standard deviation is just the square root.   Binomial distribution plays important role in understanding average of several Bernoulli random variables, say , which have the same . Suppose, we denote Bernoulli variables by . Then, their sum will be a Binomial random variable, if Bernoulli random variables take or as awe have discussed above. The average will be a scaled Binomial variable. We will denote this random variable by with a bar above the symbol and a reminder that it is average of Bernoulli variables. This random variable is called sample mean . It will take the following values: With being probability of any of the individual Bernoulli variables to produce a success, i.e., for every one of the . But the variance is quite interesting which translates to the standard deviation  Where does this matter?       Estimation: In statistics, we often estimate by from data.     Interpretation: If you run multiple experiments, your average success rate will be centered at and become more concentrated as grows since the standard deviation drops as . This is illustrated in .     Connection to the Central Limit Theorem: For large , it can be shown that the probability distribution of the random variable tends to become Gaussian with the mean and variance . We write this as even though each is a discrete random variable. This goes by the name Central Limit Theorem .     Illustrating that for large the distribution of the average of Bernoulli variables of the same tends towards a Gaussian distribution.   Illustrating that for large the distribution of the average of Bernoulli variables of the same tends towards a Gaussian distribution.       Poisson Distribution   The Poisson distribution models the number of times an event occurs in a fixed interval of time or space, given that:   Events occur independently.    Events happen at a constant average rate, usually denoted by Greek letter lambda .    Two events cannot occur at the exact same instant. That means we are usually interested in events that are rare within the interval we choose to work with so that it can be safely assumed that two events do not coincide.   A Poisson random variable can take any non-negative integer values since it's just a count. The probability mass function for a Poisson random variable will give probabilities for for each non-negative value for a constant average rate is given by It is obviously normalized since Therefore The mean of Poisson distribution is the average count, \\lambda. as we can show by the following calculations. The variance of Poisson distribution is similarly shown to be also . where the missing steps are left for the student to practice, using the same type of argument as introducing operators appropriately.  A mathematically interesting result is that in an appropriate limit, a Binomial distribution can be shown to become same as Poisson distribution. I will just state the result without giving you the detailed calculations. (hint: You can get factors of from ).    Example Radioactivity is one of the classic and most intuitive real-life examples of the Poisson distribution. Let's look at it a little closely. Radioactive decay is a random process. Each atom has a constant probability of decaying in a fixed time interval. The decays are:    Independent (one decay does not affect another).     Rare events relative to the huge number of atoms.    Occurring with a constant average rate .   These aspects make the Poisson distribution a perfect model for studying the statistics of radioactivity.  Suppose we measure the number of particles emitted from a radioactive source in 10-second intervals. From past experiments, we know that the detector records on average 3 decays per 10 seconds. So, per second, we expect on-average decays. That completely specifies the Poisson distribution. Therefore, we can immediately calculate all sorts of things for the phenomenon. For instance, probability of seeing exactly decays in seconds will be Probability of exactly 3 decays in a second will be Now, for a trick question. What will be the probability of 10 decays in one minute? Well, we will convert our lambda per second to a new lambda per minute. Let's label lambda's by the intervals they refer to. Then   A visual representation of the PMF often helps to build intuition. A simple program in Python can be used to to do that. The plot with is shown in .  import numpy as np import matplotlib.pyplot as plt from scipy.stats import poisson # Average decay rate lam = 3 # Range of possible counts x = np.arange(0, 15) pmf = poisson.pmf(x, lam) plt.figure(figsize=(8,5)) plt.vlines(x, 0, pmf, colors='darkred', lw=2, alpha=0.7, label=f'λ={lam}') plt.plot(x, pmf, 'o', color='black', markersize=5) plt.xlabel('Number of Decays in One Interval') plt.ylabel('Probability P(X=k)') plt.title('Poisson Distribution of Radioactive Decays (λ=3)') plt.grid(axis='y', linestyle='--', alpha=0.6) plt.legend() plt.show()     Poisson Distribution for . The most likely outcome is 3 decays per 10 seconds (the mean). Note that 0 or 1 decay is possible, but much less likely. Seeing 6 or more decays is rare, but not impossible.   Poisson Distribution for .      Poisson Process  A Poisson process is a stochastic process used to model the occurrence of events that happen independently and at a constant average rate over time or space.  Formally, a Poisson process is a counting process , where represents the number of events that have occurred up to time  , and it satisfies the following properties:    Initial Condition : , meaning the process starts with no events at time zero.     Independent Increments : The number of events in non-overlapping time intervals is independent. For example, the number of events in is independent of the number in if the intervals do not overlap.     Stationary Increments : The number of events in a time interval of length , i.e., , depends only on the length and not on the starting point .     Poisson Distribution : The number of events in any interval of length follows a Poisson distribution with mean , where is the rate parameter (average number of events per unit time). The probability of events in an interval of length is:      No Simultaneous Events : The probability of two or more events occurring at exactly the same time is negligible (technically, the probability of multiple events in an infinitesimally small interval is zero).       Examples :    Queueing Systems : Customers arriving at a store at an average rate of customers per hour.     Telecommunications : Phone calls arriving at a call center with a constant average rate.     Reliability : Failures of a machine occurring randomly at an average rate of failures per hour.     Traffic : Cars passing a checkpoint on a highway at a constant average rate.       "
},
{
  "id": "tab-Binomial-frequency-and-prob",
  "level": "2",
  "url": "sec-Example-Discrete-Probability-Distributions.html#tab-Binomial-frequency-and-prob",
  "type": "Table",
  "number": "1.6.1",
  "title": "Binomial Experiment: Frequency and Approximate Probability",
  "body": " Binomial Experiment: Frequency and Approximate Probability    Total Number of Heads  Frequency  Approximate Probability    0  8  P(0) \\approx 0.004    1  45  P(1) \\approx 0.0225    2  120  P(2) \\approx 0.06    3  220  P(3) \\approx 0.11    4  300  P(4) \\approx 0.15    5  350  P(5) \\approx 0.175    6  300  P(6) \\approx 0.15    7  250  P(7) \\approx 0.125    8  200  P(8) \\approx 0.10    9  150  P(9) \\approx 0.075    10  57  P(10) \\approx 0.0285    Total Number of Trials:  2000    "
},
{
  "id": "fig-binomial-10N20",
  "level": "2",
  "url": "sec-Example-Discrete-Probability-Distributions.html#fig-binomial-10N20",
  "type": "Figure",
  "number": "1.6.2",
  "title": "",
  "body": " Illustrating that different values in Binomial distribution correspond to different distributions. Here, with and . See that the probabilities for same value are different for the two distributions.   llustrating that different values in Binomial distribution correspond to different distributions. Here, with and . See that the probabilities for same value are different for the two distributions.   "
},
{
  "id": "fig-binomial-N10p2p5pp8",
  "level": "2",
  "url": "sec-Example-Discrete-Probability-Distributions.html#fig-binomial-N10p2p5pp8",
  "type": "Figure",
  "number": "1.6.3",
  "title": "",
  "body": " Illustrating that different values in Binomial distribution correspond to different distributions but with .   Illustrating that different values in Binomial distribution correspond to different distributions but with .   "
},
{
  "id": "fig-bernoulli-to-CLT",
  "level": "2",
  "url": "sec-Example-Discrete-Probability-Distributions.html#fig-bernoulli-to-CLT",
  "type": "Figure",
  "number": "1.6.4",
  "title": "",
  "body": " Illustrating that for large the distribution of the average of Bernoulli variables of the same tends towards a Gaussian distribution.   Illustrating that for large the distribution of the average of Bernoulli variables of the same tends towards a Gaussian distribution.   "
},
{
  "id": "fig-poisson-distribution",
  "level": "2",
  "url": "sec-Example-Discrete-Probability-Distributions.html#fig-poisson-distribution",
  "type": "Figure",
  "number": "1.6.5",
  "title": "",
  "body": " Poisson Distribution for . The most likely outcome is 3 decays per 10 seconds (the mean). Note that 0 or 1 decay is possible, but much less likely. Seeing 6 or more decays is rare, but not impossible.   Poisson Distribution for .   "
},
{
  "id": "sec-Example-Continuous-Probability-Distributions",
  "level": "1",
  "url": "sec-Example-Continuous-Probability-Distributions.html",
  "type": "Section",
  "number": "1.7",
  "title": "Example Continuous Probability Distributions",
  "body": " Example Continuous Probability Distributions   Continuous random variables take values on an interval of the real line. Unlike discrete random variables, we do not assign probability to single points (which would be zero). Instead, we assign probability to intervals, using probability density functions ( PDFs ).  In this section, we will discuss three distributions that are important for ML use.    Uniform: All values equally likely     Normal (Gaussian): Bell-shaped curve     Exponential: Time until an event occurs   There are, of course, other distributions such as beta and gamma distributions that are also in common use. By presenting just these three here, I hope to give the reader enough feel for what to look for when studying other distributions.    Uniform Distribution   When a random variable is equally likely to take any value between two real numbers and , we say that the distribution is uniform between these values. The distribution is usually designated by with , and say that which means that where the part without the is the PDF of the distribution, which we denote by the Greek letter rho, .  The mean value of the distribution will clearly be half way between and as can easily be computed by performing the simple integral. The variance, which is square of the standard deviation , is similarly calculated to yield where Thus, standard deviation of is:   The cumulative distribution function, , which gives the probability that is easily calculated for uniform distribution. where I used for the dummy variable since now is a particular value. This will be a step function since the formula resulting from the integration depends on where the point happens to lie. The last line says that probability that has any value less than any value in is 1.0 since obviously the entire range is included in this case. The second line says that the probability increases linearly between and .   shows plots of PDF and CDF of uniform distribution . You can see that as we scan through the interval of the uniform PDF, the probability accumulates in the CDF and eventually, CDF becomes , which represents probability of any of the values in the interval.   PDF and CDF of Uniform distribution . Note the value of PDF is uniformly while that of CDF increases linear in the interval.   PDF and CDF of Uniform distribution . Note the value of PDF is uniformly while that of CDF increases linear in the interval.    To generate these plots import: from scipy.stats import uniform and then use the methods uniform.pdf() and uniform.cdf().    Inverse Uniform CDF  Inverse of a CDF is used for sampling from a distribution. Although, we will see below that inverse CDF of the uniform distribution is trivial, the inverse CDF is highly useful when you need to generate samples from some other distributions such as Normal, Exponential, Gamma, etx.  The inverse of CDF is written as - it's not a negative one power of ; that is just the name of the inverse function. As the name implies when you successively apply an to some number , you would get that number back. So, what does it look like for the uniform distribution ? The CDF in the range is The inverse will be Let's check if that's true. So, if you wanted to generate samples in the range that act like they are sampled from the uniform distribution , you would first generate pseudo-random numbers in unit interval using algorithms lke Mersenne Twister. Suppose, you have such sample . Then, you will plug them in the inverse CDF to generate samples from . This is trivial here since we have an analytic expression for the inverse CDF. In other districutions, such as the Gaussian, i.e., Normal distribution, the inverse can only be computed numerically. The stats packages usually have functions that do it for you. For instance, ppf() method in Python\/scipy.stats is used for that purpose. In case of uniform distribution, the command is  from scipy.stats import uniform samples = uniform.ppf(u, loc=a, scale=b-a).       Normal (Gaussian) Distribution   The PDF of a Gaussian or Normal distribution is a bell-shaped curve with only two parameters, a mean and a standard deviation . The name Gaussian is preferred in Physics and Engineering, and Normal is preferred in statistics and data science. In these notes I will use both of them, just for fun.  The PDF of the Gaussian distribution of mean and standard deviation for a scalar variable is defined by where The reason I am writing rather than is that exponent in the latter expression usually prints too small on the screen. While doing calculations by hand, you should stick to notation.  When the distribution of a random variable is Gaussian of mean and standard deviation , i.e., variance we denote this as a short hand notation by The special case of and is called standard normal distribution . A standard normal random variable will obey   As always, the PDF in Eq. has the probability interpretation in an infinitesimal interval around . Thus, if you want the probability of , you will just integrate it. The CDF is just such an integral for the probability of . If , then entire real line is included. That would make The integral is unwieldy and only done numerically. The Fundamental Theorem of Calculus gives us an analytic expression of the derivative of CDF, which is used in formal analytical work. This is, of course, a general result and applies to all PDF\/CDF and forms a powerful tool of formal work. We are not going to do much in that direction.   shows plots of PDF and CDF of Gaussian distribution .   PDF and CDF of Gaussian distribution . Note the bell-shape of the PDF and the soft step of the CDF which goes from to .   PDF and CDF of Gaussian distribution . Note the bell-shape of the PDF and the soft step of the CDF which goes from to .    To generate these plots import: from scipy.stats import uniform and then use the methods uniform.pdf() and uniform.cdf().  The mean and variance of a Gaussian is in the definition itself and can be readily checked if you know how do Gaussian integrals. Here are couple of tricks of doing Gaussian integrals.     Inverse CDF and Sampling  As we defined for the uniform distribution above, inverse of the Cumulative Distribution Function (CDF) is another function, denoted by (NotE: it's not . The negative power in the symbol is just a symbol.) The functions and are inverses so that when you act by them, one does the effect of the other. But unlike the case was with the uniform distribution, here we only know in the integral form. However, you can find of Gaussian distribution numerically. It is already programmed in stats packages. For instance, we can compute the percent point function (PPF), which is the numerical inverse CDF by scipy.stats.norm.ppf() function by passing the appropriate parameters.  The CDF function is a mapping from to . The inverse will map to . To sample randomly from Gaussian distribution means producing random values of . That means if we obtain random values and feed that into , the result will be random values of sampled according to the distribution of the , which is Gaussian in the present case. Obtaining random values can be done by just sampling from the uniform distribution .  Thus the steps of sampling from a distribution is:   Generate uniform random numbers .    Apply , if using inverse of standard normal. If using scipy.stats pacakge, the code for will be scipy.stats.norm.ppf( , loc = , scale = ) . In scipy.stats, you can include the actual loc and scale in the argument itself as shown in the program listing below.    Use as your Gaussian samples.   Let us look at an example of drawing from a Gaussian distribution and how the samples match up with the theoretical distribution. This is shown in . It was produced by the code below. Clearly, the histogram based on the samples is very representative of the theoretical curve.   import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm # Parameters mu, sigma = 0, 1 n_samples = 1000 # Step 1: uniform samples u = np.random.rand(n_samples) # Step 2: transform with inverse CDF (ppf) samples = norm.ppf(u, loc=mu, scale=sigma) # Plot histogram vs theoretical PDF x = np.linspace(-4, 4, 200) pdf = norm.pdf(x, mu, sigma) plt.figure(figsize=(7,5)) plt.hist(samples, bins=30, density=True, alpha=0.6, label=\"Sampled (inverse CDF)\") plt.plot(x, pdf, 'r-', lw=2, label=\"Theoretical PDF\") plt.xlabel(\"x\") plt.ylabel(\"Density\") plt.title(\"Sampling from Normal(0,1) using Inverse CDF\") plt.legend() plt.show()    Samples from a Gaussian distribution and the theoretical curve. The histogram is based on 1000 sample points.   Samples from a Gaussian distribution and the theoretical curve. The histogram is based on 1000 sample points.       Exponential Distribution  Exponential distribution is commonly used to model the time between successive events in a Poisson process, where events occur independently and at a constant average rate. The distribution is defined for non-negative values ( ) and is characterized by a single parameter, (lambda), which is the rate parameter, as described in the section on the Poisson distribution.  The PDF of Exponential distribution is given by As usual, it has the following probability interpretation. You can verify that the PDF in Eq. is properly normalized to give probability over the entire range f values, i.e, is . The Cumulative Distribution Function, (CDF), is the probability for . Therefore,  shows the PDF and CDF of the exponential distribution for .   PDF and CDF of Exponential distribution . Note the exponential decaying property of the PDF and the corresponding rise of the CDF which goes from to .   PDF and CDF of Exponential distribution . Note the exponential decaying property of the PDF and the corresponding rise of the CDF which goes from to .    The complement of the CDF, i.e., the probability that is called survival function (SF).   From the survival function, it is possible to prove and important property of exponential distribution: that it is meomryless , meaning that the probability of an event occurring in the next time interval does not depend on how much time has already elapsed. In formulas, this will be a condition on the conditional probability. That is, whether you wait upto and then look at the next units of time ot you don't wait and look at the next interval of time, the two will give the same probability - same probability for intervals of time will be independent of the starting instant.   Proof of Memoryless Property : Let's write the left side of Eq. in terms of joint and prior, based on the definition of conditional probabilities given in an earlier section. Since the joint probability, you see that joint probability will simply equal . Therefore, Now, we use the survival function given in Eq. to write the right hand side quantities and then simplify.   Finally, let's go over the mean and variance of exponential distribution. Mean as usual is the expectation value of the random variable . The variance will be from which we get the standard deviation .    Real-World Examples :   Radioactive Decay:  Time until the next decay event for a particle with decay rate .    Queueing Systems:  Time until the next customer arrives at a store, assuming arrivals follow a Poisson process with rate customers per hour. Mean waiting time = hours.    Reliability:  Lifetime of a lightbulb that fails at a constant rate failures per hour. The probability it lasts more than hours is .      "
},
{
  "id": "fig-uniform-pdf-cdf",
  "level": "2",
  "url": "sec-Example-Continuous-Probability-Distributions.html#fig-uniform-pdf-cdf",
  "type": "Figure",
  "number": "1.7.1",
  "title": "",
  "body": " PDF and CDF of Uniform distribution . Note the value of PDF is uniformly while that of CDF increases linear in the interval.   PDF and CDF of Uniform distribution . Note the value of PDF is uniformly while that of CDF increases linear in the interval.   "
},
{
  "id": "fig-gaussian-pdf-cdf",
  "level": "2",
  "url": "sec-Example-Continuous-Probability-Distributions.html#fig-gaussian-pdf-cdf",
  "type": "Figure",
  "number": "1.7.2",
  "title": "",
  "body": " PDF and CDF of Gaussian distribution . Note the bell-shape of the PDF and the soft step of the CDF which goes from to .   PDF and CDF of Gaussian distribution . Note the bell-shape of the PDF and the soft step of the CDF which goes from to .   "
},
{
  "id": "fig-gaussian-sampling",
  "level": "2",
  "url": "sec-Example-Continuous-Probability-Distributions.html#fig-gaussian-sampling",
  "type": "Figure",
  "number": "1.7.3",
  "title": "",
  "body": " Samples from a Gaussian distribution and the theoretical curve. The histogram is based on 1000 sample points.   Samples from a Gaussian distribution and the theoretical curve. The histogram is based on 1000 sample points.   "
},
{
  "id": "fig-exponential-pdf-cdf",
  "level": "2",
  "url": "sec-Example-Continuous-Probability-Distributions.html#fig-exponential-pdf-cdf",
  "type": "Figure",
  "number": "1.7.4",
  "title": "",
  "body": " PDF and CDF of Exponential distribution . Note the exponential decaying property of the PDF and the corresponding rise of the CDF which goes from to .   PDF and CDF of Exponential distribution . Note the exponential decaying property of the PDF and the corresponding rise of the CDF which goes from to .   "
},
{
  "id": "backmatter-2",
  "level": "1",
  "url": "backmatter-2.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')
  this.metadataWhitelist = ['position']

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
