<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Basic Probability for Machine Learning</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="ML Notes">
<script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math"
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "_static/pretext/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/themes/prism.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-numbers/prism-line-numbers.min.js" integrity="sha512-dubtf8xMHSQlExGRQ5R7toxHLgSDZ0K7AunqPWHXmJQ8XyVIG19S1T95gBxlAeGOK02P4Da2RTnQz0Za0H0ebQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-numbers/prism-line-numbers.min.css" integrity="sha512-cbQXwDFK7lj2Fqfkuxbo5iD1dSbLlJGXGpfTDqbggqjHJeyzx88I3rfwjS38WJag/ihH7lzuGlGHpDBymLirZQ==" crossorigin="anonymous" referrerpolicy="no-referrer">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-highlight/prism-line-highlight.min.css" integrity="sha512-nXlJLUeqPMp1Q3+Bd8Qds8tXeRVQscMscwysJm821C++9w6WtsFbJjPenZ8cQVMXyqSAismveQJc0C1splFDCA==" crossorigin="anonymous" referrerpolicy="no-referrer">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-highlight/prism-line-highlight.min.js" integrity="sha512-93uCmm0q+qO5Lb1huDqr7tywS8A2TFA+1/WHvyiWaK6/pvsFl6USnILagntBx8JnVbQH5s3n0vQZY6xNthNfKA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="_static/pretext/js/pretext_search.js"></script><link href="_static/pretext/css/pretext_search.css" rel="stylesheet" type="text/css">
<script src="_static/pretext/js/lib/jquery.min.js"></script><script src="_static/pretext/js/lib/jquery.sticky.js"></script><script src="_static/pretext/js/lib/jquery.espy.min.js"></script><script src="_static/pretext/js/pretext.js"></script><script src="_static/pretext/js/pretext_add_on.js?x=1"></script><script src="_static/pretext/js/user_preferences.js"></script><script src="_static/pretext/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
<link href="_static/pretext/css/pretext.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/shell_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/banner_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/navbar_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/toc_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/knowls_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/style_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/colors_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/setcolors.css" rel="stylesheet" type="text/css">
<style>
#subsec-Central-Tendency-3 { counter-set: subsec-Central-Tendency-3 0; }
#subsec-Central-Tendency-3 > li::marker { content: ""counter(subsec-Central-Tendency-3,decimal)"."; }
#subsec-Central-Tendency-3 > li { counter-increment: subsec-Central-Tendency-3; }
#subsec-Dispersion-3 { counter-set: subsec-Dispersion-3 0; }
#subsec-Dispersion-3 > li::marker { content: ""counter(subsec-Dispersion-3,decimal)"."; }
#subsec-Dispersion-3 > li { counter-increment: subsec-Dispersion-3; }
#subsec-Pandas-2-1-1 { counter-set: subsec-Pandas-2-1-1 0; }
#subsec-Pandas-2-1-1 > li::marker { content: ""counter(subsec-Pandas-2-1-1,decimal)"."; }
#subsec-Pandas-2-1-1 > li { counter-increment: subsec-Pandas-2-1-1; }
#subsec-Pandas-4-2 { counter-set: subsec-Pandas-4-2 0; }
#subsec-Pandas-4-2 > li::marker { content: ""counter(subsec-Pandas-4-2,decimal)"."; }
#subsec-Pandas-4-2 > li { counter-increment: subsec-Pandas-4-2; }
#sec-Numerical-and-Categorical-Data-2-1-1 { counter-set: sec-Numerical-and-Categorical-Data-2-1-1 0; }
#sec-Numerical-and-Categorical-Data-2-1-1 > li::marker { content: ""counter(sec-Numerical-and-Categorical-Data-2-1-1,decimal)"."; }
#sec-Numerical-and-Categorical-Data-2-1-1 > li { counter-increment: sec-Numerical-and-Categorical-Data-2-1-1; }
#subsec-Axiomatic-View-of-Probability-7-2 { counter-set: subsec-Axiomatic-View-of-Probability-7-2 0; }
#subsec-Axiomatic-View-of-Probability-7-2 > li::marker { content: ""counter(subsec-Axiomatic-View-of-Probability-7-2,decimal)""; }
#subsec-Axiomatic-View-of-Probability-7-2 > li { counter-increment: subsec-Axiomatic-View-of-Probability-7-2; }
#subsec-three-types-of-probabilities-3 { counter-set: subsec-three-types-of-probabilities-3 0; }
#subsec-three-types-of-probabilities-3 > li::marker { content: ""counter(subsec-three-types-of-probabilities-3,decimal)"."; }
#subsec-three-types-of-probabilities-3 > li { counter-increment: subsec-three-types-of-probabilities-3; }
#subsub-Poisson-Process-3-5 { counter-set: subsub-Poisson-Process-3-5 0; }
#subsub-Poisson-Process-3-5 > li::marker { content: ""counter(subsub-Poisson-Process-3-5,decimal)"."; }
#subsub-Poisson-Process-3-5 > li { counter-increment: subsub-Poisson-Process-3-5; }
#subsub-Poisson-Process-4-2 { counter-set: subsub-Poisson-Process-4-2 0; }
#subsub-Poisson-Process-4-2 > li::marker { content: ""counter(subsub-Poisson-Process-4-2,decimal)"."; }
#subsub-Poisson-Process-4-2 > li { counter-increment: subsub-Poisson-Process-4-2; }
#subsec-Hypothesis-Testing-4 { counter-set: subsec-Hypothesis-Testing-4 0; }
#subsec-Hypothesis-Testing-4 > li::marker { content: ""counter(subsec-Hypothesis-Testing-4,decimal)"."; }
#subsec-Hypothesis-Testing-4 > li { counter-increment: subsec-Hypothesis-Testing-4; }
</style>
</head>
<body id="ML-Notes" class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" target="_blank" href=""></a><div class="title-container">
<h1 class="heading"><a href="my-ML-Notes.html"><span class="title">ML Notes:</span> <span class="subtitle">Theoretical and Practical ML Concepts</span></a></h1>
<p class="byline"></p>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" title="Contents"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5d2;</span><span class="name">Contents</span></button><div class="searchbox">
<div class="searchwidget"><button id="searchbutton" class="searchbutton button" type="button" title="Search book"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe8b6;</span><span class="name">Search Book</span></button></div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<div class="search-results-controls">
<input aria-label="Search term" id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search term"><button title="Close search" id="closesearchresults" class="closesearchresults"><span class="material-symbols-outlined">close</span></button>
</div>
<h2 class="search-results-heading">Search Results: </h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div>
<span class="nav-other-controls"></span><span class="treebuttons"><a class="previous-button button" href="sec-Numerical-and-Categorical-Data.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="up-button button" href="ch-Essential-Probability-and-Statistics.html" title="Up"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Up</span></a><a class="next-button button" href="sec-Random-Variables-and-Probabilities.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a></span></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\N}{\mathbb N} \newcommand{\Z}{\mathbb Z} \newcommand{\Q}{\mathbb Q} \newcommand{\R}{\mathbb R}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural contains-active toc-item-list">
<li class="toc-item toc-frontmatter"><div class="toc-title-box"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div></li>
<li class="toc-item toc-chapter contains-active">
<div class="toc-title-box"><a href="ch-Essential-Probability-and-Statistics.html" class="internal"><span class="codenumber">1</span> <span class="title">Essential Probability and Statistics</span></a></div>
<ul class="structural toc-item-list contains-active">
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Descriptive-Statistics.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Descriptive Statistics</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Central-Tendency" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Measures of Central Tendency</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Dispersion" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Measures of Dispersion</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Distribution-Shape" class="internal"><span class="codenumber">1.1.3</span> <span class="title">Distribution Shape</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Kurtosis: Checking Out the Tails</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#types-of-kurtosis" class="internal"><span class="codenumber">1.1.4.1</span> <span class="title">Types of Kurtosis</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-formula" class="internal"><span class="codenumber">1.1.4.2</span> <span class="title">How’s It Calculated?</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-examples" class="internal"><span class="codenumber">1.1.4.3</span> <span class="title">Real-World Examples</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-visual" class="internal"><span class="codenumber">1.1.4.4</span> <span class="title">Seeing Kurtosis in Action</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-python" class="internal"><span class="codenumber">1.1.4.5</span> <span class="title">Calculating Kurtosis with Python</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-wrapup" class="internal"><span class="codenumber">1.1.4.6</span> <span class="title">Why Kurtosis Matters</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Computation and Visualization Tools</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#sec-useful-descriptive-statistics-tools-3" class="internal"><span class="codenumber">1.2.1</span> <span class="title">The Power of NumPy and SciPy</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-Pandas" class="internal"><span class="codenumber">1.2.2</span> <span class="title">Pandas: Data Manipulation and Analysis</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-Visualization" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Visualization with Matplotlib and Seaborn</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Numerical and Categorical Data</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-Categorical-Data" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Categorical Data and One-Hot Encoding</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-Ordinal-Data" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Ordinal Data and Safe Encoding</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-Numerical-Data" class="internal"><span class="codenumber">1.3.3</span> <span class="title">Numerical Data: Discrete vs Continuous</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-data-type-summary" class="internal"><span class="codenumber">1.3.4</span> <span class="title">Data Type Summary</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section active">
<div class="toc-title-box"><a href="sec-Basic-Probability.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Basic Probability for Machine Learning</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-Axiomatic-View-of-Probability" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Axiomatic View of Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-sum-product-rules" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Sum and Product Rules for Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-conditional-probability-independence" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Conditional Probability and Independence</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-probability-distributions" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Probability Distributions</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-three-types-of-probabilities" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Three Types of Probabilities</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Random Variables and Probabilities</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Random-Variables" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Random Variables, Probabilities, and Expectations</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Probability-Mass-Function" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Probability Mass Function</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Expectation-Values" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Expectation Values</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Probability-Distribution-of-Continuous-Variable" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Probability Density of Continuous Variables</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Cumulative-Distribution-Function" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Cumulative Distribution Function (CDF)</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html" class="internal"><span class="codenumber">1.6</span> <span class="title">Two or More Random Variables</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Joint-Probability" class="internal"><span class="codenumber">1.6.1</span> <span class="title">Joint Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Marginal-Probability" class="internal"><span class="codenumber">1.6.2</span> <span class="title">Marginal Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Conditional-Probability" class="internal"><span class="codenumber">1.6.3</span> <span class="title">Conditional Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-conditional-probability-from-joint-probability" class="internal"><span class="codenumber">1.6.4</span> <span class="title">Bayes’ Rule</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Covariance-and-Correlation" class="internal"><span class="codenumber">1.6.5</span> <span class="title">Covariance and Correlation</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Continuous-Random-Variables" class="internal"><span class="codenumber">1.6.6</span> <span class="title">Continuous Random Variables</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsubsec-Probability-Density" class="internal"><span class="codenumber">1.6.6.1</span> <span class="title">Probability Density</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsubsec-Cumulative-Distribution-Function" class="internal"><span class="codenumber">1.6.6.2</span> <span class="title">Cumulative Distribution Function</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Independent-Variables" class="internal"><span class="codenumber">1.6.7</span> <span class="title">Independent Random Variables</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Application-Medical-Diagnosis" class="internal"><span class="codenumber">1.6.8</span> <span class="title">Application: Medical Diagnosis</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html" class="internal"><span class="codenumber">1.7</span> <span class="title">Example Discrete Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Bernoulli-Distribution" class="internal"><span class="codenumber">1.7.1</span> <span class="title">Bernoulli Distribution</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Binomial-Distribution" class="internal"><span class="codenumber">1.7.2</span> <span class="title">Binomial Distribution</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Poisson-Distribution" class="internal"><span class="codenumber">1.7.3</span> <span class="title">Poisson Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsub-Poisson-Process" class="internal"><span class="codenumber">1.7.3.1</span> <span class="title">Poisson Process</span></a></div></li></ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html" class="internal"><span class="codenumber">1.8</span> <span class="title">Example Continuous Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Uniform-Distribution" class="internal"><span class="codenumber">1.8.1</span> <span class="title">Uniform Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsubsec-Inverse-Uniform-CDF" class="internal"><span class="codenumber">1.8.1.1</span> <span class="title">Inverse Uniform CDF</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Normal-Distribution" class="internal"><span class="codenumber">1.8.2</span> <span class="title">Normal (Gaussian) Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsubsec-Inverse-CDF-and-Sampling" class="internal"><span class="codenumber">1.8.2.1</span> <span class="title">Inverse CDF and Sampling</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Continuous-Probability-Distributions.html#subsec-Exponential-Distribution" class="internal"><span class="codenumber">1.8.3</span> <span class="title">Exponential Distribution</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-LLN-and-CLT.html" class="internal"><span class="codenumber">1.9</span> <span class="title">Law of Large Numbers and Central Limit Theorem</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Law-of-Large-Numbers" class="internal"><span class="codenumber">1.9.1</span> <span class="title">Law of Large Numbers</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Central-Limit-Theorem" class="internal"><span class="codenumber">1.9.2</span> <span class="title">Central Limit Theorem</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-LLN-vs-CLT" class="internal"><span class="codenumber">1.9.3</span> <span class="title">LLN vs. CLT</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Berry-Esseen" class="internal"><span class="codenumber">1.9.4</span> <span class="title">Berry-Esseen Theorem</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Why-Large-n-Matters" class="internal"><span class="codenumber">1.9.5</span> <span class="title">Why LLN and CLT Matter</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Inferential-Statistics.html" class="internal"><span class="codenumber">1.10</span> <span class="title">Inferential Statistics</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Point-Estimation" class="internal"><span class="codenumber">1.10.1</span> <span class="title">Point Estimation</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Sampling-Distributions" class="internal"><span class="codenumber">1.10.2</span> <span class="title">Sampling Distributions</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Hypothesis-Testing" class="internal"><span class="codenumber">1.10.3</span> <span class="title">Hypothesis Testing</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Confidence-Intervals" class="internal"><span class="codenumber">1.10.4</span> <span class="title">Confidence Intervals</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Types-of-Errors" class="internal"><span class="codenumber">1.10.5</span> <span class="title">Types of Errors</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Power-Effect-Size" class="internal"><span class="codenumber">1.10.6</span> <span class="title">Statistical Power and Effect Size</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Multiple-Testing" class="internal"><span class="codenumber">1.10.7</span> <span class="title">Multiple Testing</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Other-Tests" class="internal"><span class="codenumber">1.10.8</span> <span class="title">Common Statistical Tests</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-backmatter"><div class="toc-title-box"><a href="backmatter.html" class="internal"><span class="title">Backmatter</span></a></div></li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content"><section class="section" id="sec-Basic-Probability"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">1.4</span><span class="space"> </span><span class="title">Basic Probability for Machine Learning</span>
</h2>
<section class="introduction" id="sec-Basic-Probability-2"><div class="para" id="sec-Basic-Probability-2-1">Probability is the backbone of machine learning, helping us model uncertainty in data, predictions, and outcomes. In machine learning, probability underpins tasks like classification (e.g., predicting labels), evaluating model confidence, and handling noisy data. This section introduces probability concepts such as sample spaces, events, and axioms—and connects them to practical machine learning applications using Python.</div> <div class="para" id="sec-Basic-Probability-2-2">An <em class="alert">event</em> is a specific outcome or set of outcomes from an experiment, represented as a set. For a coin toss, "heads" is <span class="process-math">\(\{H\}\text{,}\)</span> "tails" is <span class="process-math">\(\{T\}\text{,}\)</span> and "heads or tails" is <span class="process-math">\(\{H, T\}\text{.}\)</span> Each trial answers whether an event occurred (yes/no). For a die roll yielding <span class="process-math">\(2\text{,}\)</span> events like <span class="process-math">\(\{2\}\)</span> or <span class="process-math">\(\{1,2,3\}\)</span> occur if they include <span class="process-math">\(2\text{.}\)</span> Sets allow combining events via union (<span class="process-math">\(\cup\)</span>) or intersection (<span class="process-math">\(\cap\)</span>), such as <span class="process-math">\(\{H\} \cup \{T\} = \{H, T\}\text{.}\)</span>
</div></section><section class="subsection" id="subsec-Axiomatic-View-of-Probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.1</span><span class="space"> </span><span class="title">Axiomatic View of Probability</span>
</h3>
<div class="para" id="subsec-Axiomatic-View-of-Probability-2">In 1933, Andrey Kolmogorov formalized probability with three axioms, providing a mathematical framework. Think of these as rules that ensure probabilities make sense, like ensuring a weather forecast never predicts negative rain or more than <span class="process-math">\(100\%\)</span> chance.</div>
<div class="para" id="subsec-Axiomatic-View-of-Probability-3">
<em class="alert">Sample Space <span class="process-math">\(\Omega\)</span></em>: The set of all possible outcomes. For a six-sided die, <span class="process-math">\(\Omega = \{1, 2, 3, 4, 5, 6\}\text{.}\)</span> For a student passing an exam, <span class="process-math">\(\Omega = \{\text{Pass}, \text{Fail}\}\text{.}\)</span>
</div>
<div class="para" id="subsec-Axiomatic-View-of-Probability-4">
<em class="alert">Event Space <span class="process-math">\(F\)</span></em>: All possible subsets of <span class="process-math">\(\Omega\text{,}\)</span> including the empty set <span class="process-math">\(\varnothing\)</span> (impossible event) and <span class="process-math">\(\Omega\)</span> (event certain to happen). For a coin toss (<span class="process-math">\(\Omega = \{H, T\}\)</span>), <span class="process-math">\(F = \{\varnothing, \{H\}, \{T\}, \{H, T\}\}\text{.}\)</span> With <span class="process-math">\(N\)</span> outcomes, <span class="process-math">\(F\)</span> has <span class="process-math">\(2^N\)</span> events.</div>
<div class="para logical" id="subsec-Axiomatic-View-of-Probability-5">
<div class="para">With every event <span class="process-math">\(E\)</span> we can identify its complementary event or complement <span class="process-math">\(E^c\text{.}\)</span> The complementary event <span class="process-math">\(E^c\)</span> includes all other possibilities that excluded the event <span class="process-math">\(E\text{.}\)</span> Thus, in a six-sided die, say <span class="process-math">\(E = \{ 1, 3, 4 \}\text{.}\)</span> Its complement will be the event <span class="process-math">\(E^c = \{ 2, 5, 6\}\text{.}\)</span> Clearly, their intersection will be the null event.</div>
<div class="displaymath process-math">
\begin{equation*}
E \cap E^c = \varnothing.
\end{equation*}
</div>
</div>
<div class="para" id="subsec-Axiomatic-View-of-Probability-6">
<em class="alert">Probability Measure <span class="process-math">\(P\)</span></em>: Assigns a number <span class="process-math">\(P(E)\)</span> to each event <span class="process-math">\(E \in F\text{,}\)</span> representing its likelihood. For example, for a fair die, <span class="process-math">\(P(\{1\}) = 1/6\text{.}\)</span> You only need <span class="process-math">\(P\)</span> for elementary events, which are the elements of <span class="process-math">\(\Omega\text{,}\)</span> each taken as one event since you can use the Aditivity law below to get probability of any event in the entire event space <span class="process-math">\(F\text{.}\)</span>
</div>
<div class="para logical" id="subsec-Axiomatic-View-of-Probability-7">
<div class="para">The probability space is the triplet <span class="process-math">\((\Omega, F, P)\text{.}\)</span> Kolmogorov’s axioms are:</div>
<ol class="decimal" id="subsec-Axiomatic-View-of-Probability-7-2">
<li id="subsec-Axiomatic-View-of-Probability-7-2-1"><div class="para" id="p-derived-subsec-Axiomatic-View-of-Probability-7-2-1">
<em class="alert">Non-negativity</em>: Although, negative probabilities may be taunting science fiction scenarios, they do not make sense in the probailities we deal with everyday. We require that<div class="displaymath process-math" id="eqn-first-axiom-non-negativity">
\begin{equation}
P(E) \ge 0 \text{ for all } E \in F.\tag{1.4.1}
\end{equation}
</div>
</div></li>
<li id="subsec-Axiomatic-View-of-Probability-7-2-2"><div class="para" id="p-derived-subsec-Axiomatic-View-of-Probability-7-2-2">
<em class="alert">Normalization</em>: Since the event <span class="process-math">\(\Omega\)</span> has all possible elementary events, except the null event, every possible event is inculded in <span class="process-math">\(\Omega\text{.}\)</span> Therefor, <span class="process-math">\(\Omega\)</span> is called event of <span class="process-math">\(100\%\)</span> certainty. Thus,<div class="displaymath process-math" id="eqn-second-axiom-normalization">
\begin{equation}
P(\Omega) = 1\text{,}\tag{1.4.2}
\end{equation}
</div>ensures total certainty for event <span class="process-math">\(\Omega\text{.}\)</span> This is why although frequencies are proportional to probabilities, we need to divide them by total number of trials to convert them into normalized probabilities.</div></li>
<li id="subsec-Axiomatic-View-of-Probability-7-2-3"><div class="para" id="p-derived-subsec-Axiomatic-View-of-Probability-7-2-3">
<em class="alert">Additivity</em>: If two events that are disjoint, i.e., there is no situation in which both events can occur together, i.e, <span class="process-math">\(E_1 \cap E_2 = \varnothing\)</span>), the probability of either of them occuring, i.e., their union, must be sum of the probabilities of the events occuring separately.<div class="displaymath process-math" id="eqn-third-axiom-additivity">
\begin{equation}
P(E_1 \cup E_2) = P(E_1) + P(E_2),\quad \text{ if } E_1 \cap E_2 = \varnothing.\text{.}\tag{1.4.3}
\end{equation}
</div>Of course, if there was an overlap, between the two events, then, we would need to subtrat the overlap part since that would have been counted twice, once in <span class="process-math">\(E_1\)</span> and another time in <span class="process-math">\(E_2\text{.}\)</span><div class="displaymath process-math" id="eqn-sum-rule-as-fundamental">
\begin{equation}
P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2).\tag{1.4.4}
\end{equation}
</div>
</div></li>
</ol>
</div>
<div class="para logical" id="subsec-Axiomatic-View-of-Probability-8">
<div class="para">Derived results:</div>
<div class="displaymath process-math" id="subsec-Axiomatic-View-of-Probability-8-1">
\begin{align}
\amp P(\varnothing) = 0. \tag{1.4.5}\\
\amp P(E^c) = 1 - P(E), \text{ where } E^c \text{ is the complement of } E. \tag{1.4.6}\\
\amp P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2). \tag{1.4.7}
\end{align}
</div>
</div>
<div class="para logical" id="subsec-Axiomatic-View-of-Probability-9">
<div class="para">
<a href="sec-Basic-Probability.html#fig-venn-diagram-E1-E2" class="xref" data-knowl="./knowl/xref/fig-venn-diagram-E1-E2.html" data-reveal-label="Reveal" data-close-label="Close" title="Figure 1.4.1">Figure 1.4.1</a> illustrated the union formula in which two events <span class="process-math">\(E_1 = \{1,3,5\}\)</span> (odd numbers), <span class="process-math">\(E_2 = \{3,5,6\}\)</span> have an overlap. Theoretically,</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-venn-diagram-E1-E2.html ./knowl/xref/fig-venn-diagram-E1-E2.html">
\begin{equation*}
P(E_1 \cup E_2) = P(\{1,3,5,6\}) = 4/6 = 2/3.
\end{equation*}
</div>
<div class="para">Let’s see how it plays out in the <span class="process-math">\(1000\)</span> rolls simulation shown in <a href="sec-Basic-Probability.html#fig-venn-diagram-E1-E2" class="xref" data-knowl="./knowl/xref/fig-venn-diagram-E1-E2.html" data-reveal-label="Reveal" data-close-label="Close" title="Figure 1.4.1">Figure 1.4.1</a>. The left region <span class="process-math">\((181)\)</span> counts rolls of <span class="process-math">\(1\)</span> (in <span class="process-math">\(E_1\)</span> only), the right region <span class="process-math">\((155)\)</span> counts rolls of <span class="process-math">\(6\)</span> (in <span class="process-math">\(E_2\)</span> only), the overlap <span class="process-math">\((326)\)</span> counts rolls of <span class="process-math">\(3 \text{ or } 5\)</span> (in <span class="process-math">\(E_1 \cap E_2\)</span>), and a total of <span class="process-math">\(622\)</span> counts for <span class="process-math">\(1\text{ or } 3 \text{ or } 5 \text{ or } 6\text{.}\)</span> From these counts we estimate probabilities. The counts <span class="process-math">\(f_{E_1} = 181+326 = 507\text{,}\)</span> <span class="process-math">\(f(E_2) = 155+326 = 481\text{,}\)</span> <span class="process-math">\(f_{E_1 \cap E_2} = 326\)</span> and <span class="process-math">\(f_{E_1 \cup E_2} = 622\)</span> in total rolls <span class="process-math">\(N = 1000\text{.}\)</span>  From these counts we get</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-venn-diagram-E1-E2.html ./knowl/xref/fig-venn-diagram-E1-E2.html" id="subsec-Axiomatic-View-of-Probability-9-23">
\begin{align*}
\amp \hat{P}(E_1) = \frac{507}{1000} = 0.507 \\
\amp \hat{P}(E_2) = \frac{481}{1000} = 0.481 \\
\amp \hat{P}(E_1 \cap E_2) = \frac{326}{1000} = 0.326 \\
\amp \hat{P}(E_1 \cup E_2) = \frac{622}{1000} = 0.622 
\end{align*}
</div>
<div class="para">Now let’s check</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-venn-diagram-E1-E2.html ./knowl/xref/fig-venn-diagram-E1-E2.html">
\begin{equation*}
\hat{P}(E_1) + \hat{P}(E_2) - \hat{P}(E_1 \cap E_2)= 0.507 + 0.481 - 0.326 = 0.662 =  \hat{P}(E_1 \cup E_2).
\end{equation*}
</div>
</div>
<figure class="figure figure-like" id="fig-venn-diagram-E1-E2"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/venn-diagram-E1-E2.png" class="contained" alt="Venn diagram of intersecting events."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.1<span class="period">.</span></span><span class="space"> </span>Venn diagram illustrating events <span class="process-math">\(E_1 = \{1,3,5\}\)</span> (odd numbers) and <span class="process-math">\(E_2 = \{3,5,6\}\)</span> for 1,000 simulated rolls of a fair six-sided die. The code of the simulation is given below.</figcaption></figure><div class="code-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><pre class="program line-numbers"><code class="language-py">Simulating die roll for union probability

# --- DIE ROLL VENN DIAGRAM ---
import numpy as np
from matplotlib_venn import venn2
import matplotlib.pyplot as plt

np.random.seed(42)
n_trials = 1000
rolls = np.random.randint(1, 7, n_trials)

# Events
e1 = np.isin(rolls, [1, 3, 5])  # Odd numbers
e2 = np.isin(rolls, [3, 5, 6])  # 3,5,6
e1_only = np.sum(e1 \amp; ~e2)
e2_only = np.sum(e2 \amp; ~e1)
both = np.sum(e1 \amp; e2)

# Venn diagram
plt.figure(figsize=(6, 4))
venn2(subsets=(e1_only, e2_only, both), set_labels=('E1 (Odd)', 'E2 (3,5,6)'))
plt.title('Venn Diagram of Die Roll Events')
plt.savefig('venn-diagram-E1-E2.png', dpi=300)
plt.show()

# Probabilities
p_e1 = np.mean(e1)
p_e2 = np.mean(e2)
p_inter = np.mean(e1 \amp; e2)
p_union = np.mean(e1 | e2)
print(f"P(E1): {p_e1:.3f}, P(E2): {p_e2:.3f}, P(E1 ∩ E2): {p_inter:.3f}, P(E1 ∪ E2): {p_union:.3f}")
# --- END CODE ---
</code></pre></div></section><section class="subsection" id="subsec-sum-product-rules"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.2</span><span class="space"> </span><span class="title">Sum and Product Rules for Probability</span>
</h3>
<div class="para" id="subsec-sum-product-rules-2">The sum and product rules are two cornerstones of probability theory. They allow us to compute probabilities of unions and intersections of events, which are essential in many machine learning applications, from estimating marginal distributions to building classifiers.</div>
<div class="para logical" id="subsec-sum-product-rules-3">
<div class="para">
<em class="alert">Sum Rule</em>: The probability of the union of two events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> is given by:</div>
<div class="displaymath process-math" id="eqn-sum-rule">
\begin{equation}
P(A \cup B) = P(A) + P(B) - P(A \cap B).\tag{1.4.8}
\end{equation}
</div>
<div class="para">The subtraction of <span class="process-math">\(P(A \cap B)\)</span> avoids double-counting the overlap between the two events. This can be clearly understood using a Venn diagram.</div>
</div>
<figure class="figure figure-like" id="fig-venn-sum-rule"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/sum-rule-venn.png" class="contained" alt="Venn diagram showing the sum rule for two overlapping events."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.2<span class="period">.</span></span><span class="space"> </span>Illustration of the sum rule. The probability of <span class="process-math">\(A \cup B\)</span> is the sum of the shaded areas of <span class="process-math">\(A\)</span> and <span class="process-math">\(B\text{,}\)</span> minus the overlapping region <span class="process-math">\(A \cap B\)</span> that would otherwise be counted twice.</figcaption></figure><div class="para logical" id="subsec-sum-product-rules-5">
<div class="para">
<em class="alert">Product Rule</em>: The joint probability of two events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> is:</div>
<div class="displaymath process-math" id="eqn-product-rule">
\begin{equation}
P(A \cap B) = P(A|B)P(B),\tag{1.4.9}
\end{equation}
</div>
<div class="para">where <span class="process-math">\(P(A|B)\)</span> is the conditional probability of <span class="process-math">\(A\)</span> given <span class="process-math">\(B\text{.}\)</span> This factorization is the basis for probabilistic models such as Naive Bayes.</div>
</div></section><section class="subsection" id="subsec-conditional-probability-independence"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.3</span><span class="space"> </span><span class="title">Conditional Probability and Independence</span>
</h3>
<div class="para logical" id="subsec-conditional-probability-independence-2">
<div class="para">
<em class="alert">Conditional Probability</em>: The probability of an event <span class="process-math">\(A\)</span> given that event <span class="process-math">\(B\)</span> has occurred is written as</div>
<div class="displaymath process-math" id="eqn-conditional-prob-defn">
\begin{equation}
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \text{ provided that } P(B) \gt; 0\text{.}\tag{1.4.10}
\end{equation}
</div>
<div class="para">Intuitively, this represents <em class="emphasis">updating our belief about <span class="process-math">\(A\)</span></em> once we know that <span class="process-math">\(B\)</span> is true. For instance, the probability that a student passes an exam may be different depending on whether we already know they studied more than 20 hours.</div>
</div>
<article class="example example-like" id="subsec-conditional-probability-independence-3"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">1.4.3</span><span class="period">.</span><span class="space"> </span><span class="title">Probability of King if the Card is known to be Spade.</span>
</h4> <div class="para" id="subsec-conditional-probability-independence-3-2">Imagine we have a standard deck of 52 playing cards. Event <span class="process-math">\(A\)</span> = "the card is a King". Event <span class="process-math">\(B\)</span> = "the card is a Spade".</div> <div class="para" id="subsec-conditional-probability-independence-3-3">The unconditional probability of drawing a King is <span class="process-math">\(P(A) = 4/52 = 1/13\text{.}\)</span> But suppose we are told that the card drawn is a Spade. Now, the sample space is only 13 Spade cards. Out of these, exactly one is a King (the King of Spades).</div> <div class="para" id="subsec-conditional-probability-independence-3-4">Thus the conditional probability is <span class="process-math">\(P(A \mid B) = \tfrac{1}{13}\text{,}\)</span> which differs from <span class="process-math">\(P(A)\text{.}\)</span> This illustrates how new information (that the card is a Spade) changes the probability of <span class="process-math">\(A\text{.}\)</span>
</div></article><div class="para logical" id="subsec-conditional-probability-independence-4">
<div class="para">
<em class="alert">Independence</em>: Two events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> are independent if the occurrence of one does not affect the probability of the other. Formally, the eventa <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> are independent if</div>
<div class="displaymath process-math" id="eqn-independent-events-def">
\begin{equation}
P(A \cap B) = P(A)P(B).\quad \Leftrightarrow\quad  A \text{ and } B \text{ independent}.\text{.}\tag{1.4.11}
\end{equation}
</div>
<div class="para">Equivalently,</div>
<div class="displaymath process-math" id="eqn-independent-events-def-2">
\begin{equation}
P(A \mid B) = P(A)\text{ when } P(B) &gt; 0.\quad \Leftrightarrow\quad  A \text{ and } B \text{ independent}.\tag{1.4.12}
\end{equation}
</div>
<div class="para">Independence means that knowing whether <span class="process-math">\(B\)</span> occurred does not provide any new information about <span class="process-math">\(A\text{.}\)</span>
</div>
</div>
<article class="example example-like" id="subsec-conditional-probability-independence-5"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">1.4.4</span><span class="period">.</span><span class="space"> </span><span class="title">Tosses of a fair coin are independent.</span>
</h4> <div class="para" id="subsec-conditional-probability-independence-5-2">Suppose we toss a fair coin twice. Let event <span class="process-math">\(A\)</span> be "the first toss is Heads" and event <span class="process-math">\(B\)</span> be "the second toss is Heads." The sample space is <span class="process-math">\(\{HH, HT, TH, TT\}\text{.}\)</span>
</div> <div class="para" id="subsec-conditional-probability-independence-5-3">We have <span class="process-math">\(P(A) = 1/2\text{,}\)</span> <span class="process-math">\(P(B) = 1/2\text{,}\)</span> and <span class="process-math">\(P(A \cap B) = 1/4\text{.}\)</span> Therefore, <span class="process-math">\(P(A \cap B) = P(A)P(B)\text{,}\)</span> which means the events are independent.</div> <div class="para" id="subsec-conditional-probability-independence-5-4">Now let event <span class="process-math">\(C\)</span> be "at least one toss is Heads." Then <span class="process-math">\(P(C) = 3/4\text{,}\)</span> <span class="process-math">\(P(A \cap C) = 1/2\text{,}\)</span> and <span class="process-math">\(P(A \mid C) = (1/2) / (3/4) = 2/3\text{.}\)</span> But <span class="process-math">\(P(A) = 1/2\text{,}\)</span> so <span class="process-math">\(P(A \mid C) \neq P(A)\text{.}\)</span> Thus, <span class="process-math">\(A\)</span> and <span class="process-math">\(C\)</span> are not independent.</div></article><div class="para" id="subsec-conditional-probability-independence-6"><em class="alert">Example (Student Study Data):</em></div>
<div class="para logical" id="subsec-conditional-probability-independence-7">
<div class="para">Suppose we record whether students studied more than 20 hours (High Study) and whether they passed an exam (Pass). Using the dataset below, we will estimate probabilities empirically:</div>
<ul class="disc" id="subsec-conditional-probability-independence-7-1">
<li id="subsec-conditional-probability-independence-7-1-1"><div class="para" id="subsec-conditional-probability-independence-7-1-1-1">
<span class="process-math">\(P(\text{Pass})\text{:}\)</span> overall fraction of students who passed,</div></li>
<li id="subsec-conditional-probability-independence-7-1-2"><div class="para" id="subsec-conditional-probability-independence-7-1-2-1">
<span class="process-math">\(P(\text{High Study})\text{:}\)</span> fraction who studied more than 20 hours,</div></li>
<li id="subsec-conditional-probability-independence-7-1-3"><div class="para" id="subsec-conditional-probability-independence-7-1-3-1">
<span class="process-math">\(P(\text{Pass} \cap \text{High Study})\text{:}\)</span> fraction who both passed and studied more than 20 hours.</div></li>
<li id="subsec-conditional-probability-independence-7-1-4"><div class="para" id="subsec-conditional-probability-independence-7-1-4-1">
<span class="process-math">\(P(\text{Pass} \mid \text{High Study})\text{:}\)</span> Just look at the students who studied more than 20 hours (High Study), what fraction Passed the exam (Pass).</div></li>
</ul>
<div class="para">The sum rule gives <span class="process-math">\(P(\text{Pass} \cup \text{High Study}) = P(\text{Pass}) + P(\text{High Study}) - P(\text{Pass} \cap \text{High Study})\text{.}\)</span> The product rule verifies that <span class="process-math">\(P(\text{Pass} \cap \text{High Study}) = P(\text{Pass}|\text{High Study})P(\text{High Study})\text{.}\)</span>
</div>
</div>
<div class="code-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><pre class="program line-numbers"><code class="language-py">Sum and product rules with student data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(42)
data = pd.DataFrame({
    "Hours_Studied": np.random.normal(20, 5, 200).clip(0, 40),
    "Passed": np.random.binomial(1, 0.7, 200)
})
data["High_Study"] = data["Hours_Studied"] &gt; 20

# Probabilities
p_pass = data["Passed"].mean()
p_high = data["High_Study"].mean()
p_both = ((data["Passed"] == 1) \amp; (data["High_Study"] == True)).mean()
p_union = p_pass + p_high - p_both
p_conditional = p_both / p_high

print(f"P(Pass) = {p_pass:.3f}")
print(f"P(High Study) = {p_high:.3f}")
print(f"P(Pass ∩ High Study) = {p_both:.3f}")
print(f"P(Pass ∪ High Study) = {p_union:.3f}")
print(f"P(Pass|High Study) * P(High Study) = {p_conditional:.3f} * {p_high:.3f} = {p_conditional * p_high:.3f}")

# Visualize joint distribution
joint_table = pd.crosstab(data["Passed"], data["High_Study"], normalize="all")
sns.heatmap(joint_table, annot=True, cmap="Blues", fmt=".3f")
plt.xlabel("High Study Hours (&gt;20)")
plt.ylabel("Pass (0=No, 1=Yes)")
plt.title("Joint Probability of Pass and High Study Hours")
plt.savefig("joint-probability-heatmap.png", dpi=300)
plt.show()
</code></pre></div>
<figure class="figure figure-like" id="fig-joint-probability-heatmap"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/joint-probability-heatmap.png" class="contained" alt="Heatmap of joint probabilities for pass and study hours."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.5<span class="period">.</span></span><span class="space"> </span>Joint probability table of passing and high study hours. Each cell represents <span class="process-math">\(P(\text{Pass}, \text{High Study})\text{.}\)</span> Row and column sums recover marginals (<span class="process-math">\(P(\text{Pass})\)</span> and <span class="process-math">\(P(\text{High Study})\)</span>), illustrating the sum rule. The product rule is verified by comparing the joint probability with <span class="process-math">\(P(\text{Pass}|\text{High Study})P(\text{High Study})\text{.}\)</span></figcaption></figure><div class="para logical" id="subsec-conditional-probability-independence-10">
<div class="para">Let’s see how we can read the heatmap in <a href="sec-Basic-Probability.html#fig-joint-probability-heatmap" class="xref" data-knowl="./knowl/xref/fig-joint-probability-heatmap.html" data-reveal-label="Reveal" data-close-label="Close" title="Figure 1.4.5">Figure 1.4.5</a>. I will read by the rows first. Let us do notation</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html" id="subsec-conditional-probability-independence-10-2">
\begin{align*}
\amp x_1 = \text{Not Studied }\gt 20\text{ hr}, x_2 = \text{Studied }\gt 20\text{ hr}\\
\amp y_1 = \text{Passed }, y_2 = \text{Not Passed }
\end{align*}
</div>
<div class="para">Then the joint probabilities in the heatmap are:</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html" id="subsec-conditional-probability-independence-10-3">
\begin{align*}
\amp P(x_1, y_2) = 0.120,\qquad P(x_2, y_2) = 0.130\\
\amp P(x_1, y_1) = 0.420,\qquad P(x_2, y_1) = 0.330
\end{align*}
</div>
<div class="para">First we will check that the total probaility is actually <span class="process-math">\(1\text{.}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html">
\begin{equation*}
0.120 + 0.130 + 0.420 + 0.330 = 1.000.
\end{equation*}
</div>
<div class="para">Great! Now, let us the probability of Passing the exam, whether you studied or not. This is asking for probability of <span class="process-math">\(y_1\)</span> regardless of the <span class="process-math">\(X\)</span> values; So, we need to sum ovr the <span class="process-math">\(X\)</span> values.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html">
\begin{equation*}
P(\text{Pass}) = P(y1) = P(x_1, y_1) + P(x_2, y_1) = 0.420 + 0.330 = 0.750.
\end{equation*}
</div>
<div class="para">That’s pretty high chance of passing. The chance of not passing will just be it’s complement.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html">
\begin{equation*}
P(y_2) = 1 - P(y_1) = 1 - 0.750 = 0.250.
\end{equation*}
</div>
<div class="para">Now, what is the chance that a random student has actullay studied hard regardless of whatever happened in the test. That will be <span class="process-math">\(P(x_2)\text{,}\)</span> which we can get from the joint probabilities by summing over the <span class="process-math">\(Y\)</span> values while keeping the <span class="process-math">\(X\)</span> values to <span class="process-math">\(x_2\text{.}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html">
\begin{equation*}
P(x_2) = P(x_2, y_1) + P(x_2, y_2) = 0.330 + 0.130 = 0.460.
\end{equation*}
</div>
<div class="para">That would mean the probability of a random student having not studied excessively is</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html">
\begin{equation*}
P(x_1) = 1 - P(x_2) = 1 - 0.460 = 0.540.
\end{equation*}
</div>
<div class="para">Now, how about the conditional probabilities? From this heatmap, it is easy to get conditional probabilities. For instance, suppose we want to know “Had you studied more than 20 hrs <span class="process-math">\(x_2\text{,}\)</span> wat would be your change of passing <span class="process-math">\((y_1)\)</span>”</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/fig-joint-probability-heatmap.html">
\begin{equation*}
P(y_1 \mid x_2) = \frac{ P(x_2, y_1) }{ P(x_2)} = \frac{0.330}{0.460} = 0.717. 
\end{equation*}
</div>
<div class="para">Wow, this data produced by simulation showing us that had you studied excessively, your chance of passing the exam actually went down from <span class="process-math">\(0.750\)</span> to <span class="process-math">\(0.717\text{.}\)</span> Wild! Okay, now your turn of computing sme other conditional probabilities.</div>
</div></section><section class="subsection" id="subsec-probability-distributions"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.4</span><span class="space"> </span><span class="title">Probability Distributions</span>
</h3>
<div class="para" id="subsec-probability-distributions-2">Probability distributions describe how probabilities are distributed over outcomes. In machine learning, distributions model data or predictions.</div>
<div class="para" id="subsec-probability-distributions-3">
<em class="alert">Bernoulli Distribution</em>: Models a binary outcome (e.g., pass/fail) with probability <span class="process-math">\(p\text{.}\)</span> For passing an exam, <span class="process-math">\(P(\text{Pass}) = p\text{,}\)</span> <span class="process-math">\(P(\text{Fail}) = 1-p\text{.}\)</span>
</div>
<div class="para" id="subsec-probability-distributions-4">
<em class="alert">Binomial Distribution</em>: Counts successes in <span class="process-math">\(n\)</span> independent Bernoulli trials. For 10 students, the number who pass follows a binomial distribution.</div>
<div class="code-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><pre class="program line-numbers"><code class="language-py">            Binomial distribution for student passes
            
# --- BINOMIAL DISTRIBUTION ---
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

n, p = 10, 0.7  # 10 students, P(Pass) = 0.7
k = np.arange(0, 11)
pmf = binom.pmf(k, n, p)

plt.bar(k, pmf)
plt.xlabel('Number of Passes')
plt.ylabel('Probability')
plt.title('Binomial Distribution (n=10, p=0.7)')
plt.grid(True, alpha=0.3)
plt.savefig('./images/essential-probability-and-statistics/binomial-dist.png', dpi=300)
plt.show()
# --- END CODE ---
</code></pre></div>
<figure class="figure figure-like" id="fig-binomial-dist"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/binomial-dist.png" class="contained" alt="Bar plot of binomial distribution."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.6<span class="period">.</span></span><span class="space"> </span>Bar plot of the binomial probability mass function (PMF) for the number of students passing an exam out of 10, with a pass probability <span class="process-math">\(p=0.7\text{.}\)</span> Each bar represents the probability of <span class="process-math">\(k\)</span> students passing, calculated as <span class="process-math">\(P(k) = \binom{n}{k} p^k (1-p)^{n-k}\text{.}\)</span> The peak around 7 passes reflects the high likelihood of most students passing given <span class="process-math">\(p=0.7\text{.}\)</span> This distribution is critical in machine learning for modeling binary outcomes, such as predicting the number of successful predictions in a classification task.</figcaption></figure></section><section class="subsection" id="subsec-three-types-of-probabilities"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.4.5</span><span class="space"> </span><span class="title">Three Types of Probabilities</span>
</h3>
<div class="para" id="subsec-three-types-of-probabilities-2">Probability can be approached theoretically, empirically (frequentist), or subjectively (Bayesian).</div>
<ol class="decimal" id="subsec-three-types-of-probabilities-3">
<li id="subsec-three-types-of-probabilities-3-1"><div class="para" id="subsec-three-types-of-probabilities-3-1-1">
<em class="alert">Theoretical Probability</em>: Uses symmetry. For a fair die, <span class="process-math">\(P(\{1\}) = 1/6\text{.}\)</span> For even numbers, <span class="process-math">\(P(\{2,4,6\}) = 3/6 = 0.5\text{.}\)</span>
</div></li>
<li id="subsec-three-types-of-probabilities-3-2">
<div class="para logical" id="subsec-three-types-of-probabilities-3-2-1">
<div class="para">
<em class="alert">Frequentist Probability</em>: Estimates probability from trial frequencies:</div>
<div class="displaymath process-math" id="eqn-frequentist-probability-definition">
\begin{equation}
p = \lim_{N \to \infty} \frac{n}{N}\text{.}\tag{1.4.13}
\end{equation}
</div>
</div>
<div class="code-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><pre class="program line-numbers"><code class="language-py">Frequentist estimation for fair and biased dice

# --- FREQUENTIST SIMULATION ---
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
n_trials = 1000
fair_rolls = np.random.randint(1, 7, n_trials)
biased_rolls = np.random.choice([1, 2, 3, 4, 5, 6], n_trials, 
                                p=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1])

# Cumulative probabilities
cum_fair = np.cumsum(fair_rolls == 1) / np.arange(1, n_trials + 1)
cum_biased = np.cumsum(biased_rolls == 1) / np.arange(1, n_trials + 1)

plt.plot(cum_fair, label='Fair Die (P=1/6)')
plt.plot(cum_biased, label='Biased Die (P=0.2)')
plt.axhline(1/6, color='red', linestyle='--', label='Theoretical P=1/6')
plt.xlabel('Trials')
plt.ylabel('Estimated P(1)')
plt.title('Frequentist Estimates: Fair vs. Biased Die')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('frequentist-convergence.png', dpi=300)
plt.show()
# --- END CODE ---
</code></pre></div>
<figure class="figure figure-like" id="fig-frequentist-convergence"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/frequentist-convergence.png" class="contained" alt="Convergence plot for frequentist estimates."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.4.7<span class="period">.</span></span><span class="space"> </span>Plot showing the convergence of frequentist probability estimates for rolling a 1 on a fair die (<span class="process-math">\(P(1)=1/6 \approx 0.167\)</span>) and a biased die (<span class="process-math">\(P(1)=0.2\)</span>) over 1,000 trials. The fair die’s estimate (blue) fluctuates but approaches 1/6 (red dashed line), while the biased die’s estimate (orange) converges to 0.2, reflecting the higher probability of rolling a 1. This visualization demonstrates how empirical frequencies approximate true probabilities in large samples, a technique used in machine learning to estimate probabilities from training data.</figcaption></figure>
</li>
<li id="subsec-three-types-of-probabilities-3-3">
<div class="para" id="subsec-three-types-of-probabilities-3-3-1">
<em class="alert">Bayesian Probability</em>. This is also an empirical definition of probability. But, rather than give you one number for probability of an event, Bayesian gives you a probability distribution of the values of probability of the event. From that, you can work out the mean value, which you can use as one value for the probability of the event.</div>
<div class="para" id="subsec-three-types-of-probabilities-3-3-2">It is based on incorporating belief about the probability of an outcome BEFORE we even conduct the experiment and then updated this so-called prior assumption or bias with what we observe in the experiment. The updated belief is the posterior, and improved value of the probability.</div>
<div class="para" id="subsec-three-types-of-probabilities-3-3-3">Clearly, as we repeat the experiment infinitely many times, the effect of our initial belief would disappear and the answer will match the results of the frequentists’ experiments. However, since we can never do infinite number of trials, the Bayesian gives an edge in cases where we have some information about the outcome even before we start the trials.</div>
<div class="para" id="subsec-three-types-of-probabilities-3-3-4">
<em class="alert">Example:</em> This example is a little bit ahead of my presentation here as it requres a little bit of math to properly express how th Bayesian probability works. If you feel up to it, you can ahead and read on, but it’s okay to skip it for now.</div>
<div class="para logical" id="subsec-three-types-of-probabilities-3-3-5">
<div class="para">In the case of a six-sided die, suppose we want to estimate the probability <span class="process-math">\(p_1\)</span> for one-dot face up as we illustrated in the frequentist case above. First, we would need to choose a prior belief, i.e., a probability distribution for <span class="process-math">\(p_1\text{,}\)</span> i.e., how likely is any value of <span class="process-math">\(p_1\)</span> between its range of values, which will be from <span class="process-math">\(0\)</span> to <span class="process-math">\(1\text{,}\)</span> inclusive, <span class="process-math">\(0\le p_1 \le 1\text{.}\)</span> Since, we do not know which value is right, we might decide that it could be 1/2 times it will be face up and 1/2 of the time it will be not face up (I know a fair die will be 1/6 times face up, but I want to show you how even a very off prior will eventually converge to the proper value). In such cases and our trial each time being either face up true or false ( which is a case of Bernoulli trials ), it is traditional to choose a beta distribution, which has two parameters <span class="process-math">\(\alpha\)</span> and <span class="process-math">\(\beta\text{,}\)</span> with <span class="process-math">\(\alpha=1\)</span>  and <span class="process-math">\(\beta = 1\text{.}\)</span> Using symbol <span class="process-math">\(x\)</span> for <span class="process-math">\(p_1\)</span> and <span class="process-math">\(P(x)\text{,}\)</span> probability density of <span class="process-math">\(p_1\text{,}\)</span> we will write this as follows where <span class="process-math">\(0 \le x \le 1\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-beta-distribution-prior">
\begin{equation}
\text{P(x)} dx = \frac{1}{B(\alpha, \beta)}\, x^{\alpha - 1} (1-x)^{\beta - 1},\tag{1.4.14}
\end{equation}
</div>
<div class="para">where <span class="process-math">\(B(\alpha, \beta)\)</span> is beta function. The mean value of beta distribution is an important result and can be easily found.</div>
<div class="displaymath process-math" id="eqn-mean-of-beta-distribution">
\begin{equation}
\langle x \rangle = \int_0^1\, x P(x) dx = \frac{\alpha}{\alpha + \beta}.\tag{1.4.15}
\end{equation}
</div>
<div class="para">Here, I have introduced physicists’ notation for the mean of a quantity, <span class="process-math">\(\langle \cdots \rangle\text{.}\)</span> Thus, by choosing <span class="process-math">\(B(1,1)\)</span> as the prior distribution, we are assuming that somehow we suspect that <span class="process-math">\(p_1\)</span> is close to <span class="process-math">\(1/2\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-mean-beta-distribution">
\begin{equation}
\langle p_1 \rangle = \frac{\alpha}{\alpha + \beta} = \frac{1}{1+1} = \frac{1}{2} = 0.5.\tag{1.4.16}
\end{equation}
</div>
<div class="para">So, we are basically, starting way off in our belief.</div>
</div>
<div class="para logical" id="subsec-three-types-of-probabilities-3-3-6">
<div class="para">Just a side math info: Beta function is usually written in terms of factorial or Gamma function.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-beta-distribution-prior.html">
\begin{equation*}
B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)},
\end{equation*}
</div>
<div class="para">where, for integer arguments <span class="process-math">\(n\text{,}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-beta-distribution-prior.html">
\begin{equation*}
\Gamma(n) = (n-1)! = [ m! = (m)(m-1) \cdots 1 = m \times (m-1)!\ \text{with}\ 0! = 1.]
\end{equation*}
</div>
<div class="para">and in general,</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-beta-distribution-prior.html">
\begin{equation*}
\Gamma(\alpha) = \int_0^\infty\, e^{-t}\,t^{\alpha -1}\, dt.
\end{equation*}
</div>
<div class="para">To hide all the mathematical details in our work below, we will, as is normally done, just express the probability <span class="process-math">\(P(p_1)\)</span> by a simpler notation and represent Eq. <a href="sec-Basic-Probability.html#eqn-beta-distribution-prior" class="xref" data-knowl="./knowl/xref/eqn-beta-distribution-prior.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.4.14">(1.4.14)</a>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-beta-distribution-prior.html" id="eqn-probability-distribution-simple-notation">
\begin{equation}
P_1 \sim B(\alpha, \beta) = B(1,1),\tag{1.4.17}
\end{equation}
</div>
<div class="para">where instead of lower case variable name <span class="process-math">\(p_1\text{,}\)</span> we use the notation of upper case <span class="process-math">\(P_1\text{.}\)</span>
</div>
</div>
<div class="para logical" id="subsec-three-types-of-probabilities-3-3-7">
<div class="para">Let’s get back to our rolling experiment and see how our belief of the true value of <span class="process-math">\(p_1\)</span> evolves with each roll’s result. Suppose we roll the die and observe that the up face is not one, then without showing you the calculations here, which will be done later in the chapter, we use Bayes rule, to be discussed later, to show that the probability distribution now shifts to <span class="process-math">\(B(1,2)\text{.}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P_1 \sim B(1,1) \rightarrow \text{[Toss, No one]} \rightarrow \sim B(1,2) \Rightarrow \langle p_1 \rangle = \frac{1}{1+2} = \frac{1}{3}.
\end{equation*}
</div>
<div class="para">How did we go from distribution <span class="process-math">\(B(1,1)\)</span> to <span class="process-math">\(B(1,2)\text{?}\)</span> I used Bayesian theorem. We will not show the calculation here but differ to a later section.</div>
</div>
<div class="para logical" id="subsec-three-types-of-probabilities-3-3-8">
<div class="para">Toss second time, let’s say the result is a one. Then our belief will be update with this new data to <span class="process-math">\(B(2,2)\text{.}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P_1 \sim B(1,2) \rightarrow \text{[Toss, Yes one]} \rightarrow \sim B(2,2) \Rightarrow \langle p_1 \rangle = \frac{2}{4} = 0.5.
\end{equation*}
</div>
<div class="para">Toss again, say no one.</div>
<div class="displaymath process-math">
\begin{equation*}
P_1 \sim B(2,2) \rightarrow \text{[Toss, No one]} \rightarrow \sim B(2,3) \Rightarrow \langle p_1 \rangle = \frac{2}{2+3} = \frac{2}{5}.
\end{equation*}
</div>
<div class="para">We keep updating the probability distribution of <span class="process-math">\(p_1\text{.}\)</span> At any point, we can take the expectation value of the the variable <span class="process-math">\(p_1\)</span> in the current distribution to give us the "best current value" for <span class="process-math">\(p_1\text{.}\)</span> Thus, after three trials above, we will say that <span class="process-math">\(p_1 = 0.4\text{.}\)</span>
</div>
</div>
<div class="para logical" id="subsec-three-types-of-probabilities-3-3-9">
<div class="para">Suppose you continued rolling and you had the following next 7 trials:</div>
<div class="displaymath process-math">
\begin{equation*}
\text{1, not 1, not 1, not 1, not 1, 1, not 1}.
\end{equation*}
</div>
<div class="para">After these 10 trials in total, the distribution will be</div>
<div class="displaymath process-math">
\begin{equation*}
P_1 \sim B(4, 8) \Rightarrow \langle p_1 \rangle = \frac{4}{12} = \frac{1}{3}.
\end{equation*}
</div>
<div class="para">This is still far away from <span class="process-math">\(1/6\)</span> that you would expect from a fair die, but you don’t know if the die was fair. So, empirical results are all you have to go by.</div>
</div>
<div class="para logical" id="subsec-three-types-of-probabilities-3-3-10">
<div class="para">For the same rolling results, frequentists’ probability will give us the following estimate:</div>
<div class="displaymath process-math">
\begin{equation*}
p_1(\text{frequentist}) = \frac{n_1}{N} = \frac{3}{10} = 0.3.
\end{equation*}
</div>
</div>
<div class="para logical" id="subsec-three-types-of-probabilities-3-3-11">
<div class="para">They look similar. But, had you expected the die was fair, you would start with a better prior, with say <span class="process-math">\(B(1,5)\text{.}\)</span> Then the 10 trials would update to</div>
<div class="displaymath process-math">
\begin{equation*}
P_1 \sim B(4, 12) \Rightarrow \langle p_1\rangle = \frac{4}{4+12} = \frac{1}{4} = 0.25.
\end{equation*}
</div>
<div class="para">It would have revealed if the die was not a fair die. It’s either not a fair die or we have rolled it too few times.</div>
</div>
</li>
</ol></section><section class="conclusion" id="sec-Basic-Probability-8"><h3 class="heading">Conclusion.<span></span>
</h3> <div class="para" id="sec-Basic-Probability-8-2">Probability provides the foundation for modeling uncertainty in machine learning. Axioms define the rules, while theoretical, frequentist, and Bayesian approaches offer different perspectives. Conditional probability and distributions are used in machine learning models to make predictions.</div></section></section></div>
<div class="ptx-content-footer">
<a class="previous-button button" href="sec-Numerical-and-Categorical-Data.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Top</span></a><a class="next-button button" href="sec-Random-Variables-and-Probabilities.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
