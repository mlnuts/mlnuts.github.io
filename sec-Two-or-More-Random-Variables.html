<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Two or More Random Variables</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="ML Notes">
<script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math"
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "_static/pretext/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/themes/prism.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-numbers/prism-line-numbers.min.js" integrity="sha512-dubtf8xMHSQlExGRQ5R7toxHLgSDZ0K7AunqPWHXmJQ8XyVIG19S1T95gBxlAeGOK02P4Da2RTnQz0Za0H0ebQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-numbers/prism-line-numbers.min.css" integrity="sha512-cbQXwDFK7lj2Fqfkuxbo5iD1dSbLlJGXGpfTDqbggqjHJeyzx88I3rfwjS38WJag/ihH7lzuGlGHpDBymLirZQ==" crossorigin="anonymous" referrerpolicy="no-referrer">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-highlight/prism-line-highlight.min.css" integrity="sha512-nXlJLUeqPMp1Q3+Bd8Qds8tXeRVQscMscwysJm821C++9w6WtsFbJjPenZ8cQVMXyqSAismveQJc0C1splFDCA==" crossorigin="anonymous" referrerpolicy="no-referrer">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/line-highlight/prism-line-highlight.min.js" integrity="sha512-93uCmm0q+qO5Lb1huDqr7tywS8A2TFA+1/WHvyiWaK6/pvsFl6USnILagntBx8JnVbQH5s3n0vQZY6xNthNfKA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="_static/pretext/js/pretext_search.js"></script><link href="_static/pretext/css/pretext_search.css" rel="stylesheet" type="text/css">
<script src="_static/pretext/js/lib/jquery.min.js"></script><script src="_static/pretext/js/lib/jquery.sticky.js"></script><script src="_static/pretext/js/lib/jquery.espy.min.js"></script><script src="_static/pretext/js/pretext.js"></script><script src="_static/pretext/js/pretext_add_on.js?x=1"></script><script src="_static/pretext/js/user_preferences.js"></script><script src="_static/pretext/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
<link href="_static/pretext/css/pretext.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/shell_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/banner_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/navbar_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/toc_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/knowls_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/style_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/colors_default.css" rel="stylesheet" type="text/css">
<link href="_static/pretext/css/setcolors.css" rel="stylesheet" type="text/css">
<style>
#subsec-Central-Tendency-3 { counter-set: subsec-Central-Tendency-3 0; }
#subsec-Central-Tendency-3 > li::marker { content: ""counter(subsec-Central-Tendency-3,decimal)"."; }
#subsec-Central-Tendency-3 > li { counter-increment: subsec-Central-Tendency-3; }
#subsec-Dispersion-3 { counter-set: subsec-Dispersion-3 0; }
#subsec-Dispersion-3 > li::marker { content: ""counter(subsec-Dispersion-3,decimal)"."; }
#subsec-Dispersion-3 > li { counter-increment: subsec-Dispersion-3; }
#subsec-Pandas-2-1-1 { counter-set: subsec-Pandas-2-1-1 0; }
#subsec-Pandas-2-1-1 > li::marker { content: ""counter(subsec-Pandas-2-1-1,decimal)"."; }
#subsec-Pandas-2-1-1 > li { counter-increment: subsec-Pandas-2-1-1; }
#subsec-Pandas-4-2 { counter-set: subsec-Pandas-4-2 0; }
#subsec-Pandas-4-2 > li::marker { content: ""counter(subsec-Pandas-4-2,decimal)"."; }
#subsec-Pandas-4-2 > li { counter-increment: subsec-Pandas-4-2; }
#sec-Numerical-and-Categorical-Data-2-1-1 { counter-set: sec-Numerical-and-Categorical-Data-2-1-1 0; }
#sec-Numerical-and-Categorical-Data-2-1-1 > li::marker { content: ""counter(sec-Numerical-and-Categorical-Data-2-1-1,decimal)"."; }
#sec-Numerical-and-Categorical-Data-2-1-1 > li { counter-increment: sec-Numerical-and-Categorical-Data-2-1-1; }
#subsec-Axiomatic-View-of-Probability-7-2 { counter-set: subsec-Axiomatic-View-of-Probability-7-2 0; }
#subsec-Axiomatic-View-of-Probability-7-2 > li::marker { content: ""counter(subsec-Axiomatic-View-of-Probability-7-2,decimal)""; }
#subsec-Axiomatic-View-of-Probability-7-2 > li { counter-increment: subsec-Axiomatic-View-of-Probability-7-2; }
#subsec-three-types-of-probabilities-3 { counter-set: subsec-three-types-of-probabilities-3 0; }
#subsec-three-types-of-probabilities-3 > li::marker { content: ""counter(subsec-three-types-of-probabilities-3,decimal)"."; }
#subsec-three-types-of-probabilities-3 > li { counter-increment: subsec-three-types-of-probabilities-3; }
#subsub-Poisson-Process-3-5 { counter-set: subsub-Poisson-Process-3-5 0; }
#subsub-Poisson-Process-3-5 > li::marker { content: ""counter(subsub-Poisson-Process-3-5,decimal)"."; }
#subsub-Poisson-Process-3-5 > li { counter-increment: subsub-Poisson-Process-3-5; }
#subsub-Poisson-Process-4-2 { counter-set: subsub-Poisson-Process-4-2 0; }
#subsub-Poisson-Process-4-2 > li::marker { content: ""counter(subsub-Poisson-Process-4-2,decimal)"."; }
#subsub-Poisson-Process-4-2 > li { counter-increment: subsub-Poisson-Process-4-2; }
#subsec-Hypothesis-Testing-4 { counter-set: subsec-Hypothesis-Testing-4 0; }
#subsec-Hypothesis-Testing-4 > li::marker { content: ""counter(subsec-Hypothesis-Testing-4,decimal)"."; }
#subsec-Hypothesis-Testing-4 > li { counter-increment: subsec-Hypothesis-Testing-4; }
</style>
</head>
<body id="ML-Notes" class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" target="_blank" href=""></a><div class="title-container">
<h1 class="heading"><a href="my-ML-Notes.html"><span class="title">ML Notes:</span> <span class="subtitle">Theoretical and Practical ML Concepts</span></a></h1>
<p class="byline"></p>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" title="Contents"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5d2;</span><span class="name">Contents</span></button><div class="searchbox">
<div class="searchwidget"><button id="searchbutton" class="searchbutton button" type="button" title="Search book"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe8b6;</span><span class="name">Search Book</span></button></div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<div class="search-results-controls">
<input aria-label="Search term" id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search term"><button title="Close search" id="closesearchresults" class="closesearchresults"><span class="material-symbols-outlined">close</span></button>
</div>
<h2 class="search-results-heading">Search Results: </h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div>
<span class="nav-other-controls"></span><span class="treebuttons"><a class="previous-button button" href="sec-Random-Variables-and-Probabilities.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="up-button button" href="ch-Essential-Probability-and-Statistics.html" title="Up"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Up</span></a><a class="next-button button" href="sec-Example-Discrete-Probability-Distributions.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a></span></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\N}{\mathbb N} \newcommand{\Z}{\mathbb Z} \newcommand{\Q}{\mathbb Q} \newcommand{\R}{\mathbb R}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural contains-active toc-item-list">
<li class="toc-item toc-frontmatter"><div class="toc-title-box"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div></li>
<li class="toc-item toc-chapter contains-active">
<div class="toc-title-box"><a href="ch-Essential-Probability-and-Statistics.html" class="internal"><span class="codenumber">1</span> <span class="title">Essential Probability and Statistics</span></a></div>
<ul class="structural toc-item-list contains-active">
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Descriptive-Statistics.html" class="internal"><span class="codenumber">1.1</span> <span class="title">Descriptive Statistics</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Central-Tendency" class="internal"><span class="codenumber">1.1.1</span> <span class="title">Measures of Central Tendency</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Dispersion" class="internal"><span class="codenumber">1.1.2</span> <span class="title">Measures of Dispersion</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#subsec-Distribution-Shape" class="internal"><span class="codenumber">1.1.3</span> <span class="title">Distribution Shape</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis" class="internal"><span class="codenumber">1.1.4</span> <span class="title">Kurtosis: Checking Out the Tails</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#types-of-kurtosis" class="internal"><span class="codenumber">1.1.4.1</span> <span class="title">Types of Kurtosis</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-formula" class="internal"><span class="codenumber">1.1.4.2</span> <span class="title">How’s It Calculated?</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-examples" class="internal"><span class="codenumber">1.1.4.3</span> <span class="title">Real-World Examples</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-visual" class="internal"><span class="codenumber">1.1.4.4</span> <span class="title">Seeing Kurtosis in Action</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-python" class="internal"><span class="codenumber">1.1.4.5</span> <span class="title">Calculating Kurtosis with Python</span></a></div></li>
<li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Descriptive-Statistics.html#kurtosis-wrapup" class="internal"><span class="codenumber">1.1.4.6</span> <span class="title">Why Kurtosis Matters</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Computation and Visualization Tools</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#sec-useful-descriptive-statistics-tools-3" class="internal"><span class="codenumber">1.2.1</span> <span class="title">The Power of NumPy and SciPy</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-Pandas" class="internal"><span class="codenumber">1.2.2</span> <span class="title">Pandas: Data Manipulation and Analysis</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-useful-descriptive-statistics-tools.html#subsec-Visualization" class="internal"><span class="codenumber">1.2.3</span> <span class="title">Visualization with Matplotlib and Seaborn</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html" class="internal"><span class="codenumber">1.3</span> <span class="title">Numerical and Categorical Data</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-Categorical-Data" class="internal"><span class="codenumber">1.3.1</span> <span class="title">Categorical Data and One-Hot Encoding</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-Ordinal-Data" class="internal"><span class="codenumber">1.3.2</span> <span class="title">Ordinal Data and Safe Encoding</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-Numerical-Data" class="internal"><span class="codenumber">1.3.3</span> <span class="title">Numerical Data: Discrete vs Continuous</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Numerical-and-Categorical-Data.html#subsec-data-type-summary" class="internal"><span class="codenumber">1.3.4</span> <span class="title">Data Type Summary</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Basic-Probability.html" class="internal"><span class="codenumber">1.4</span> <span class="title">Basic Probability for Machine Learning</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-Axiomatic-View-of-Probability" class="internal"><span class="codenumber">1.4.1</span> <span class="title">Axiomatic View of Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-sum-product-rules" class="internal"><span class="codenumber">1.4.2</span> <span class="title">Sum and Product Rules for Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-conditional-probability-independence" class="internal"><span class="codenumber">1.4.3</span> <span class="title">Conditional Probability and Independence</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-probability-distributions" class="internal"><span class="codenumber">1.4.4</span> <span class="title">Probability Distributions</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Basic-Probability.html#subsec-three-types-of-probabilities" class="internal"><span class="codenumber">1.4.5</span> <span class="title">Three Types of Probabilities</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html" class="internal"><span class="codenumber">1.5</span> <span class="title">Random Variables and Probabilities</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Random-Variables" class="internal"><span class="codenumber">1.5.1</span> <span class="title">Random Variables, Probabilities, and Expectations</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Probability-Mass-Function" class="internal"><span class="codenumber">1.5.2</span> <span class="title">Probability Mass Function</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Expectation-Values" class="internal"><span class="codenumber">1.5.3</span> <span class="title">Expectation Values</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Probability-Distribution-of-Continuous-Variable" class="internal"><span class="codenumber">1.5.4</span> <span class="title">Probability Density of Continuous Variables</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Random-Variables-and-Probabilities.html#subsec-Cumulative-Distribution-Function" class="internal"><span class="codenumber">1.5.5</span> <span class="title">Cumulative Distribution Function (CDF)</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section active">
<div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html" class="internal"><span class="codenumber">1.6</span> <span class="title">Two or More Random Variables</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Joint-Probability" class="internal"><span class="codenumber">1.6.1</span> <span class="title">Joint Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Marginal-Probability" class="internal"><span class="codenumber">1.6.2</span> <span class="title">Marginal Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Conditional-Probability" class="internal"><span class="codenumber">1.6.3</span> <span class="title">Conditional Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-conditional-probability-from-joint-probability" class="internal"><span class="codenumber">1.6.4</span> <span class="title">Bayes’ Rule</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Independent-Variables" class="internal"><span class="codenumber">1.6.5</span> <span class="title">Independent Random Variables</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Two-or-More-Random-Variables.html#subsec-Covariance-and-Correlation" class="internal"><span class="codenumber">1.6.6</span> <span class="title">Covariance and Correlation</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html" class="internal"><span class="codenumber">1.7</span> <span class="title">Example Discrete Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Bernoulli-Distribution" class="internal"><span class="codenumber">1.7.1</span> <span class="title">Bernoulli Distribution</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Binomial-Distribution" class="internal"><span class="codenumber">1.7.2</span> <span class="title">Binomial Distribution</span></a></div></li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsec-Poisson-Distribution" class="internal"><span class="codenumber">1.7.3</span> <span class="title">Poisson Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Example-Discrete-Probability-Distributions.html#subsub-Poisson-Process" class="internal"><span class="codenumber">1.7.3.1</span> <span class="title">Poisson Process</span></a></div></li></ul>
</li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Continuous-Probability-Distributions.html" class="internal"><span class="codenumber">1.8</span> <span class="title">Example Continuous Probability Distributions</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Continuous-Probability-Distributions.html#subsec-Uniform-Distribution" class="internal"><span class="codenumber">1.8.1</span> <span class="title">Uniform Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Continuous-Probability-Distributions.html#subsubsec-Inverse-Uniform-CDF" class="internal"><span class="codenumber">1.8.1.1</span> <span class="title">Inverse Uniform CDF</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection">
<div class="toc-title-box"><a href="sec-Continuous-Probability-Distributions.html#subsec-Normal-Distribution" class="internal"><span class="codenumber">1.8.2</span> <span class="title">Normal (Gaussian) Distribution</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-subsubsection"><div class="toc-title-box"><a href="sec-Continuous-Probability-Distributions.html#subsubsec-Inverse-CDF-and-Sampling" class="internal"><span class="codenumber">1.8.2.1</span> <span class="title">Inverse CDF and Sampling</span></a></div></li></ul>
</li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Continuous-Probability-Distributions.html#subsec-Exponential-Distribution" class="internal"><span class="codenumber">1.8.3</span> <span class="title">Exponential Distribution</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-LLN-and-CLT.html" class="internal"><span class="codenumber">1.9</span> <span class="title">Law of Large Numbers and Central Limit Theorem</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Law-of-Large-Numbers" class="internal"><span class="codenumber">1.9.1</span> <span class="title">Law of Large Numbers</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Central-Limit-Theorem" class="internal"><span class="codenumber">1.9.2</span> <span class="title">Central Limit Theorem</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-LLN-vs-CLT" class="internal"><span class="codenumber">1.9.3</span> <span class="title">LLN vs. CLT</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Berry-Esseen" class="internal"><span class="codenumber">1.9.4</span> <span class="title">Berry-Esseen Theorem</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-LLN-and-CLT.html#subsec-Why-Large-n-Matters" class="internal"><span class="codenumber">1.9.5</span> <span class="title">Why LLN and CLT Matter</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-Inferential-Statistics.html" class="internal"><span class="codenumber">1.10</span> <span class="title">Inferential Statistics</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Point-Estimation" class="internal"><span class="codenumber">1.10.1</span> <span class="title">Point Estimation</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Sampling-Distributions" class="internal"><span class="codenumber">1.10.2</span> <span class="title">Sampling Distributions</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Hypothesis-Testing" class="internal"><span class="codenumber">1.10.3</span> <span class="title">Hypothesis Testing</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Confidence-Intervals" class="internal"><span class="codenumber">1.10.4</span> <span class="title">Confidence Intervals</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Types-of-Errors" class="internal"><span class="codenumber">1.10.5</span> <span class="title">Types of Errors</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Power-Effect-Size" class="internal"><span class="codenumber">1.10.6</span> <span class="title">Statistical Power and Effect Size</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Multiple-Testing" class="internal"><span class="codenumber">1.10.7</span> <span class="title">Multiple Testing</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-Inferential-Statistics.html#subsec-Other-Tests" class="internal"><span class="codenumber">1.10.8</span> <span class="title">Common Statistical Tests</span></a></div></li>
</ul>
</li>
</ul>
</li>
<li class="toc-item toc-backmatter"><div class="toc-title-box"><a href="backmatter.html" class="internal"><span class="title">Backmatter</span></a></div></li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content"><section class="section" id="sec-Two-or-More-Random-Variables"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">1.6</span><span class="space"> </span><span class="title">Two or More Random Variables</span>
</h2>
<section class="introduction" id="sec-Two-or-More-Random-Variables-2"><div class="para logical" id="sec-Two-or-More-Random-Variables-2-1">
<div class="para">Suppose we have two or more random variables that characterize our data. For example, we may be interested in studying heights and weights and weights of all children in age <span class="process-math">\(1\)</span> to <span class="process-math">\(18\text{.}\)</span> Our probabilities of interest will look like</div>
<div class="displaymath process-math">
\begin{equation*}
P( 2\,\text{ft} \le \text{Height} \le 4\,\text{ft}\quad \textrm{AND}\quad  20\,\text{kg} \le \text{Weight} \le 50\,\text{kg} ).
\end{equation*}
</div>
<div class="para">This kind of probability is called a <dfn class="terminology">joint probability</dfn> - it is probability of two events together. You might think of the expressions on either side of <span class="process-math">\(\textrm{AND}\)</span> as events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> and replace <span class="process-math">\(\textrm{AND}\)</span> by the set symbol <span class="process-math">\(\cap\text{,}\)</span> and write it more abstractly as</div>
<div class="displaymath process-math">
\begin{equation*}
P( A \cap B).
\end{equation*}
</div>
<div class="para">Or, more generally, you might think of this as a statement about random variables <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> and write it as</div>
<div class="displaymath process-math">
\begin{equation*}
P(X,Y).
\end{equation*}
</div>
<div class="para">Sometimes, a more verbose notation is used:</div>
<div class="displaymath process-math">
\begin{equation*}
P_{X,Y}(x,y).
\end{equation*}
</div>
</div> <div class="para logical" id="sec-Two-or-More-Random-Variables-2-2">
<div class="para">We may also be interested in just to probability of some range of Height itself, regardless of the weights.</div>
<div class="displaymath process-math">
\begin{equation*}
P( 2\,\text{ft} \le \text{Height} \le 4\,\text{ft} ) \equiv P(A) \equiv P(X) \equiv P_X(x).
\end{equation*}
</div>
<div class="para">Since we have collected data on two variables, but we are looking at only one of the variables, we say that we have <em class="alert">margined out</em> the weight variable. This type of probability is called <dfn class="terminology">marginal probability</dfn>, here of Height.</div>
</div> <div class="para logical" id="sec-Two-or-More-Random-Variables-2-3">
<div class="para">We may instead be interested in a more complicated type of probaility: what if we look at children of Weight between <span class="process-math">\(20\text{ kg}\)</span> and <span class="process-math">\(50\text{ kg}\text{,}\)</span> what will be the probability of a Height range?</div>
<div class="displaymath process-math">
\begin{equation*}
\text{ Given } \left( 20\,\text{kg} \le \text{Weight} \le 50\,\text{kg}\right),\ P( 2\,\text{ft} \le \text{Height} \le 4\,\text{ft} ) = ?
\end{equation*}
</div>
<div class="para">We write these using a different symbol to indicate the constraining part of this sentence, i.e, the given part.</div>
<div class="displaymath process-math">
\begin{equation*}
P( \left( 2\,\text{ft} \le \text{Height} \le 4\,\text{ft}\right) \mid  \left( 20\,\text{kg} \le \text{Weight} \le 40 \right) ) \equiv P (X \mid Y)\equiv P_{X\mid Y}(x).
\end{equation*}
</div>
<div class="para">Or, we may be interested in a question that is other way around,</div>
<div class="displaymath process-math">
\begin{equation*}
P( \left( 20\,\text{kg} \le \text{Weight} \le 40 \right) \mid  \left( 2\,\text{ft} \le \text{Height} \le 4\,\text{ft}\right)  ) \equiv P (Y \mid X) \equiv P_{Y\mid X}(y).
\end{equation*}
</div>
<div class="para">The answers to the two questions will be different since you would be selcting different distibutions based on the given condition; <span class="process-math">\(P_{X\mid Y}(x)\)</span> is probability in the <span class="process-math">\(X\)</span>-space and <span class="process-math">\(P_{Y\mid X}(y)\)</span> is probability in the <span class="process-math">\(Y\)</span>-space. These types of probability are called <dfn class="terminology">Conditional Probability</dfn>. Conditional probability plays an important role in Machine Learning Algorithms.</div>
</div> <div class="para" id="sec-Two-or-More-Random-Variables-2-4">Below, we will look at these probabilities in more detail.</div></section><section class="subsection" id="subsec-Joint-Probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.6.1</span><span class="space"> </span><span class="title">Joint Probability</span>
</h3>
<div class="para" id="subsec-Joint-Probability-2">Now, let’s dig in a little deeper into joint probability and work out an example.</div>
<div class="para logical" id="subsec-Joint-Probability-3">
<div class="para">Suppose we have two discrete random variables <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{.}\)</span> Suppose the unique values that <span class="process-math">\(A\)</span> can take are <span class="process-math">\(a_1, a_2, \cdots, a_m\)</span> and <span class="process-math">\(B\)</span> can take are <span class="process-math">\(b_1, b_2, \cdots, b_n\text{.}\)</span> Then, the pair <span class="process-math">\((A, B)\)</span> can take <span class="process-math">\(m\times n \)</span> value pairs <span class="process-math">\((a_i, b_j)\)</span> with <span class="process-math">\(1 \le i \le m\)</span> and <span class="process-math">\(1 \le j \le n\text{.}\)</span> Joint porbability  <span class="process-math">\(P(A,B)\text{,}\)</span> i.e., the Joint PMF, assigns probabilities to every unique pairs <span class="process-math">\((a_i, b_j)\)</span> so that they are normalized to sum to <span class="process-math">\(1\text{.}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html" id="eqn-discrete-joint-probability">
\begin{equation}
P(A=a_i, B=b_j) = p_{ij}, \quad \sum_i \sum_j p_{ij} = 1.\tag{1.6.1}
\end{equation}
</div>
<div class="para">If you are dealing with continuous variables <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{,}\)</span> then we can only talk about probabilities in ranges since both <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> take values on the real line <span class="process-math">\(\mathbb{R}\text{.}\)</span> Here, the space of all possibilities will be the <span class="process-math">\(xy\)</span>-plane, which is <span class="process-math">\(\mathbb{R}^2\text{.}\)</span> Thus joint probability in an infinitesimal rectangle will be</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html" id="eqn-continuous-joint-probability">
\begin{equation}
P(x \le X \le x+dx, y \le Y \le y+dy) = \rho(x,y)\,dxdy,\tag{1.6.2}
\end{equation}
</div>
<div class="para">where the Joint PDF <span class="process-math">\(\rho(x,y)\)</span> wil be normalized by the following integral:</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html">
\begin{equation*}
\int_{-\infty}^\infty \int_{-\infty}^\infty\,\rho(x,y)\,dxdy= 1.
\end{equation*}
</div>
<div class="para">In a weird way the Joint PMF <span class="process-math">\(p_{ij}\)</span> of the discrete case corresponds to a  the Joint PDF except that you need to take integral of the later in place of sum for the PMF.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html" id="eqn-Joint-PMF-Joint-PDF-correspondence">
\begin{equation}
\text{Discrete : PMF, Sum} \longleftrightarrow \text{Continuous : PDF, Integral}.\tag{1.6.3}
\end{equation}
</div>
<div class="para"><article class="example example-like" id="ex-Patient-disease-and-test-results"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">1.6.1</span><span class="period">.</span><span class="space"> </span><span class="title">Patient disease and test results.</span>
</h4> <div class="para logical" id="ex-Patient-disease-and-test-results-2">
<div class="para">Consider a dataset of 1,000 patients, with two discrete random variables <span class="process-math">\(X\)</span> indicating disease status and <span class="process-math">\(Y\)</span> indicating test result on a partcular test for the said disease. For each patient, the disease status variable <span class="process-math">\(X\)</span> has on of the two values:</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html">
\begin{equation*}
X = \begin{cases}
\textrm{D} \amp \text{if the patient has disease}\\
\textrm{N} \amp \text{if the patient has no disease}
\end{cases}
\end{equation*}
</div>
<div class="para">Similarly, the test result variable <span class="process-math">\(Y\)</span> for each patient has one of the two values</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html">
\begin{equation*}
Y = \begin{cases}
+ \amp \text{if the test was positive}\\
- \amp \text{if the test was negative}
\end{cases}
\end{equation*}
</div>
<div class="para">Suppose, in <span class="process-math">\(1000\)</span> people in the dataset you found <span class="process-math">\(150\)</span> people had tested positive and also had the disease. This will be the joint probability</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html">
\begin{equation*}
p_{D+} = \frac{150}{1000} = 0.150.
\end{equation*}
</div>
<div class="para">Suppose, you found that <span class="process-math">\(50\)</span> patients who had the disease but somehow tested negative. That would be</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html">
\begin{equation*}
p_{D-} = \frac{50}{1000} = 0.050
\end{equation*}
</div>
<div class="para">Now, it turned out that <span class="process-math">\(300\)</span> people who didn’t have the disease but their tests came out positive.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html">
\begin{equation*}
p_{N+} = \frac{300}{1000} = 0.300.
\end{equation*}
</div>
<div class="para">Finally, <span class="process-math">\(500\)</span> people that didn’t have the disease were also found to be negative on the test.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html">
\begin{equation*}
p_{N-} = \frac{500}{1000} = 0.500.
\end{equation*}
</div>
<div class="para">These joint probabilities are usually organized in a table form as shown in <a href="sec-Two-or-More-Random-Variables.html#tab-Patient-Disease-and-Test-Results" class="xref" data-knowl="./knowl/xref/tab-Patient-Disease-and-Test-Results.html" data-reveal-label="Reveal" data-close-label="Close" title="Table 1.6.2: Disease and Test Results">Table 1.6.2</a>
</div>
</div> <figure class="table table-like" id="tab-Patient-Disease-and-Test-Results"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">1.6.2<span class="period">.</span></span><span class="space"> </span>Disease and Test Results</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b3 r0 l0 t0 lines"></td>
<td class="l m b3 r0 l0 t0 lines"></td>
<td class="l m b3 r0 l0 t0 lines"></td>
</tr>
<tr>
<td class="l m b1 r1 l0 t0 lines"></td>
<td class="l m b1 r1 l0 t0 lines">Test: <span class="process-math">\(+\)</span>
</td>
<td class="l m b1 r0 l0 t0 lines">Test: <span class="process-math">\(-\)</span>
</td>
</tr>
<tr>
<td class="l m b0 r1 l0 t0 lines">Disease (D)</td>
<td class="l m b0 r1 l0 t0 lines"><span class="process-math">\(p_{D+} = 0.150\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(p_{D-} = 0.050\)</span></td>
</tr>
<tr>
<td class="l m b3 r1 l0 t0 lines">No Disease (N)</td>
<td class="l m b3 r1 l0 t0 lines"><span class="process-math">\(p_{N+}=0.300\)</span></td>
<td class="l m b3 r0 l0 t0 lines"><span class="process-math">\(p_{N-}=0.500\)</span></td>
</tr>
</table></div></figure></article></div>
</div></section><section class="subsection" id="subsec-Marginal-Probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.6.2</span><span class="space"> </span><span class="title">Marginal Probability</span>
</h3>
<div class="para" id="subsec-Marginal-Probability-2">
<dfn class="terminology">Marginal probability</dfn> is an unconditional probability of a single event. for instance marginal probability of event <span class="process-math">\(A\)</span> is the same old regular probability of <span class="process-math">\(A\text{,}\)</span> viz. <span class="process-math">\(P(A)\)</span> we are aware of. In the context of joint probabilities, we use the phrase marginal probability. Outside of this context, it is just probability.</div>
<div class="para" id="subsec-Marginal-Probability-3">So, what is it’s relation to the joint probability? To present the answer, let’s look at joint probability in the context of two discrete random variables <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{,}\)</span> which take values in finite sets <span class="process-math">\(\{x_1, x_2, \cdots x_m \}\)</span>  and <span class="process-math">\(\{y_1, y_2, \cdots y_n \}\text{,}\)</span> respectively. Let <span class="process-math">\(P(X,Y)\)</span> be their joint probability, meaning collections of <span class="process-math">\(P(X=x_i,\,Y=y_j)\)</span> pairs. Marginalized probabilities will be probabilities of events like <span class="process-math">\(P(X=x_1)\text{,}\)</span> <span class="process-math">\(P(X=x_2)\text{,}\)</span> etc. without any consideration of the variable <span class="process-math">\(Y\text{.}\)</span> Or, alternatively, <span class="process-math">\(P(Y=y_1)\text{,}\)</span> <span class="process-math">\(P(Y=y_2)\text{,}\)</span> etc. without any consideration of the variable <span class="process-math">\(X\text{.}\)</span>
</div>
<div class="para logical" id="subsec-Marginal-Probability-4">
<div class="para">We can get <span class="process-math">\(P(X=x_1)\)</span> from <span class="process-math">\(P(X=x_1, Y=\text{any }y_j\text{.}\)</span> That means, we need to sum over all the values of <span class="process-math">\(Y\)</span> in <span class="process-math">\(P(X=x_1, Y=y)\text{.}\)</span> Rather than clutter the formula  we are going to use the following notation in equations.</div>
<div class="displaymath process-math">
\begin{equation*}
P(x_i, y_j) \equiv P(X=x_i,\, Y=y_j)
\end{equation*}
</div>
<div class="para">Therefore, marginal probability <span class="process-math">\(P(x_1) \equiv P(X=x_1)\text{:}\)</span>
</div>
<div class="displaymath process-math" id="eqn-Marginal-Prob-From-Joint-Prob">
\begin{equation}
P(x_1) = P(x_1, y_1) + P(x_1, y_2) + \cdots + P(x_1, y_n).\tag{1.6.4}
\end{equation}
</div>
<div class="para">For arbitrary <span class="process-math">\(x_i\text{:}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P(x_i) = P(x_i, y_1) + P(x_i, y_2) + \cdots + P(x_i, y_n).
\end{equation*}
</div>
<div class="para">Of course, we can write this in a compact notation and drop the subscripts.</div>
<div class="displaymath process-math">
\begin{equation*}
P(x) = \sum_y\, P(x,y).
\end{equation*}
</div>
<div class="para">If <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> were continuois variables, we will work with the PDF and replace sum by an integral, in this case integral over <span class="process-math">\(y\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-marginal-continuous-from-joint-PDF">
\begin{equation}
\rho(x) = \int_{-\infty}^{\infty}\, \rho(x,y)\, dy.\tag{1.6.5}
\end{equation}
</div>
<div class="para">Note that we are using same symbol <span class="process-math">\(\rho\)</span> for TWO DIFFERENT PDFs. To remove the confusion, subscripts are used to remind the random variable names.</div>
<div class="displaymath process-math">
\begin{equation*}
\rho_X(x) = \int_{-\infty}^{\infty}\, \rho_{XY}(x,y)\, dy.
\end{equation*}
</div>
</div>
<article class="example example-like" id="subsec-Marginal-Probability-5"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">1.6.3</span><span class="period">.</span><span class="space"> </span><span class="title">Marginal Probabilites from Joint Probability Table.</span>
</h4> <div class="para logical" id="subsec-Marginal-Probability-5-2">
<div class="para">Let’s work out Marginal probabilities <span class="process-math">\(P(D)\text{,}\)</span> <span class="process-math">\(P(N)\text{,}\)</span> <span class="process-math">\(P(+)\text{,}\)</span> anbd <span class="process-math">\(P(-)\)</span> from the joint probabilites given in <a href="sec-Two-or-More-Random-Variables.html#tab-Patient-Disease-and-Test-Results" class="xref" data-knowl="./knowl/xref/tab-Patient-Disease-and-Test-Results.html" data-reveal-label="Reveal" data-close-label="Close" title="Table 1.6.2: Disease and Test Results">Table 1.6.2</a>.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html" id="subsec-Marginal-Probability-5-2-6">
\begin{align*}
\amp P(D) = P(D, +) + P(D, -) = 0.150 + 0.050 = 0.200 \\
\amp P(N) = P(N, +) + P(N, -) = 0.300 + 0.500 = 0.800
\end{align*}
</div>
<div class="para">These make up the PMF of the random variable “Disease Status”, regardless of what ever happened in the tests. The PMF of the “Test Result” variable regarless of the “Disease Status” are</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/tab-Patient-Disease-and-Test-Results.html" id="subsec-Marginal-Probability-5-2-10">
\begin{align*}
\amp P(+) = P(D, +) + P(N, +) = 0.150 + 0.300 = 0.450 \\
\amp P(-) = P(D, -) + P(N, -) = 0.050 + 0.500 = 0.550
\end{align*}
</div>
</div> <div class="para" id="subsec-Marginal-Probability-5-3">Often, we display joint and marginal probabilities in a heatmap as shown in <a href="sec-Two-or-More-Random-Variables.html#fig-joint-heatmap" class="xref" data-knowl="./knowl/xref/fig-joint-heatmap.html" data-reveal-label="Reveal" data-close-label="Close" title="Figure 1.6.4">Figure 1.6.4</a> with the marginals shown in the margins outside the Table.</div> <figure class="figure figure-like" id="fig-joint-heatmap"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/joint_heatmap.png" class="contained" alt="Heatmap of joint probabilities with marginals."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.6.4<span class="period">.</span></span><span class="space"> </span>Heatmap of joint probabilities for disease status (<span class="process-math">\(X \in \{D, N\}\)</span>) and test result (<span class="process-math">\(Y \in \{+, -\}\)</span>) from <span class="process-math">\(1,000\)</span> patients (<a href="sec-Two-or-More-Random-Variables.html#tab-patient-data-for-jt-and-conditional-probs" class="xref" data-knowl="./knowl/xref/tab-patient-data-for-jt-and-conditional-probs.html" data-reveal-label="Reveal" data-close-label="Close" title="Table 1.6.6: Patient Counts (Disease vs. Test)">Table 1.6.6</a>). Darker shades indicate higher probabilities. Marginal probabilities (<span class="process-math">\(P(X)\text{,}\)</span> <span class="process-math">\(P(Y)\)</span>) are shown in the margins.</figcaption></figure> The following program was used to create the heatmap. <div class="code-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><pre class="program line-numbers"><code class="language-py">Joint probability heatmap

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Joint probabilities from patient table
joint = np.array([[0.15, 0.05], [0.30, 0.50]])  # Rows: X=D, X=N; Cols: Y=+, Y=-
marginal_x = np.sum(joint, axis=1)  # P(D), P(N)
marginal_y = np.sum(joint, axis=0)  # P(+), P(-)

# Heatmap with marginals
fig, ax = plt.subplots()
sns.heatmap(joint, annot=True, fmt=".3f", cmap="Blues", cbar=False,
            xticklabels=["$Y=+$", "$Y=-$"], yticklabels=["$X=D$", "$X=N$"],
            annot_kws={"size": 16}, ax=ax)
for i, m in enumerate(marginal_x):
    ax.text(2.1, i + 0.5, f"{m:.3f}", va="center")
for j, m in enumerate(marginal_y):
    ax.text(j + 0.33, 2.2, f"{m:.3f}", ha="center")
ax.text(2.2, 2.2, "1.000", va="center", ha="center")
plt.title("Joint Probability Heatmap with Marginals")
plt.tight_layout()
plt.savefig("joint_heatmap.png", dpi=300)
plt.show()
</code></pre></div></article></section><section class="subsection" id="subsec-Conditional-Probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.6.3</span><span class="space"> </span><span class="title">Conditional Probability</span>
</h3>
<div class="para logical" id="subsec-Conditional-Probability-2">
<div class="para">
<dfn class="terminology">Conditional probability</dfn> is probability of one event <span class="process-math">\(A\)</span> given the knowledge that another event <span class="process-math">\(B\)</span> has occured. That is, you are allowed to look into the world in which <span class="process-math">\(B\)</span> should occur and then in that world, ask what fraction of that world is where <span class="process-math">\(A\)</span> would also occur, i.e., the event <span class="process-math">\(A\cap B\text{.}\)</span> Thus, conditional probability, denoted by <span class="process-math">\(P(A \mid B)\)</span>  will be the ratio of <span class="process-math">\(P(A\cap B)\)</span> to <span class="process-math">\(P(B)\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-conditonal-probability-conceptual-def">
\begin{equation}
P(A | B) = \frac{P(A\cap B)}{ P(B)},\quad P(B) \ne 0.\tag{1.6.6}
\end{equation}
</div>
<div class="para">Of course, <span class="process-math">\(P(B) \ne 0\)</span> from the setup itself, since we are assuming <span class="process-math">\(B\)</span> has occured, i.e., its unconditional probability must have been zero.</div>
</div>
<div class="para logical" id="subsec-Conditional-Probability-3">
<div class="para">So, how is it related to joint and marginal probabilities? Let’s look at our ongoing example of two discrete random variables <span class="process-math">\(X \in \{x_1, x_2, \cdots, x_m \}\)</span> and <span class="process-math">\(Y \in \{y_1, y_2, \cdots, y_n \}\)</span> with joint probabilities <span class="process-math">\(P(X=x_i, Y=y_j) \equiv P(x_i, y_j) \equiv p_{ij}\)</span> and marginal probabilities <span class="process-math">\(P(X=x_i) \equiv p_i\)</span> and <span class="process-math">\(P(Y=y_j) \equiv q_i\text{,}\)</span> where I used <span class="process-math">\(q\)</span> in <span class="process-math">\(Y\)</span> so that we don’t confused by <span class="process-math">\(p_1\)</span> for <span class="process-math">\(X=x_1\)</span> and another <span class="process-math">\(p_1\)</span> from <span class="process-math">\(Y=y_1\)</span> in the same context. The <em class="alert">conditional probability</em> of event <span class="process-math">\(X=x_i\)</span> for <span class="process-math">\(A\)</span> given event <span class="process-math">\(Y=y_j\)</span> for <span class="process-math">\(B\)</span> will be, using the simplified notation of dropping <span class="process-math">\(X=\)</span> and <span class="process-math">\(Y=\text{,}\)</span>
</div>
<div class="displaymath process-math" id="eqn-conditional-definition">
\begin{equation}
P(x_i \mid y_j) = \frac{P(x_i, y_j)}{P(y_j)}, \quad P(y_j) \gt 0.\tag{1.6.7}
\end{equation}
</div>
<div class="para">Now, notice that <span class="process-math">\(P(x_i \mid y_j)\)</span> is a distribution in a world where <span class="process-math">\(Y\)</span> has a particula value and we are uncertain about <span class="process-math">\(X\text{,}\)</span> which could be in any of its <span class="process-math">\(m\)</span> values, and we are asking what will be the probability that the value will be <span class="process-math">\(x_i\text{.}\)</span> We can denote the PMF resulting from the <span class="process-math">\(m\)</span> values of <span class="process-math">\(X\)</span> when <span class="process-math">\(Y\)</span> is kept at <span class="process-math">\(y_j\)</span> as <span class="process-math">\(P(X,y_j)\text{.}\)</span>
</div>
</div>
<div class="para logical" id="subsec-Conditional-Probability-4">
<div class="para">Thus, we will have <span class="process-math">\(n\)</span> DIFFERENT CONDITIONAL PROBABILITY MASS FUNCTIONS in <span class="process-math">\(X\)</span> space, one for each value of <span class="process-math">\(Y\text{:}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P(X, y_1),\ P(X, y_2),\ P(X, y_3),\cdots, P(X, y_n).
\end{equation*}
</div>
<div class="para">That is the complication that conditional probability brings and also richness of the questions you can ask of the data!</div>
</div>
<div class="para logical" id="subsec-Conditional-Probability-5">
<div class="para">Now, let’s look at Conditonal PMF <span class="process-math">\(P(X,y_1)\text{.}\)</span> It is a bonafide probability mass function with values:</div>
<div class="displaymath process-math">
\begin{equation*}
P(x_1, y_1),\ P(x_2, y_1),\ P(x_3, y_1),\cdots, P(x_m, y_1).
\end{equation*}
</div>
<div class="para">They must add to <span class="process-math">\(1\text{.}\)</span>
</div>
<div class="displaymath process-math" id="eqn-normalization-codnitional-PMF">
\begin{equation}
P(x_1, y_1) + P(x_2, y_1) + P(x_3, y_1) + \cdots + P(x_m, y_1) = 1.\tag{1.6.8}
\end{equation}
</div>
</div>
<article class="example example-like" id="exp-Patient-disease-and-test-results-conditional-probabilities"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">1.6.5</span><span class="period">.</span><span class="space"> </span><span class="title">Patient disease and test results.</span>
</h4> <div class="para" id="exp-Patient-disease-and-test-results-conditional-probabilities-2">Using the patient dataset (<a href="sec-Two-or-More-Random-Variables.html#tab-patient-data-for-jt-and-conditional-probs" class="xref" data-knowl="./knowl/xref/tab-patient-data-for-jt-and-conditional-probs.html" data-reveal-label="Reveal" data-close-label="Close" title="Table 1.6.6: Patient Counts (Disease vs. Test)">Table 1.6.6</a>), compute all conditional probabilities:</div> <figure class="table table-like" id="tab-patient-data-for-jt-and-conditional-probs"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">1.6.6<span class="period">.</span></span><span class="space"> </span>Patient Counts (Disease vs. Test)</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b0 r0 l0 t0 lines"></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(+\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(-\)</span></td>
<td class="l m b0 r0 l0 t0 lines">Marginal</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(D\)</span></td>
<td class="l m b0 r0 l0 t0 lines">150</td>
<td class="l m b0 r0 l0 t0 lines">50</td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_D=200\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(N\)</span></td>
<td class="l m b0 r0 l0 t0 lines">300</td>
<td class="l m b0 r0 l0 t0 lines">500</td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_N=800\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">Marginal</td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_+=450\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_-=550\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(n_T=1000\)</span></td>
</tr>
</table></div></figure> <div class="para logical" id="exp-Patient-disease-and-test-results-conditional-probabilities-4">
<div class="para">Joint probabilities:</div>
<div class="displaymath process-math">
\begin{equation*}
P(D, +) = 0.150, \quad P(D, -) = 0.050, \quad P(N, +) = 0.300, \quad P(N, -) = 0.500.
\end{equation*}
</div>
</div> <div class="para" id="exp-Patient-disease-and-test-results-conditional-probabilities-5">Marginals: <span class="process-math">\(P(D) = 0.200\text{,}\)</span> <span class="process-math">\(P(N) = 0.800\text{,}\)</span> <span class="process-math">\(P(+) = 0.450\text{,}\)</span> <span class="process-math">\(P(-) = 0.550\text{.}\)</span>
</div> <div class="para" id="exp-Patient-disease-and-test-results-conditional-probabilities-6">Conditional probabilities: Let us do one of the conditional probabilities in detail and just write the answers for the rest:</div> <div class="para logical" id="exp-Patient-disease-and-test-results-conditional-probabilities-7">
<div class="para">Suppose we want to know that if test came out positive, what is the proability that the individual has disease? Since, we have the full joint probability table, we can answer this question by computing <span class="process-math">\(P( D \mid +)\text{.}\)</span>
</div>
<div class="displaymath process-math" id="exp-Patient-disease-and-test-results-conditional-probabilities-7-2">
\begin{align*}
P( D \mid +) \amp  = \frac{P(D,\ +) }{P(+)}\\
\amp =  \frac{0.150}{0.450} \approx 0.3333
\end{align*}
</div>
<div class="para">That is, based on the table, it is only <span class="process-math">\(33.3\%\)</span> chance that the individual has disease. If we use <span class="process-math">\(X\)</span> for the Disease/No Disease variable, We will have the PDF <span class="process-math">\(P(X \mid +)\)</span> by completing the calculation on <span class="process-math">\(P( N \mid + )\text{,}\)</span> by</div>
<div class="displaymath process-math">
\begin{equation*}
P( N \mid +) = \frac{P(N,\ +) }{P(+)} = \frac{0.300}{0.450} \approx 0.6667.
\end{equation*}
</div>
<div class="para">OR, simply by using the normalization condition on the PDF.</div>
<div class="displaymath process-math">
\begin{equation*}
P( N \mid +)  = 1  - P( D \mid -) 1 - 0.3333 = 0.667. 
\end{equation*}
</div>
<div class="para">Now, you should compute the PDF’s <span class="process-math">\(P(X \mid -)\text{,}\)</span> <span class="process-math">\(P(Y \mid D)\text{,}\)</span> and P(Y \mid N), where <span class="process-math">\(Y\)</span> is the variable for Test and check your answer for the following table.</div>
<div class="displaymath process-math" id="exp-Patient-disease-and-test-results-conditional-probabilities-7-12">
\begin{gather*}
P(D \mid -) \approx 0.0909, \quad P(N \mid -)  \approx 0.9091, \\
P(+ \mid D) = 0.750, \quad P(- \mid D) = 0.250, \\
P(+ \mid N) = 0.375, \quad P(- \mid N)  = 0.625. 
\end{gather*}
</div>
</div> <div class="para" id="exp-Patient-disease-and-test-results-conditional-probabilities-8">For two variables, each with two possible values, i.e., <span class="process-math">\(m=2\)</span> and <span class="process-math">\(n=2\)</span> there are <span class="process-math">\(m + n = 4\)</span> possible conditional PMFs, same as the number of marginal PMFs, but only one joint PMF.</div> <div class="para" id="exp-Patient-disease-and-test-results-conditional-probabilities-9">Another useful way to display the information in a conditional probability is to plot the PMFs in a bar chart as shown in <a href="sec-Two-or-More-Random-Variables.html#fig-patient-bar-conditional" class="xref" data-knowl="./knowl/xref/fig-patient-bar-conditional.html" data-reveal-label="Reveal" data-close-label="Close" title="Figure 1.6.7">Figure 1.6.7</a>, where each PMF is called a group.</div> <figure class="figure figure-like" id="fig-patient-bar-conditional"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/patient_conditional_bars.png" class="contained" alt="Grouped bar chart of conditional probabilities."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.6.7<span class="period">.</span></span><span class="space"> </span>Grouped bar chart of conditional probabilities for disease status (<span class="process-math">\(X \in \{D, N\}\)</span>) given test result (<span class="process-math">\(Y \in \{+, -\}\)</span>) and test result given disease status. Each group shows a probability distribution (summing to 1), illustrating how conditional probabilities slice the joint distribution.</figcaption></figure> <div class="code-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><pre class="program line-numbers"><code class="language-py"># === Grouped bar chart for all conditional probabilities ===
import matplotlib.pyplot as plt
import numpy as np

# Joint counts and probabilities
counts = {"D+": 150, "D-": 50, "N+": 300, "N-": 500}
total = 1000
P_D_plus = counts["D+"]/total
P_D_minus = counts["D-"]/total
P_N_plus = counts["N+"]/total
P_N_minus = counts["N-"]/total
P_plus = P_D_plus + P_N_plus
P_minus = P_D_minus + P_N_minus
P_D = P_D_plus + P_D_minus
P_N = P_N_plus + P_N_minus

# Conditional probabilities
probs = {
    "P(D|+)": P_D_plus / P_plus, "P(N|+)": P_N_plus / P_plus,
    "P(D|-)": P_D_minus / P_minus, "P(N|-)": P_N_minus / P_minus,
    "P(+|D)": P_D_plus / P_D, "P(-|D)": P_D_minus / P_D,
    "P(+|N)": P_N_plus / P_N, "P(-|N)": P_N_minus / P_N
}

# Grouped bar plot
labels = ["Given $Y=+$", "Given $Y=-$", "Given $X=D$", "Given $X=N$"]
values = [[probs["P(D|+)"], probs["P(N|+)"]], [probs["P(D|-)"], probs["P(N|-)"]],
        [probs["P(+|D)"], probs["P(-|D)"]], [probs["P(+|N)"], probs["P(-|N)"]]]
x = np.arange(len(labels))
width = 0.2
fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(x - width/2, [v[0] for v in values], width, label="First Outcome", color="blue")
ax.bar(x + width/2, [v[1] for v in values], width, label="Second Outcome", color="red")
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.set_ylabel("Conditional Probability")
ax.set_title("Conditional Probabilities from Patient Table")
ax.legend(["$P(D|·)$, $P(+|·)$", "$P(N|·)$, $P(-|·)$"])
ax.set_ylim(0, 1)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("patient_conditional_bars.png", dpi=300)
plt.show()
</code></pre></div></article></section><section class="subsection" id="subsec-conditional-probability-from-joint-probability"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.6.4</span><span class="space"> </span><span class="title">Bayes’ Rule</span>
</h3>
<div class="para" id="subsec-conditional-probability-from-joint-probability-2">Bayes’s rule (also called Bayes’s theorem) is a fundamental principle in probability theory. It describes how to update the probability of a hypothesis, which is expressed as a conditional probability when new evidence is observed. This looks and feels like too abstract. So, to get a better feel for it, let’s look at the mechanism of this process.</div>
<div class="para logical" id="subsec-conditional-probability-from-joint-probability-3">
<div class="para">From the conditional probability definition and/or product rule of probabilities, we have the following relations between the Joint, Marginal, and Condtional probabilites concerning two events <span class="process-math">\(A\)</span> and <span class="process-math">\(B\text{:}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-product-rule-joint-marginal-conditional-1.html ./knowl/xref/eqn-product-rule-joint-marginal-conditional-2.html" id="eqn-product-rule-joint-marginal-conditional-1">
\begin{equation}
P(A, B) = P(A \mid B) P(B).\tag{1.6.9}
\end{equation}
</div>
<div class="para">Switching <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> is equally valid, giving us</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-product-rule-joint-marginal-conditional-1.html ./knowl/xref/eqn-product-rule-joint-marginal-conditional-2.html" id="eqn-product-rule-joint-marginal-conditional-2">
\begin{equation}
P(B, A) = P(B \mid A) P(A).\tag{1.6.10}
\end{equation}
</div>
<div class="para">But, the order of listing of events in a joint probability is immaterial since the list is just another way of indicating intersection of the events,</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-product-rule-joint-marginal-conditional-1.html ./knowl/xref/eqn-product-rule-joint-marginal-conditional-2.html">
\begin{equation*}
(A, B) \equiv A \cap B \equiv (B, A).
\end{equation*}
</div>
<div class="para">Therefore, equating the right sides of Eqs. <a href="sec-Two-or-More-Random-Variables.html#eqn-product-rule-joint-marginal-conditional-1" class="xref" data-knowl="./knowl/xref/eqn-product-rule-joint-marginal-conditional-1.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.6.9">(1.6.9)</a> and <a href="sec-Two-or-More-Random-Variables.html#eqn-product-rule-joint-marginal-conditional-2" class="xref" data-knowl="./knowl/xref/eqn-product-rule-joint-marginal-conditional-2.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.6.10">(1.6.10)</a> gives</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-product-rule-joint-marginal-conditional-1.html ./knowl/xref/eqn-product-rule-joint-marginal-conditional-2.html" id="eqn-Bayes-Rule-1">
\begin{equation}
P(A \mid B) P(B) = P(B \mid A) P(A).\tag{1.6.11}
\end{equation}
</div>
<div class="para">Suppose we observe <span class="process-math">\(B\text{,}\)</span> then we would conclude that <span class="process-math">\(P(B)\ne 0\text{.}\)</span> Then, we can predict probability of event <span class="process-math">\(A\)</span> using the conditional probability.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-product-rule-joint-marginal-conditional-1.html ./knowl/xref/eqn-product-rule-joint-marginal-conditional-2.html" id="eqn-Bayes-Rule-2">
\begin{equation}
P(A \mid B)= \frac{P(B \mid A) P(A)}{P(B)},\quad P(B) \ne 0.\tag{1.6.12}
\end{equation}
</div>
<div class="para">This is <dfn class="terminology">Bayes’s Rule</dfn>. We are using the information that <span class="process-math">\(B\)</span> has occured to make prediction on another event <span class="process-math">\(A\text{.}\)</span> Of course, we also need <span class="process-math">\(P(B \mid A)\)</span> and <span class="process-math">\(P(A)\text{.}\)</span>
</div>
</div>
<div class="para logical" id="subsec-conditional-probability-from-joint-probability-4">
<div class="para">Let us now look at Bayes’s rule in the context of two discrete random variables, <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{,}\)</span> which take values <span class="process-math">\(\{ x_1, x_2, \cdots, x_m \}\)</span> and <span class="process-math">\(\{ y_1, y_2, \cdots, y_n \}\text{,}\)</span> respectively, as we have presented above in earlier subsections. For concreteness, let us focus on particular events</div>
<div class="displaymath process-math">
\begin{equation*}
A = \{X = x_i\},\ B = \{Y=y_j\}.
\end{equation*}
</div>
<div class="para">Then, Bayes’s rule will be</div>
<div class="displaymath process-math">
\begin{equation*}
P(X = x_i \mid Y=y_j)= \frac{P(Y=y_j \mid X = x_i) P(X = x_i)}{P(Y=y_j)},\quad P(Y=y_j) \ne 0.
\end{equation*}
</div>
<div class="para">Now, let us write these in our short notation by dropping <span class="process-math">\(X=\)</span> and <span class="process-math">\(Y=\text{,}\)</span> just remembering this detail in out mind only.</div>
<div class="displaymath process-math">
\begin{equation*}
P(x_i \mid y_j)= \frac{P(y_j \mid x_i) P(x_i)}{P(y_j)},\quad P(y_j) \ne 0.
\end{equation*}
</div>
<div class="para">Now, recall that marginal probabilities can be obtaine by margining out other variables. Say, we want the denominator part on the right side, <span class="process-math">\(P(y_j)\text{.}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
P(y_j) = \sum_{k=1}^m\,P(x_k, y_j) = \sum_{k=1}^m\,P(y_j \mid x_k)P(x_k).
\end{equation*}
</div>
<div class="para">That is the denominator is just a normalizing factor of the the numerator! If you didn’t have this factor, the PMF on the left side, <span class="process-math">\(P(X \mid y_j)\)</span> will not be normalized to <span class="process-math">\(1\text{.}\)</span> In this equation, I introduced another index <span class="process-math">\(k\text{,}\)</span> which is a dummy index for summation, so that we do not get confused by particular events connected with our original event <span class="process-math">\(A=(X=x_i)\text{.}\)</span> Using this normalization, we get a form of Bayes’s Rule that can be interpreted as it being an update rule on probabilities based on evidence.</div>
<div class="displaymath process-math" id="eqn-Bayes-Rule-denominator-normalizing">
\begin{equation}
P(x_i \mid y_j)= \frac{P(y_j \mid x_i) P(x_i)}{\sum_{k=1}^m\,P(y_j\mid x_k)P(x_k)}.\tag{1.6.13}
\end{equation}
</div>
<div class="para">In this form, everything on the right side is either probability of <span class="process-math">\(X=x_i\)</span> or is conditioned on the prossibility of <span class="process-math">\(X=x_k\)</span> for all <span class="process-math">\(x_k\text{.}\)</span> That is =, on the right side we have some prior knowledge about <span class="process-math">\(X\)</span> and on the right side we have an updated predictions on <span class="process-math">\(X\text{.}\)</span> It’s important to remember that all of these occur event by event although sometimes you will see Bayes’s rule written in a grand notation using the random variables themselves.</div>
<div class="displaymath process-math">
\begin{equation*}
P(X \mid Y)= \frac{P(Y \mid X) P(X)}{\sum_{X'}P( Y \mid X')P(X')}.
\end{equation*}
</div>
</div>
<article class="example example-like" id="subsec-conditional-probability-from-joint-probability-5"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">1.6.8</span><span class="period">.</span><span class="space"> </span><span class="title">Use of Bayes’s Rule in Medical Diagnosis.</span>
</h4> <div class="para" id="subsec-conditional-probability-from-joint-probability-5-2">Suppose, in a population of women in the age group 30 to 40, <span class="process-math">\(1\%\)</span>  develop breast cancer. It’s also known that mammogram test identifies <span class="process-math">\(80\%\)</span> of cancers accurately and misses <span class="process-math">\(20\%\)</span> of them. This means that if a woman has breast cancer, the test will be positive only <span class="process-math">\(80%\)</span> of the time. Furthermore, the test also give negative values <span class="process-math">\(95\%\)</span> of the time correctly, i.e., if a woman doesn’t have cancer, the test would came out negative <span class="process-math">\(95%\)</span> of the times.</div> <div class="para" id="subsec-conditional-probability-from-joint-probability-5-3">Now, a new woman in that age group comes in the lab and unfortunately, she tests positive. So, what would you say about here chances of actually having the cancer? Will it be <span class="process-math">\(80\%\)</span> or something else?</div> <div class="para logical" id="subsec-conditional-probability-from-joint-probability-5-4">
<div class="para">Let us denote <span class="process-math">\(C\)</span> for cancer <span class="process-math">\(N\)</span> for no cancer, <span class="process-math">\(+\)</span> for mamomgram being positive and <span class="process-math">\(-\)</span> if negative. What we want is</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html ./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html">
\begin{equation*}
P(C \mid +).
\end{equation*}
</div>
<div class="para">Let’s write the Bayes’s rule to identify what we need to get this.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html ./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html" id="eqn-Bayes-Rule-example-breast-cancer-main-equation">
\begin{equation}
P(C \mid +) = \frac{ P(+ \mid C) P(C)  }{  P(+ \mid C) P(C) + P(+ \mid N) P(N)  }.\tag{1.6.14}
\end{equation}
</div>
<div class="para">From the description, we know quite a bit. For instante, probability of cancer in that population is <span class="process-math">\(1\%\text{.}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html ./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html">
\begin{equation*}
P(C) = 0.01\ \Rightarrow P(N) = 1 - P(C) = 1- 0.01 = 0.99.
\end{equation*}
</div>
<div class="para">We are also told that if someone had Cancer, test will be positive <span class="process-math">\(80\%\)</span> of the time. This is a conditional probability data on condition <span class="process-math">\(C\text{.}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html ./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html">
\begin{equation*}
P(+ \mid C) = 0.80\ \Rightarrow P(- \mid C) = 1 - P(+ \mid C) = 1- 0.80 = 0.20.
\end{equation*}
</div>
<div class="para">Now, only thing left in Eq. <a href="sec-Two-or-More-Random-Variables.html#eqn-Bayes-Rule-example-breast-cancer-main-equation" class="xref" data-knowl="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.6.14">(1.6.14)</a> is <span class="process-math">\(P(+ \mid N)\text{.}\)</span> But we are also told that if someone did not have cancer, the test will show negative <span class="process-math">\(95\%\)</span> of the time.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html ./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html">
\begin{equation*}
P(- \mid N ) = 0.95\ \Rightarrow  P(+ \mid N ) = 1 - P(- \mid N ) = 1 -  0.95 = 0.05.
\end{equation*}
</div>
<div class="para">Hence, using Eq. <a href="sec-Two-or-More-Random-Variables.html#eqn-Bayes-Rule-example-breast-cancer-main-equation" class="xref" data-knowl="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.6.14">(1.6.14)</a> we will answer</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html ./knowl/xref/eqn-Bayes-Rule-example-breast-cancer-main-equation.html">
\begin{equation*}
P(C \mid +) = \frac{0.8\times 0.01}{0.8\times 0.01 + 0.05\times 0.99} = 0.139.
\end{equation*}
</div>
<div class="para">That is the probability of the individual having cancer will be <span class="process-math">\(13.9\%\text{.}\)</span>
</div>
</div></article></section><section class="subsection" id="subsec-Independent-Variables"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.6.5</span><span class="space"> </span><span class="title">Independent Random Variables</span>
</h3>
<div class="para logical" id="subsec-Independent-Variables-2">
<div class="para">Two random variables are said to be independent if their joint probability factorizes in the product of their marginal probabilities.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html" id="eqn-independent-variables">
\begin{equation}
P(X,Y) = P(X) P(Y)\ \ \text{(Independent variables)}\tag{1.6.15}
\end{equation}
</div>
<div class="para">Keep in mind that behind the scene, probabilities in this equation are over events, i.e., it’s for <span class="process-math">\(X\)</span> having some particular value and <span class="process-math">\(Y\)</span> having its own particular value. If <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are independent variables, we expect</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html">
\begin{equation*}
P(X = x_i\, \text{and}\, Y = y_j) = P(X=x_i) P(Y=y_j)\ \ \text{for all }x_i \text{ and } y_j.
\end{equation*}
</div>
<div class="para">We wouldn’t write our equations in the verbose manner, prefering to keep it simple. But, beware that probabilities are probabilities of events! Now, we write the left side of Eq. <a href="sec-Two-or-More-Random-Variables.html#eqn-independent-variables" class="xref" data-knowl="./knowl/xref/eqn-independent-variables.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.6.15">(1.6.15)</a> using marginal and conditional probabilities.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html">
\begin{equation*}
P(X|Y)P(Y) = P(X) P(Y)\ \ \text{(Independent variables)}
\end{equation*}
</div>
<div class="para">Canceling <span class="process-math">\(P(Y)\)</span> from both sides we get</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-independent-variables.html">
\begin{equation*}
P(X|Y) = P(X)\ \ \text{(Independent variables)}
\end{equation*}
</div>
<div class="para">That is knowing something about <span class="process-math">\(Y\)</span> does not tell you anything about probability of <span class="process-math">\(X\text{.}\)</span> That is, conditioning on <span class="process-math">\(Y\)</span> is useless when <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are independent.</div>
</div>
<div class="para logical" id="subsec-Independent-Variables-3">
<div class="para">For continuous variables, the equivalent statement is about the joint PDF factorizing into component marginal PDFs.</div>
<div class="displaymath process-math" id="eqn-independent-density">
\begin{equation}
\rho_{X,Y}(x, y) = \rho_X(x) \rho_Y(y).\tag{1.6.16}
\end{equation}
</div>
</div>
<div class="para logical" id="subsec-Independent-Variables-4">
<div class="para">Independence also implies  that their covariance is zero:</div>
<div class="displaymath process-math">
\begin{equation*}
\text{Independent }X,Y \implies \mathrm{Cov}(X,Y) = 0.
\end{equation*}
</div>
<div class="para">But zero covariance does not imply independence, except for jointly normal variables.</div>
<div class="displaymath process-math">
\begin{equation*}
\mathrm{Cov}(X,Y) = 0  \not \implies \text{Independent }X,Y.
\end{equation*}
</div>
</div></section><section class="subsection" id="subsec-Covariance-and-Correlation"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">1.6.6</span><span class="space"> </span><span class="title">Covariance and Correlation</span>
</h3>
<div class="para" id="subsec-Covariance-and-Correlation-2">A very common aspect of dealing with more than one random variable, say <span class="process-math">\(X\)</span> (e.g., height of a man) and <span class="process-math">\(Y\)</span> (e.g., the weight of the man) is to find out to what extent they tend to vary together. <em class="alert">Covariance</em> is a measure of their varying together either in the same direction or in the opposite direction. The normalized version of covariance so that result lies between <span class="process-math">\(-1\)</span> and <span class="process-math">\(+1\)</span> is called <em class="alert">correlation</em>.</div>
<div class="para logical" id="subsec-Covariance-and-Correlation-3">
<div class="para">Let <span class="process-math">\(P(X,Y)\)</span> denote the joint probability of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\text{.}\)</span> Then, covariance is the following expectation value computed in this probability distribution.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html ./knowl/xref/fig-pos-neg-no-correlation-examples.html" id="eqn-Covariance">
\begin{equation}
\text{Cov}(X,Y) = \langle  \left( X - \mu_X \right)   \left( Y - \mu_Y \right) \rangle,\tag{1.6.17}
\end{equation}
</div>
<div class="para">where to keep the formula simpler, the mean values of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are dented by <span class="process-math">\(\mu_X\)</span> and <span class="process-math">\(\mu_Y\)</span> respectively.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html ./knowl/xref/fig-pos-neg-no-correlation-examples.html">
\begin{equation*}
\mu_X = \langle X \rangle,\quad \mu_Y = \langle Y \rangle
\end{equation*}
</div>
<div class="para">By opening the braces inside the angle brackets in Eq. <a href="sec-Two-or-More-Random-Variables.html#eqn-Covariance" class="xref" data-knowl="./knowl/xref/eqn-Covariance.html" data-reveal-label="Reveal" data-close-label="Close" title="Equation 1.6.17">(1.6.17)</a>, we can rewrite the Covariance formula in another way.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html ./knowl/xref/fig-pos-neg-no-correlation-examples.html" id="eqn-Covariance-2">
\begin{equation}
\text{Cov}(X,Y) = \langle  X\,Y \rangle - \mu_X\,\mu_Y,\tag{1.6.18}
\end{equation}
</div>
<div class="para">where <span class="process-math">\(\langle  X\,Y \rangle\)</span> is to be computed using <span class="process-math">\(P(X,Y)\text{.}\)</span> For <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> discrete variable and <span class="process-math">\(P(X,Y)\)</span> a PMF, the calculation will be</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html ./knowl/xref/fig-pos-neg-no-correlation-examples.html">
\begin{equation*}
\langle  X\,Y \rangle = \sum_{x_i} \sum_{y_j} x_i y_j P(x_i, y_j).
\end{equation*}
</div>
<div class="para">Themean values <span class="process-math">\(\mu_X\)</span> and <span class="process-math">\(\mu_Y\)</span> in this case would relate to the marginals <span class="process-math">\(P(X\)</span> and <span class="process-math">\(P(Y)\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html ./knowl/xref/fig-pos-neg-no-correlation-examples.html" id="subsec-Covariance-and-Correlation-3-22">
\begin{align*}
\mu_X\amp = \sum_{x_i} \sum_{y_j} x_i P(x_i, y_j) = \sum_{x_i} x_i\, P(x_i).\\
\mu_Y\amp = \sum_{x_i} \sum_{y_j} y_j P(x_i, y_j) = \sum_{y_j} y_j\, P(y_j). 
\end{align*}
</div>
<div class="para">Covariance can take any real value, <span class="process-math">\(\text{Cov}(X,Y) \in (-\infty, +\infty)\text{.}\)</span> By dividing the covariance by the standard deviations of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> we get correlations, whose values are in the range <span class="process-math">\(\text{Corr}(X,Y) \in [-1, +]\text{.}\)</span>
</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html ./knowl/xref/fig-pos-neg-no-correlation-examples.html" id="eqn-Correlation">
\begin{equation}
\text{Corr}(X,Y) = \frac{ \text{Cov}(X,Y) }{ \sigma_X\, \sigma_Y }.\tag{1.6.19}
\end{equation}
</div>
<div class="para">The standard deviations in the joint distribution are the same as the standard deviation in the marginal as was the case for the means <span class="process-math">\(\mu_X\)</span> and <span class="process-math">\(\mu_Y\text{.}\)</span> For instance, in the case of discrete random variables, we will get the following for the variances, which is the square of standard deviations.</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/eqn-Covariance.html ./knowl/xref/fig-pos-neg-no-correlation-examples.html" id="subsec-Covariance-and-Correlation-3-30">
\begin{align*}
\sigma_X^2 \amp = \sum_{x_i} \sum_{y_j} \left( x_i -\mu_X \right)^2 P(x_i, y_j) = \sum_{x_i} \left( x_i -\mu_X \right)^2\, P(x_i).\\
\sigma_Y^2\amp = \sum_{x_i} \sum_{y_j}  \left( y_j -\mu_Y \right)^2 P(x_i, y_j) = \sum_{y_j} \left( y_j -\mu_Y \right)^2\, P(y_j). 
\end{align*}
</div>
<div class="para">The positive correlation (or covariance) means <span class="process-math">\(X\)</span> vary in the same direction, i.e., increasing <span class="process-math">\(X\)</span> and increasing <span class="process-math">\(Y\)</span> occur together. The opposite is the case for negative correlation (or covariance). See <a href="sec-Two-or-More-Random-Variables.html#fig-pos-neg-no-correlation-examples" class="xref" data-knowl="./knowl/xref/fig-pos-neg-no-correlation-examples.html" data-reveal-label="Reveal" data-close-label="Close" title="Figure 1.6.9">Figure 1.6.9</a>.</div>
</div>
<figure class="figure figure-like" id="fig-pos-neg-no-correlation-examples"><div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="external/./images/essential-probability-and-statistics/correlation_examples.png" class="contained" alt="Positive correlation shows upward trend, nagative shows downward trend and no correlation shows no discenable trend."></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">1.6.9<span class="period">.</span></span><span class="space"> </span>Positive correlation shows upward trend, nagative shows downward trend and no correlation shows no discenable trend.</figcaption></figure><div class="para" id="subsec-Covariance-and-Correlation-5">
<em class="alert">Beware, Correlation is not the same thing as Causation!</em> Just because two things move together doesn’t mean one causes the other. For example, suppose you find that ice cream sales and shark attacks may rise together (both happen in hot weather when people flock to the beaches), but it would be ludicurous to suggest that it means ice cream sales cause the shark attack or vice versa! Machine learning models may find patterns, but not always causal ones.</div></section><section class="conclusion" id="sec-Two-or-More-Random-Variables-9"><div class="para" id="sec-Two-or-More-Random-Variables-9-1">Joint, marginal, and conditional probabilities form the foundation for modeling relationships between random variables. Joint probabilities capture co-occurrence, marginals summarize individual variables, and conditionals refine probabilities based on evidence. Bayes’ rule updates beliefs, while independence simplifies joint distributions. Visualizations like heatmaps, scatter plots, and tree diagrams clarify these concepts. Apply these tools in fields like medical diagnostics, as shown, or explore further with resources like <a class="external" href="https://www.probabilitycourse.com/" target="_blank">Probability Course</a><details class="ptx-footnote" aria-live="polite" id="sec-Two-or-More-Random-Variables-9-1-2"><summary class="ptx-footnote__number" title="Footnote 1.6.1"><sup> 1 </sup></summary><div class="ptx-footnote__contents" id="sec-Two-or-More-Random-Variables-9-1-2"><code class="code-inline tex2jax_ignore">probabilitycourse.com</code></div></details>.</div></section></section></div>
<div class="ptx-content-footer">
<a class="previous-button button" href="sec-Random-Variables-and-Probabilities.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Top</span></a><a class="next-button button" href="sec-Example-Discrete-Probability-Distributions.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
