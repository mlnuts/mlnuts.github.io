<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-LLN-and-CLT">
    <title>Law of Large Numbers and Central Limit Theorem</title>
    <introduction>
        <p>
            The Law of Large Numbers (LLN) is often hand-waved as <q>averages go to the mean,</q> but there are really two distinct theorems (weak vs strong), different modes of convergence, and subtle conditions. Similary, the Central Limit Theorem (CLT) is one of the deepest ideas in probability, and it's often glossed over as <q>averages look normal.</q> 
        </p>  
        <p>  
            Let's take a slow, detailed walk through them. Although I do not intend our treatment here to be exhaustive, but there will be enough detail for you to get a fuller understanding of what they represent and how to use them.
        </p>
        <p>
            <alert>The Setup:</alert>
        </p>
        <p>
            The set up for both LLN and CLT is the same. Suppose you have a number of independent random variables <m>X_1, X_2, \cdots, X_n</m>, all with identical distribution functions <m>P(X)</m>, same for every <m>X_i</m>. Let <m>\mu</m> be the man and <m>\sigma^2</m> the variance of the common distribution.
            <mdn>
                <mrow xml:id="eqn-true-mean"> \amp \mu  = \langle X_i\rangle,\qquad \text{same for every }X_i </mrow>
                <mrow xml:id="eqn-true-var">\amp \sigma^2  = \langle X_i^2\rangle - \mu^2,\qquad \text{same for every }X_i </mrow>
            </mdn>
            We will call these values <alert>true mean</alert> and <alert>true variance</alert>.   
            Now, we introduce a new random variable, called the <alert>sample mean</alert> by 
            <men xml:id="eqn-sample-mean-random-variable">
                \bar{X}_n = \frac{1}{n}\,\left( X_1 + X_2 + \cdots + X_n  \right).
            </men>
            The bar over <m>X</m> just distinguishes it from the <m>n^\text{th}\, X</m> in the collection of random variables. More importantly, <m>\bar{X}_n</m> is not one number, but a random variable, which will take values in the range given by the components <m>X_i</m>. For instance, if <m>X_i \in \{ 0, 1 \}</m>, a Bernoulli random variable, then, <m>\bar{X}_n</m> would actually take many values, here  <m>\bar{X}_n \in \{ 0, 1/n, 2/n, \cdots, (n-1)/n, 1\}</m>.
        </p>  
        <p>
            The random variable <m>\bar{X}_n</m> will have it's own probability distribution, <m>P(\bar{X}_n)</m>. We call the mean and variance of this dstribution <alert>sample mean</alert> and <alert>sample variance</alert>.
            <men xml:id="eqn-">
                \bar{\mu}_n = \langle \bar{X}_n \rangle,\qquad \bar{\sigma}_n^2 =  \langle \bar{X}_n^2 \rangle - \bar{\mu}^2.
            </men>
            
        </p>

    </introduction>
    <subsection xml:id="subsec-Law-of-Large-Numbers">
        <title>Law of Large Numbers (LLN) </title>
        <p>
            The LLN addresses the question: <alert>What can we say about the mean of the sample mean <m>\bar{X}_n</m></alert> as <m>n</m> grows large? Clearly, the probability distribution of <m>\bar{X}_n</m> varies if we change <m>n</m>. 
        </p>
        <p>
            <alert>The weak law of LLN</alert> asserts that probablity that <m>|\bar{X}_n - \mu|</m>, where <m>\mu</m> is the true mean defined above in Eq. <xref ref="eqn-true-mean"/>, can be made smaller than any positive real number as long s you go sufficiently large <m>n</m>. Mathematically, we express it as the complement of this probility.
            <me>
                \lim_{n\rightarrow\infty}P(|\bar{X}_n - \mu| \gt \epsilon ) = 0,\qquad \text{for all } \epsilon\gt 0.
            </me>

            That is, the probability that <m>\bar{X}_n</m> strays outside of <m>\mu \pm \epsilon</m> goes to zero as <n>n \rightarrow \infty</n>. 
            This result is also expressed in a more compact notation, with a letter <m>P</m> above arrow implying <q>in probability</q>.
            <me>
                \bar{X}_n \xrightarrow{P}\, \mu.
            </me>
        </p>
        <p>
            <alert>The stron LLN</alert> uses a different criteria. You look at the sequency of sample random variables <m>\bar{X}_n</m> with increasing value of <m>n</m> and ask if these random variables themselves ted to <m>\mu</m>. The claim is
            <me>
                P(\lim_{n\rightarrow \infty} = \mu) = 1.
            </me>
            We say that <m>\bar{X}_n</m> <alert>almost surely</alert> converges to <m>\mu</m>, the true mean. 
        </p>
        <p>
            Although nt that hard, the proofs of these results are beyond these notes.
        </p>
        <p>
            For an <alert>intuitive feel</alert>, imagine tossing a fair coin. Suppose you toss it <m>n</m> times. For varying values of <m>n</m> you might find:
            <ul>
                <li>
                    <p>
                        <m>n=1</m>: <m>X_1</m> = 1 if heads and 0 if tails, with <m>\mu = \langle X_1 \rangle = 0.5</m>. <m>\bar{X}_1 = X_1</m>, hence, <m>\langle \bar{X}_1 \rangle = \mu</m>.
                    </p>
                </li>
                <li>
                    <p>
                        Suppose you flip the coin 10 times, now you have 10 variables, since outcome of each flip is a random variable in its own right. We usually, think of the tosses same experiment done 10 times. With this, <m>n = 10</m>. Suppose, you got <m>(1,1,0,0, 1, 1, 1, 1, 0, 1)</m>. That would mean, for this experiment, you got the following value of the sample mean, <m>\bar{X}_{10} = 0.7</m>.
                    </p>
                </li>
                <li>
                    <p>
                        f you flip 100 times, the value of the sample mean variable in any one set of 100 tosses will usually be close to <m>0.5</m>, say <m>0.501</m> 
                    </p>
                </li>
                <li>
                    <p>
                        Now, if you tossed the coin then <m>10,000</m>, you would get even closer to <m>o.5</m>. 
                    </p>
                </li>
            </ul>
            This is LLN in action. The strong law says: with probability 1, the sequence of averages converges to 0.5.


        </p>
 
        
    </subsection>
    <subsection xml:id="subsec-Central-Limit-Theorem">
        <title>Central Limit Theorem</title>
        <p>
            The Central Limit Theorem says that regardless of the distribution <m>P(X_i)</m> of <m>X_i</m>'s, which may be Bernouli, Binomial, Exponential, or whatever, the distribution of a <m>\bar{X}_n</m> tends towards a bell-shaped Gaussian wil larger and larger values of <m>n</m>. Specifically,
            <men xml:id="eqn-CLT">
                \lim_{n\rightarrow \infty}\sqrt{n}\left( \bar{X}_n - \mu\right)  \xrightarrow{d} \, \mathcal{N}(0, \sigma^2),          
            </men>
            where <m>d</m> above arrow just indicates the result <q>in distribution</q> and <m>\sigma^2</m> is the true variance of the component <m>X_i</m> variables. The convergence in distribution simly means that the CDF of the random variables on the left, i.e., <m>\sqrt{n}\left( \bar{X}_n - \mu\right)</m> converge to the Gaussian CDF, although it is written as <m>\mathcal{N}</m> in these equations.
        </p>  
        <p>     
            
            To emphasize the fact that this happens regarless of the original distribution of <m>X_i</m>'s. This is why we see Gaussian curves everywhere in nature — measurement errors, heights, test scores, etc. aren't inherently normal, but they are aggregates of many small random effects.
            
        </p>
        
    </subsection>
    <subsection xml:id="subsec-LLN-vs-CLT">
        <title>LLN vs CLT</title>
        <p>
            Let's summarize the basic features of the Law of Large Numbers and Central Limit Theorem.
            <dl>
                <li>
                    <title>Law of Large Numbers (LLN)</title>
                    <ul>
                        <li>
                            <p>
                                <m>\bar{X}_n \rightarrow \mu</m> as <m>n\rightarrow \infty</m> in the context of either in probability or almost surely.
                            </p>
                        </li>
                        <li>
                            <p>
                                Says the sample mean stabilizes around the true mean.
                            </p>
                        </li>
                    </ul>

                </li>
                <li>
                    <title>Central Limit Theorem (CLT)</title>
                    <ul>
                        <li>
                            <p>
                                Tells you how the deviations of <m>\bar{X}_n</m> from <m>\mu</m> are distributed.
                            </p>
                        </li>
                        <li>
                            <p>
                                Specifically, deviations are of order <m>1/\sqrt{n}</m>, and tend towards a Gaussian.
                            </p>
                        </li>
                    </ul>

                </li>
            </dl>
        So, LLN = consistency (where averages go) and CLT = fluctuations (how they’re spread around).
 

        </p>
        <p>
            <alert>How large is <m>n</m> when we say n large?</alert> A rule of thumb is that often <m>n \ge 30 </m> is enough. Why? <m>1/\sqrt{30} = 5.5</m> and when <m>\sigma = 1</m>, then probability of deviation from true <m>\mu</m> larger than this value is about <m>1\times 10^{-7}</m>, which can be safely ignored in most cases. There is actually a mathematical result that quantifies the rate of convergence. Berry-Esseen Theorem gives rates of convergence.
        </p>  
        <p>  
            Let <m>Y_1, Y_2, \cdots, Y_n</m> be i.i.d. (independent identically distributed) random variables as above. But, we now have subtracted away the true mean so that these have zero true mean. We state the conditions on <m>Y_1</m> since all others are same.
            <me>
                \langle Y_1 \rangle = 0.
            </me>
            Furthermore,
            <me>
                \text{Var}(Y_1) = \sigma^2 \gt 0,\quad \text{finite},
            </me>
            and third absolute moment is also finite.
            <me>
                \rho \equiv \langle Y_1^3 \rangle \lt \infty.
            </me>
            In place of sample mean random variable, let us define <alert>normalized sum </alert> random variable:
            <me>
                S_n = \frac{Y_1 + Y_2 + \cdots + Y_n}{ \sigma\,\sqrt{n} }.
            </me>
            Let <m>F_n</m> be the Cumulative Distribution Function (CDF) of the distribution of <m>S_n</m>, meaning
            <me>
                F_n(x) = P(S_n \le x),
            </me>
            and let us denote the CDF of standard normal, <m>\mathcal{N}(0,1)</m> by the Greek letter, <m>\Phi</m>. This is a standard notation. We keep <m>F</m> for other distributions. Then, Berry-Essen Theorem states that
            
            <men xml:id="eqn-Berry-Esseen-Theorem">
                \underset{x \in \R}{\sup}\, \left| F_n(x) - \Phi(x)  \right| \le \frac{C\, \rho}{ \sigma^3\,\sqrt{n}},
                
            </men>
            where <m>C</m> is a universal constant, independent of the distribution of the <m>Y_i</m>'s, so that the result holds for all i.i.d. variables satisfying the conditions above. Shevtsova (2011) showed <m>C \le 0.4748</m> and the best known lower bound is <m>C \ge 0.4097</m>. IExact value is not that important since in most applications <m>C \approx 1</m> is enough.
            
        </p>
        <p>
            The CLT says that as <m>n \rightarrow \infty</m>, <m>F_n \rightarrow \Phi</m>, and Berry-Esseen theorem tells us the rate at which this approach occurs. It says that the difference shrinks as <m>1/\sqrt{n}</m> with a "tail-heaviness" factor <m>\rho/\sigma^3</m>.
        </p>
        
    </subsection>
    <subsection xml:id="subsec-Why-Large-n-Matters">
        <title>Why LLN and CLT Matter?</title>
        <p>
            <alert>Why LLN Matter?</alert>
        </p>
        <p>
            In statistics, we distinguish between a sample mean and a population mean. The population mean, also called true mean <m>\mu</m>, is usually not known since we either do not have access to every point of interest, we only look at a small part. For instance, when we poll the population on some issue, we do not ask everybody, we just take <alert>sample</alert>. The LLN gives us confidence that if <m>n</m> is <alert>large enough</alert>, sample mean will be close to the population mean.
        </p>
        <p>
            As a numerical illustration of how sample mean <q>hugs</q> the true mean as <m>n</m> gets large, consider rolls of a fair six-sided die, whose faces are labeled <m>1, 2, 3, 4, 5, 6</m>. The true mean of these rolls will be just 
            <me>
                \text{True Mean}, \mu = 3.5.
            </me>
            Can you see how this can be shown? Anyway, let's roll this die <m>n</m> times and plot the sample means of the results obtained in rolls for various <m>n</m> values. This is shown in <xref ref="fig-die-rolls-sample-mean-vs-true-mean"/>. You can see that As the number of rolls increases, the sample mean stabilizes around <m>3.5</m>

            <figure xml:id="fig-die-rolls-sample-mean-vs-true-mean">
                <caption>Illustration of the Law of Large Numbers with rolls of a fair six-sided die.</caption>
                <image source="./images/essential-probability-and-statistics/die-rolls-sample-mean-vs-true-mean.png">
                    <shortdescription>Illustration of the Law of Large Numbers with rolls of a fair six-sided die.</shortdescription>
                </image>
            </figure>

            
        </p>
        <p>
            In <alert>Supervised Machine Learning</alert> we want to find the function <m>f</m> of the independent variables, called <q>features</q>, that minimizes the expected loss <m>L</m> which is a function of <m>f</m> and independent variable, also called <q>target</q> <m>Y</m>.
            <me>
                R(f) = \langle \mathcal{L}(f(X), Y)  \rangle
            </me>
            Now, the <q>true minimum</q> is not possible. The only thing we have are <m>(x,y)</m> pairs of values in each trial, i.e., each data point. Say you have <m>n</m> data points. So, we build the expectation on the right based on these <m>n</m> data points and get <q>an approximation</q> of <m>R(f)</m>, which we can designate as <m>R_n(f)</m>.
            <me>
                R_n(f) = \frac{1}{n} \sum_{i=1}^n\mathcal{L}(f(X_i), Y_i).
            </me>
            The LLN gives us confidence that
            <me>
                R_n(f) \rightarrow R(f)\quad \text{as long as }n \text{ is large enough}.
            </me>
            Hence, more the data, better the approximation!
            
            
        </p>
        <p>
            Maybe an example from Physics as well: In statistical mechanics, we often can't observe all particles in a macroscopic system. Instead, suppose we observe one particle over a long time. LLN guarantees that if the system is <alert>ergodic</alert>, the time average converges to the ensemble (population) average.
        </p>

        <p>
            <alert>Why CLT Matter?</alert>
        </p>
        <p>
            Although LLN tells us that in the large <m>n</m> limit sample mean will be close to the population or true mean <m>\mu</m>. But, we don't only want to know the approximate value of the true mean <m>\mu</m>. We also would like to know <alert>uncertainty</alert> in this knowledge. This is where we <alert>need the Central Limit Theorem</alert>. The CLT says that <m>\bar{X}_n</m> is approximately Gaussian, which gives as tools to build <alert>confidence intervals</alert>.
        </p>

        <p>
            Let us look at the fair six-sided die rolls. Suppose you N=10000 repeats of n=100 rolls. In each roll you compute the average of the n face up values and the standard deviation of the face up values from this average. In each trial, you will get the values of sample mean. That will give us the distribution of values of smaple mean from all <m>N</m> trials. We can then compute mean of these sample means and their standard deviation or variance as we learned in basic statistics. Then, because of the claim of the CLI, we can assert that
            <me>
                \bar{X}_{100} \sim \mathcal{N}\left( \text{Mean of Sample Means},  \frac{ \text{Variance of Sample Means} }{N} \right).
            </me>
            Let us use the following symbols:
            <me>
                \text{Mean of Sample Means} = \bar{m},\qquad \text{Variance of Sample Means} = \bar{s}^2.
            </me>
            
            Where will the true mean be present at say <m>95%</m> interval? We will study the details in another section,but I want just state here the results that the following range will contain the true mean of <m>X</m>.
            <me>
                95\% \text{Confindene Interval (CL)} = \left[  \bar{m} - z_{0.975}\, \frac{\bar{s}^2}{N}, \bar{m} + z_{0.975}\, \frac{\bar{s}^2}{N} \right],
            </me>
            where <m>z_a</m> is the notation for the inverse of the standard normal CDF, i.e.,
            <me>
                z_{0.975} = \Phi^{-1}(0.975),
            </me>
            and <m>0.975</m> came from the <m>95\%</m> confidence for finding <m>\mu</m> meant <m>5\%</m> being outside, i.e, <m>2.5\%</m> on either side. Thus, it gave us probability value 
            <me>
                0.95 + 0.025 = 0.975,
            </me>
            for the argument of the inverse <m>\Phi^{-1}</m>, which gave us how far from the estimated mean we need to go on either side of the estimated mean as as to be able to find the true mean. The true mean, of course, usually not known in a real situation, although it is known here simply by the equal probability of each face.
             
        </p>
        <p>
            A simulation of tossing of die is presented in <xref ref="fig-confidence-interval-die-roll"/>. Runs 10,000 experiments of rolling 100 dice each. The simulation (1) Collects sample means from each trial of 100 tosses; (2) Uses those means to estimate the 95% CI; (3) Draws a double-headed arrow under the histogram showing the CI. The program in Python that does all of this is listed after the figure.
        </p>

        <figure xml:id="fig-confidence-interval-die-roll">
            <caption>Confidence interval for 10000 trials of the computation of the distribution of mean of face values in 100 rolls of a siz-sided fair die with marking 1, 2, 3, 4, 5, 6. The true mean id just <m>(1+ 2+ 3+ 4+ 5+ 6)/6 = 3.5</m>. </caption>
            <image source="./images/essential-probability-and-statistics/confidence-interval-die-roll.png">
                <shortdescription>Confidence interval for 10000 trials of the computation of the distribution of mean of face values in 100 rolls of a siz-sided fair die with marking 1, 2, 3, 4, 5, 6.</shortdescription>
            </image>
        </figure>
        <program language="python">
            <code>
                import numpy as np
                import matplotlib.pyplot as plt
                from scipy.stats import norm

                # --- Step 1: Simulate dice rolls ---
                np.random.seed(42)
                n_trials = 10000   # number of experiments
                n_rolls = 100      # dice per experiment

                # Each experiment = 100 dice rolls
                data = np.random.randint(1, 7, size=(n_trials, n_rolls))
                sample_means = data.mean(axis=1)

                # --- Step 2: Compute 95% CI using sample statistics ---
                mean_est = np.mean(sample_means)
                std_est = np.std(sample_means, ddof=1)
                z_975 = norm.ppf(0.975)

                margin = z_975 * std_est
                ci_lower, ci_upper = mean_est - margin, mean_est + margin

                print(f"Estimated mean = {mean_est:.3f}")
                print(f"95% CI ≈ [{ci_lower:.3f}, {ci_upper:.3f}]")

                # --- Step 3: Plot histogram + CI arrow ---
                fig, ax = plt.subplots(figsize=(8,5))
                counts, bins, patches = ax.hist(sample_means, bins=50, density=True, 
                                                alpha=0.6, color='skyblue', edgecolor='black')

                # Vertical line for mean
                plt.axvline(mean_est, color='red', linestyle='--', label="Mean of sample means")

                # Place CI arrow at 1/3 of max histogram height
                y_arrow = counts.max() / 3  

                # Horizontal CI arrow
                plt.annotate(
                    '', 
                    xy=(ci_lower, y_arrow), xycoords='data',
                    xytext=(ci_upper, y_arrow), textcoords='data',
                    arrowprops=dict(arrowstyle='&lt;-&gt;', color='black', lw=2)
                )
                plt.text(mean_est, y_arrow * 1.1, "95% CI", ha='center', fontsize=12)

                # --- New: Vertical dashed lines at CI endpoints ---
                plt.axvline(ci_lower, color='black', linestyle='dashed', ymax=0.9, label="CI lower/upper")
                plt.axvline(ci_upper, color='black', linestyle='dashed', ymax=0.9)

                plt.title("Sampling Distribution of Dice Means (100 rolls, 10,000 trials)")
                plt.xlabel("Sample Mean")
                plt.ylabel("Density")
                plt.legend()
                plt.show()        
            </code>
        </program>
    </subsection>

</section>