<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-Descriptive-Statistics" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Descriptive Statistics</title>
  <introduction>
    <p>
      Descriptive statistics summarize key features of a dataset, providing insights into its central tendency, dispersion, and shape. This process, known as <alert>Exploratory Data Analysis (EDA)</alert>, helps identify patterns and trends before applying advanced statistical methods. Common measures include mean, median, mode, variance, standard deviation, range, quartiles, and visualizations like histograms and boxplots. These tools are essential for understanding data in fields like education, finance, and science.
    </p>
  </introduction>
  <subsection xml:id="subsec-Central-Tendency">
    <title>Measures of Central Tendency</title>
    <p>
      Measures of central tendency describe the "typical" value in a dataset.
    </p>
    <ol>
      <li>
        <p>
          <alert>Mean (Average)</alert>: The mean, denoted <m>\mu</m>, is the sum of all data points divided by their count. For a dataset <m>\{x_1, x_2, \dots, x_N\}</m> with <m>N</m> points, the mean is:
          <men xml:id="eqn-mean-of-N-data-points">
            \mu = \frac{1}{N} \sum_{i=1}^{N} x_i
          </men>
          where <m>\sum_{i=1}^{N} x_i = x_1 + x_2 + \dots + x_N</m>.
        </p>
        <p>
          Example: For student grades <m>\{85, 90, 90, 95, 100\}</m>, the mean is:
          <me>
            \mu = \frac{85 + 90 + 90 + 95 + 100}{5} = \frac{460}{5} = 92
          </me>
        </p>
        <p>
          However, mean of a dataset can be misleading if you have a few ouliers since mean is very senisitve to outliers. For instance, say you have a dataset of income, which is <m>\{20000</m>, <m>30000</m>, <m>35000</m>, <m>40000</m>, <m>1000000\}</m>. Clearly, most of the income is in the <m>30\text{K}</m> area, but the mean of this dataset is <m>225000</m>, skewed by the outlier. In this case, the median would better represents the typical value in the dataset.
        </p>
      </li>
      <li>
        <p>
          <alert>Median</alert>: The median is the middle value in a sorted dataset, where <m>50\%</m> of the data lies below and above. For odd <m>N</m>, it's the middle value; for even <m>N</m>, it's the average of the two middle values.
        </p>
        <p>
          Example: For <m>\{85, 90, 95\}</m> (sorted), median = <m>90</m>. For <m>\{85, 90, 91, 95\}</m>, median <m> = (90 + 91)/2 = 90.5</m>. For incomes <m>\{20000</m>, <m>30000</m>, <m>35000</m>, <m>40000</m>, <m>1000000\}</m>, median = <m>35000</m>, robust to the outlier.
        </p>
      </li>
      <li>
        <p>
          <alert>Mode</alert>: The mode is the most frequent value. A dataset may have no mode, one mode (unimodal), or multiple modes (bimodal or multimodal).
        </p>
        <p>
          Example: <m>\{85, 90, 90, 95, 100\}</m> has mode 90. <m>\{85, 90, 90, 95, 95\}</m> is bimodal <m>(90, 95)</m>. <m>\{85, 90, 95\}</m> has no mode.
        </p>
      </li>
    </ol>
    <p>
      <alert>Comparison</alert>: Consider incomes <m>\{20000</m>, <m>30000</m>, <m>35000</m>, <m>40000</m>, <m>1000000\}</m>. Mean = <m>225000</m>, median = <m>35000</m>, mode = none. The median best reflects the typical income due to the outlier. See <xref ref="fig-central-tendency"/> for a visual comparison.
    </p>
    <figure xml:id="fig-central-tendency">
      <caption>Density plot of incomes with mean, median, and no mode.</caption>
      <image source="./images/essential-probability-and-statistics/central-tendency.png">
        <shortdescription>Density plot showing central tendency measures.</shortdescription>
      </image>
    </figure>
    <!--
    <program language="python" line-numbers="yes">
      <title>Code for density plot of central tendency measures (new)</title>
      <code>
      import numpy as np
      import matplotlib.pyplot as plt
      from scipy.stats import gaussian_kde

      data = [20000, 30000, 35000, 40000, 1000000]
      kde = gaussian_kde(data, bw_method=0.5)
      x = np.linspace(0, 1100000, 1000)
      y = kde(x)

      plt.figure(figsize=(8, 5))
      plt.plot(x, y, label='Density')
      plt.axvline(np.mean(data), color='red', linestyle='-.-', label=f'Mean = {np.mean(data):.0f}')
      plt.axvline(np.median(data), color='green', linestyle='-', label=f'Median = {np.median(data):.0f}')
      plt.xlabel('Income')
      plt.ylabel('Density')
      plt.title('Central Tendency Measures for Income Data')
      plt.legend()
      plt.grid(True, alpha=0.3)
      plt.savefig('central-tendency.png')
      plt.show()

      </code>
    </program>

  -->
  </subsection>
  <subsection xml:id="subsec-Dispersion">
    <title>Measures of Dispersion</title>
    <p>
      Dispersion measures how spread out data is around the central tendency.
    </p>
    <ol>
      <li>
        <p>
          <alert>Variance and Standard Deviation</alert>: Variance (<m>\sigma^2</m>) measures average squared deviation from the mean; standard deviation (<m>\sigma</m>) is its square root, in the same units as the data.
        </p>
        <p>
          For a population:
          <men xml:id="eqn-pop-variance-of-N-data-points">
            \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2,
          </men>
          where <m>\mu</m> is the population (true) mean. The data collected from a polulation is called sample. From the sample we can only calculate as estimate of the corresponding population quantities. We define estimate of sample variance by keeping the same divisor <m>N</m> as in the true variance definitionor, define with a divisor <m>N-1</m>, which is called an unbiased estimate of variance.
          <men xml:id="eqn-sample-variance-of-N-data-points">
            s^2 = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \bar{x})^2
          </men>
          where <m>\bar{x}</m> is the sample mean.
          <me>
            \bar{x} = \frac{1}{N} \sum_{i=1}^{N}\, x_i. 
          </me>
          
        </p>
        <p>
          Example: For grades <m>\{85, 90, 90, 95, 100\}</m>, <m>\bar{x} = 92</m>. Population variance:
          <md>
            <mrow>\sigma^2 \amp= \frac{(85-92)^2 + (90-92)^2 + (90-92)^2 + (95-92)^2 + (100-92)^2}{5} </mrow>
            <mrow> \amp = \frac{126}{5} = 25.2. </mrow>
          </md>
          This will give the <term>standard deviation</term>, <m>\sigma</m>:
          <me>
            \sigma = \sqrt{\sigma^2} = \sqrt{25.2} \approx 5.0        
          </me>
          
          Sample variance, on the other hand, will be: 
          <me>s^2 = 126/4 = 31.5</me>, 
          and sample standard deviation
          <me>s \approx 5.61</me>.
        </p>
        <p>
          <xref ref="fig-variance-comparison"/> illustrated the tighter vs. wider spread for a low variance (e.g., <m>\sigma = 0.04</m>) vs. high variance (e.g., <m>\sigma = 4.00</m>).
        </p>
        <figure xml:id="fig-variance-comparison">
          <caption>Comparing low and high variance datasets.</caption>
          <image source="./images/essential-probability-and-statistics/variance-comparison.png">
            <shortdescription>Comparison of low and high variance data.</shortdescription>
          </image>
        </figure>
        <!--
        <program language="python" line-numbers="yes">
          <title>Code for variance comparison histograms (new)</title>
          <code>
          import matplotlib.pyplot as plt
          import numpy as np
          from scipy.stats import norm

          x = np.linspace(-10, 10, 1000)
          low_sig, high_sig = 0.2, 2.0

          low_var = norm.pdf(x, loc=0, scale=low_sig)  
          high_var = norm.pdf(x, loc=0, scale=high_sig)

          fig, ax = plt.subplots(1, 1, figsize=(10, 4))
          ax.plot(x, low_var, 'ob', markersize = 2, label=f"low variance, var = {low_sig**2:.2f}")
          ax.plot(x, high_var, 'xr', markersize = 2, label=f"high variance, var = {high_sig**2:.2f}")

          ax.set_title(f'Variance Comparison')
          ax.set_xlabel('Grade')
          ax.set_ylabel('Scaled Frequency')

          plt.tight_layout()
          plt.legend()
          plt.show()

          </code>
        </program>
      -->
      </li>
      <li>
        <p>
          <alert>Range and Quartiles</alert>: Range = max - min. Quartiles divide sorted data into four parts: Q1 (25th percentile), Q2 (median, 50th), Q3 (75th). Use linear interpolation: position = <m>(N-1) \cdot p</m>, where <m>p = 0.25, 0.5, 0.75</m>.
        </p>
        <p>
          Example: For grades <m>\{70, 75, 80, 85, 90, 95, 100\}</m>, <m>N=7</m>. Median <m>(\text{Q2}) = 85</m>. <m>\text{Q1} = 75</m>, <m>\text{Q3} = 95</m>. Range <m>= 100 - 70 = 30</m>. <m>\text{IQR} = \text{Q3} - \text{Q1} = 20</m>. Outliers: 
          <me>
            \text{Below: }\ \text{Q1} - 1.5\times\text{IQR} = 45; \quad \text{  Above:}\ \text{Q3} + 1.5 \times \text{IQR} = 125.
          </me>
          
          These grades have no outliers.
        </p>
      </li>
    </ol>
  </subsection>
  <subsection xml:id="subsec-Distribution-Shape">
    <title>Distribution Shape</title>
    <p>
      <alert>Histogram</alert>: Histograms show frequency distributions by grouping data into bins of equal size from min to a bin that includes the max data. So, if you have data from <m>x_\text{min}</m> to <m>x_\text{max}</m> with a bin size <m>b</m>. Then, bins will have <m>x_\text{min} \le x \lt x_\text{min} + b</m>, <m>x_\text{min} + b \le x \lt x_\text{min} + 2b</m>, <m>\cdots</m>, till you have exhausted all data. The last bin may extend beyond the data as in the example below.
    </p>
    <p>
      Example: For grades <m>\{70, 72, 75, 75, 80,</m> <m>80, 85, 90, 95, 100\}</m>, with bin size <m>10</m> from <m>70</m> to <m>110</m>, see <xref ref="tab-Histogram-Table"/>.
    </p>
    <table xml:id="tab-Histogram-Table">
      <title>Histogram of Grades</title>
      <tabular>
        <row header="yes">
          <cell>Bin</cell> <cell>Range</cell> <cell>Data</cell> <cell>Count</cell> <cell>Frequency</cell>
        </row>
        <row>
          <cell>1</cell> <cell><m>70 \le x \lt 80</m></cell> <cell><m>\{70, 72, 75, 75\}</m></cell> <cell>4</cell> <cell>0.333</cell>
        </row>
        <row>
          <cell>2</cell> <cell><m>80 \le x \lt 90</m></cell> <cell><m>\{80, 82, 85\}</m></cell> <cell>3</cell> <cell>0.333</cell>
        </row>
        <row>
          <cell>3</cell> <cell><m>90 \le x \lt 100</m></cell> <cell><m>\{93, 95\}</m></cell> <cell>2</cell> <cell>0.222</cell>
        </row>
        <row>
          <cell>4</cell> <cell><m>100 \le x \lt 110</m></cell> <cell><m>\{100\}</m></cell> <cell>1</cell> <cell>0.111</cell>
        </row>
      </tabular>
    </table>
    <p>
      Many computer libraries have histogram plotting routines. For instance <xref ref="fig-descriptive-statistics-histogram"/> was generated from the Python program listed after it. The histogram has been decorated with the mean and median of the data also.
    </p>
    <figure xml:id="fig-descriptive-statistics-histogram">
      <caption>Histogram of grades with mean and median.</caption>
      <image source="./images/essential-probability-and-statistics/histogram.png">
        <shortdescription>Histogram with mean and median lines.</shortdescription>
      </image>
    </figure>
    <program language="python" line-numbers="yes">
      <title>Example Histogram</title>
      <code>
      import matplotlib.pyplot as plt
      import numpy as np

      data = [70, 72, 75, 75, 80, 82, 85, 93, 95, 100]
      bins = [70, 80, 90, 100, 110]
      freq_arr, bins_arr = np.histogram(data, bins) # returns frequency
      width = bins_arr[1:] - bins_arr[:-1]
      plt.figure(figsize=(8, 5))
      plt.hist(data, bins=bins, edgecolor='black', alpha=0.7)
      # this is just plt.bar(bins_arr[:-1], freq_arr, width)
      mean = np.mean(data)
      median = np.median(data)
      plt.axvline(mean, color='red', linestyle='--', label=f'Mean = {mean:.1f}')
      plt.axvline(median, color='green', linestyle='-', label=f'Median = {median:.1f}')
      plt.xlabel('Grade')
      plt.ylabel('Frequency')
      plt.title('Histogram of Student Grades')
      plt.xticks(bins)
      plt.grid(axis='y', alpha=0.3)
      plt.legend()
      plt.savefig('histogram.png')
      plt.show()
      </code>
    </program>
    <p>
      <alert>Boxplot</alert>: Boxplots show min, Q1, median, Q3, max (whiskers), and outliers (points beyond Q1 <m>- 1.5\times </m>IQR or Q3 <m>+ 1.5\times</m>IQR).
    </p>
    <p>
      Example: For grades with an outlier <m>\{70, 75, 80, 85, 90, 95, 100, 150\}</m>, <m>\text{Q1} = 77.5</m>, <m>\text{Q2} = 87.5</m>, <m>\text{Q3} = 97.5</m>, <m>\text{IQR} = 20</m>. Outliers: &#x2264;<m>150</m> is above <m>\text{Q3} + 1.5*\text{IQR} = 127.5</m>. See <xref ref="fig-descriptive-statistics-boxplots"/>.
    </p>
    <figure xml:id="fig-descriptive-statistics-boxplots">
      <caption>Boxplot of grades with annotated quartiles and outlier.</caption>
      <image source="./images/essential-probability-and-statistics/boxplot.png">
        <shortdescription>Boxplot with one outlier.</shortdescription>
      </image>
    </figure>
    <program language="python" line-numbers="yes">
      <title>Updated boxplot with annotations</title>
      <code>
import matplotlib.pyplot as plt
import numpy as np

data = [70, 75, 80, 85, 90, 95, 100, 150]

plt.figure(figsize=(8, 4))
bp = plt.boxplot(data, vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'), medianprops=dict(color='red'))

q1, median, q3 = np.percentile(data, [25, 50, 75])
plt.text(q1 - 5, 1.1, 'Q1', ha='right')
plt.text(median, 1.1, 'Median', ha='center')
plt.text(q3 + 5, 1.1, 'Q3', ha='left')
plt.text(150, 1.3, 'Outlier', ha='center')

plt.title('Boxplot of Student Grades')
plt.xlabel('Grade')
plt.grid(True, alpha=0.3)
plt.savefig('boxplot.png')
plt.show()
      </code>
    </program>
    <p> 
      <alert>Skewness</alert>: Skewness tells us about the shape of the distribution, specifically if it's "tilted" to one side. In a positively skewed distribution (right skew), the tail on the right side is longer, while in a negatively skewed distribution (left skew), the tail on the left side is longer.
    </p>  
    <p>
      Positive skew example: Imagine a dataset of household incomes: <m>{20000, 30000, 35000, 40000, 1000000}</m>. The income of 1 million is much higher than the others, causing the data to be right-skewed. Most people earn a lower income, but a few very high incomes stretch the right side of the distribution, creating a longer right tail.
    </p>  
    <p>
      Negative skew example: Think of a set of exam scores <m>{50, 60, 70, 80, 90, 95}</m>. If most students score high but a few perform very poorly, the data is left-skewed. The low scores create a long tail on the left side of the distribution.
    </p>  
    <p>
      See <xref ref="fig-skewness-comparison"/> for a visual representation.
    </p>  
    <figure xml:id="fig-skewness-comparison"> 
      <caption>Histograms comparing a normal distribution and a right-skewed distribution.</caption> 
      <image source="./images/essential-probability-and-statistics/skewness-comparison.png"> <shortdescription>Comparison of normal vs. skewed distributions.</shortdescription> </image>
    </figure> 
    <p> 
      To help visualize the difference, here’s a Python code that generates two types of distributions: a normal (symmetrical) one and a right-skewed one. The plot will show how the shapes of these two distributions differ. 
    </p> 
    <program language="python" line-numbers="yes"> 
      <title>Skewness Visualization</title> 
      <code>
      import matplotlib.pyplot as plt 
      import numpy as np 
      from scipy.stats import norm, skewnorm

      np.random.seed(42)
      normal_data = np.random.normal(50, 10, 1000) # Normal distribution
      skewed_data = np.random.exponential(20000, 1000) # Right-skewed distribution

      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

      # normal distribution

      ax1.hist(normal_data, bins=30, edgecolor='black', alpha=0.7)
      ax1.set_title('Normal Distribution')
      ax1.set_xlabel('Value')
      ax1.set_ylabel('Frequency')

      #Plot right-skewed distribution

      ax2.hist(skewed_data, bins=30, edgecolor='black', alpha=0.7)
      ax2.set_title('Right-Skewed Distribution')
      ax2.set_xlabel('Value')
      ax2.set_ylabel('Frequency')

      plt.tight_layout()
      plt.savefig('skewness-comparison.png')
      plt.show()

      </code>
    </program>
  </subsection>

<subsection xml:id="kurtosis" cols="1">
  <title>Kurtosis: Checking Out the Tails</title>
  <introduction>
    <p>
      After digging into mean, median, mode, and skewness, let’s talk <em>kurtosis</em>. This stat zooms in on the "tailedness" of our data—how often we see wild outliers compared to a normal distribution. It’s like checking the edges of our data’s shape to see if they’re loaded with extreme values or totally chill.    
    </p>
  </introduction>

  <subsubsection xml:id="types-of-kurtosis">
    <title>Types of Kurtosis</title>
    <p>
      Kurtosis comes in three flavors:
    </p>
    <ul>
      <li><p><em>Mesokurtic</em>: Matches a normal distribution. Not too many outliers, not too few. It’s the just-right vibe.</p></li>
      <li><p><em>Leptokurtic</em>: Sharp peak, heavy tails. Think lots of outliers, like stock prices during a market rollercoaster.</p></li>
      <li><p><em>Platykurtic</em>: Flatter peak, light tails. Fewer outliers, like steady daily temps.</p></li>
    </ul>
  </subsubsection>

  <subsubsection xml:id="kurtosis-formula">
    <title>How’s It Calculated?</title>
    <p>
      Kurtosis measures how much data hangs out in the tails. The formula for <em>excess kurtosis</em> (comparing to a normal distribution) is:
    </p>
    <p>
      <m>\text{Excess Kurtosis} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{x_i - \mu}{\sigma} \right)^4 - 3</m>
    </p>
    <p>
      Here, <m>N</m> is the number of data points, <m>x_i</m> is each data point, <m>\mu</m> is the mean, and <m>\sigma</m> is the standard deviation. Don’t stress the math—Python will handle it in a bit!
    </p>
  </subsubsection>

  <subsubsection xml:id="kurtosis-examples">
    <title>Real-World Examples</title>
    <p>
      Let's connect kurtosis to mean, median, mode, and skewness:
    </p>
    <ul>
      <li><p><em>Leptokurtic Example</em>: Take stock returns: <c>{5, 7, 8, 8, 10, 12, 12, 12, 12, 50}</c>. That 50 is a massive outlier, bulking up the tails (like skewness, but focused on extremes). This is leptokurtic—expect some crazy swings.</p></li>
      <li><p><em>Platykurtic Example</em>: Now daily temperatures: <c>{30, 32, 33, 34, 35, 36, 38}</c>. No wild outliers, just values chilling around the mean and median. This is platykurtic—nice and calm.</p></li>
    </ul>
    <p>
      While skewness shows if our data’s lopsided, kurtosis tells us if outliers are stealing the show.
    </p>
  </subsubsection>

  <subsubsection xml:id="kurtosis-visual">
    <title>Seeing Kurtosis in Action</title>
    <p>
      Picture kurtosis with histograms (like we used for mean and skewness):
    </p>
    <ul>
      <li><p><em>Leptokurtic</em>: Sharp peak, chunky tails (lots of outliers).</p></li>
      <li><p><em>Platykurtic</em>: Flatter top, skinny tails (few outliers).</p></li>
      <li><p><em>Mesokurtic</em>: Classic bell curve, balanced tails.</p></li>
    </ul>
    <p>
      Check out this chart to see the difference:
    </p>
    <figure xml:id="kurtosis-chart">
      <caption>
        Kurtosis Comparison: The top subplot shows the full distributions, with leptokurtic (sharp peak, heavy tails with more outliers, like stock returns), mesokurtic (normal distribution, balanced tails), and platykurtic (flat peak, light tails with fewer outliers, like temperatures). The bottom subplot zooms in on the right tail, showing how leptokurtic tails decay slower (higher density at large values) compared to mesokurtic and platykurtic tails, which drop off faster.
      </caption>
      <image source="./images/essential-probability-and-statistics/kurtosis.png" width="80%"/>
    </figure>
    <program language="python" line-numbers="yes">
      <code>
      import numpy as np
      import matplotlib.pyplot as plt
      from scipy.stats import kurtosis, t, norm, uniform

      # Data for kurtosis calculation
      stock_data = [5, 7, 8, 8, 10, 12, 12, 12, 12, 50]  # Leptokurtic
      temp_data = [30, 32, 33, 34, 35, 36, 38]  # Platykurtic

      # Calculate kurtosis
      kurt_stock = kurtosis(stock_data, fisher=True)
      kurt_temp = kurtosis(temp_data, fisher=True)
      print(f"Stock Returns Kurtosis: {kurt_stock:.2f} (Leptokurtic)")
      print(f"Temperature Kurtosis: {kurt_temp:.2f} (Platykurtic)")

      # Generate data for plotting distributions
      x = np.linspace(-10, 10, 200)  # Wider range to show tails

      # Leptokurtic: Student's t-distribution (df=3 for heavy tails)
      lepto = t.pdf(x, df=3) * 1.2  # Scale for visibility

      # Mesokurtic: Normal distribution
      meso = norm.pdf(x)

      # Platykurtic: Uniform-like distribution (approximated)
      platy = uniform.pdf(x, loc=-2, scale=4) * 0.8  # Flat, light tails

      # Create figure with two subplots
      fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=False)

      # Full distribution plot (top subplot)
      ax1.plot(x, lepto, label='Leptokurtic (Heavy Tails)', color='#FF5733')
      ax1.plot(x, meso, label='Mesokurtic (Normal)', color='#33FF57')
      ax1.plot(x, platy, label='Platykurtic (Light Tails)', color='#3357FF')
      ax1.set_title('Kurtosis Comparison: Full Distribution')
      ax1.set_xlabel('Value')
      ax1.set_ylabel('Density')
      ax1.legend()
      ax1.grid(True)

      # Tail-focused plot (bottom subplot)
      ax2.plot(x, lepto, label='Leptokurtic (Heavy Tails)', color='#FF5733')
      ax2.plot(x, meso, label='Mesokurtic (Normal)', color='#33FF57')
      ax2.plot(x, platy, label='Platykurtic (Light Tails)', color='#3357FF')
      ax2.set_title('Kurtosis Comparison: Right Tail Focus')
      ax2.set_xlabel('Value')
      ax2.set_ylabel('Density')
      ax2.set_xlim(3, 10)  # Focus on right tail
      ax2.set_ylim(0, 0.05)  # Zoom in on low density
      ax2.legend()
      ax2.grid(True)

      # Adjust layout and save
      plt.tight_layout()
      plt.savefig('kurtosis.png', dpi=300, bbox_inches='tight')  # Save both subplots
      plt.show()
      </code>
    </program>

  </subsubsection>

  <subsubsection xml:id="kurtosis-python">
    <title>Calculating Kurtosis with Python</title>
    <p>
      Since we’re all about descriptive stats, let’s compute kurtosis with Python (more tools coming up later). Here’s a script for our stock returns example:
    </p>
    <program language="python" line-numbers="yes">
      <code>
      from scipy.stats import kurtosis
      import numpy as np

      # Stock returns data (leptokurtic)
      data = [5, 7, 8, 8, 10, 12, 12, 12, 12, 50]

      # Calculate excess kurtosis
      kurt = kurtosis(data, fisher=True)
      print(f"Excess Kurtosis: {kurt:.2f}")

      # Positive value means leptokurtic (heavy tails)!
      </code>
    </program>
    <p>
      Run this, and you’ll see a positive kurtosis, confirming big outliers in our stock returns. Try the temperature data <c>{30, 32, 33, 34, 35, 36, 38}</c> for a negative kurtosis and that platykurtic vibe.
    </p>
  </subsubsection>

  <subsubsection xml:id="kurtosis-wrapup">
    <title>Why Kurtosis Matters</title>
    <p>
      Kurtosis wraps up our descriptive stats crew—mean, median, mode, and skewness. It’s like a heads-up about outliers. High kurtosis (leptokurtic) yells “watch out for big swings!”—think risky stocks. Low kurtosis (platykurtic) says “smooth sailing”—like predictable weather. It’s another piece of your data’s story.
    </p>
  </subsubsection>
</subsection>





  <!-- 
  <subsection xml:id="subsec-Numerical-Summary">
    <title>Numerical Summary</title>
    <p>
      Computing descriptive statistics numerically is efficient with Python. Below is a program to calculate mean, median, mode, variance, standard deviation, quartiles, and skewness for a dataset. This program will print out the following results
      <tabular>
        <row> <cell> Mean: 83.33 </cell></row>
        <row> <cell> Median: 80.00 </cell></row>
        <row> <cell> Mode: 75 </cell></row>
        <row> <cell> Population Variance: 88.89 </cell></row>
        <row> <cell> Population Std Dev: 9.43 </cell></row>
        <row> <cell> Sample Variance: 100.00 </cell></row>
        <row> <cell> Sample Std Dev: 10.00 </cell></row>
        <row> <cell> Q1: 75.00, Q3: 90.00 </cell></row>
        <row> <cell> Skewness: 0.39 </cell></row>      
    </tabular>
    </p>
    <program language="python" line-numbers="yes">
      <title>Code for computing descriptive statistics (new)</title>
      <code>

import numpy as np
from scipy import stats

data = [70, 75, 75, 80, 80, 85, 90, 95, 100]

mean = np.mean(data)
median = np.median(data)
mode = stats.mode(data, keepdims=True)[0][0]
pop_var = np.var(data)
pop_std = np.std(data)
sam_var = np.var(data, ddof=1)
sam_std = np.std(data, ddof=1)
q1, q3 = np.percentile(data, [25, 75])
skew = stats.skew(data)

print(f"Mean: {mean:.2f}")
print(f"Median: {median:.2f}")
print(f"Mode: {mode}")
print(f"Population Variance: {pop_var:.2f}")
print(f"Population Std Dev: {pop_std:.2f}")
print(f"Sample Variance: {sam_var:.2f}")
print(f"Sample Std Dev: {sam_std:.2f}")
print(f"Q1: {q1:.2f}, Q3: {q3:.2f}")
print(f"Skewness: {skew:.2f}")

      </code>
    </program>
  </subsection>
-->
  <conclusion>
    <title>Conclusion</title>
    
    
    <p>
      In summary, descriptive statistics provide essential tools for understanding and summarizing datasets, helping to reveal underlying patterns and trends. By examining measures of central tendency (mean, median, mode), we can identify the "typical" values of a dataset, while measures of dispersion (variance, standard deviation, range, and quartiles) show how spread out the data is. Visualizations like histograms, boxplots, and density plots enhance this understanding by allowing us to visually inspect the distribution and shape of the data. As we move forward, the next section will explore the tools available for calculating these descriptive statistics, giving us the means to automate and refine these analyses in practice.
    </p>
  </conclusion>
</section>