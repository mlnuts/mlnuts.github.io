<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-LLN-and-CLT">
    <title>Law of Large Numbers and Central Limit Theorem</title>
    <introduction>
        <p>
            The Law of Large Numbers (LLN) and Central Limit Theorem (CLT) are foundational results in probability and statistics, underpinning many inferential techniques. The LLN ensures that sample averages converge to the population mean, while the CLT describes the distribution of those averages as approximately normal for large samples. This section provides a detailed yet accessible exploration of these theorems, their assumptions, and their applications.
        </p>
        <p>
            <alert>The Setup:</alert>
        </p>
        <p>
            Consider a sequence of independent and identically distributed (i.i.d.) random variables <m>X_1, X_2, \dots, X_n</m>, each with the same probability distribution, mean <m>\mu = E[X_i]</m>, and variance <m>\sigma^2 = \text{Var}(X_i) = E[X_i^2] - \mu^2</m>, where <m>E[\cdot]</m> denotes the expected value. These are called the <alert>true mean</alert> and <alert>true variance</alert>. We define the <alert>sample mean</alert> as:
            <men xml:id="eqn-sample-mean-random-variable">
                \bar{X}_n = \frac{1}{n} (X_1 + X_2 + \cdots + X_n).
            </men>
            The sample mean <m>\bar{X}_n</m> is itself a random variable with its own distribution. For example, if <m>X_i</m> is a Bernoulli random variable with <m>X_i \in \{0, 1\}</m> and <m>p = 0.5</m>, then <m>\bar{X}_n</m> takes values in <m>\{0, \frac{1}{n}, \frac{2}{n}, \dots, 1\}</m>. The LLN and CLT describe the behavior of <m>\bar{X}_n</m> as <m>n</m> increases.
        </p>
        <p>
            The LLN and CLT require that the <m>X_i</m> are i.i.d. with finite mean <m>\mu</m>. The CLT additionally requires finite variance <m>\sigma^2</m>, and the Strong LLN requires a slightly stronger condition on the moments of <m>X_i</m>.
        </p>
    </introduction>

    <subsection xml:id="subsec-Law-of-Large-Numbers">
        <title>Law of Large Numbers</title>
        <p>
            The LLN addresses the question: <alert>What happens to the sample mean <m>\bar{X}_n</m> as the sample size <m>n</m> grows large?</alert> It comes in two forms: the Weak LLN and the Strong LLN, which differ in their modes of convergence.
        </p>
        <p>
            <alert>Weak LLN:</alert> The Weak LLN states that the probability that <m>\bar{X}_n</m> deviates from the true mean <m>\mu</m> by more than any positive amount <m>\epsilon</m> approaches zero as <m>n \to \infty</m>:
            <me>
                \lim_{n \to \infty} P(|\bar{X}_n - \mu| &gt; \epsilon) = 0, \quad \text{for all } \epsilon &gt; 0.
            </me>
            This is called convergence <alert>in probability</alert>, denoted:
            <me>
                \bar{X}_n \xrightarrow{P} \mu.
            </me>
        </p>
        <p>
            <alert>Strong LLN:</alert> The Strong LLN states that the sequence of sample means <m>\bar{X}_n</m> converges to <m>\mu</m> with probability 1:
            <me>
                P(\lim_{n \to \infty} \bar{X}_n = \mu) = 1.
            </me>
            This is called <alert>almost sure convergence</alert>, a stronger condition implying that almost all sample paths of <m>\bar{X}_n</m> converge to <m>\mu</m>.
        </p>
        <p>
            <alert>Intuitive Example:</alert> Consider tossing a fair coin with <m>P(\text{heads}) = 0.5</m>, so <m>X_i = 1</m> for heads and <m>0</m> for tails, with <m>\mu = E[X_i] = 0.5</m>. For <m>n = 1</m>, <m>\bar{X}_1 = X_1</m> is either 0 or 1. For <m>n = 10</m>, suppose we observe 7 heads, so <m>\bar{X}_{10} = 7/10 = 0.7</m>. For <m>n = 100</m>, we might get <m>\bar{X}_{100} \approx 0.51</m>, and for <m>n = 10,000</m>, <m>\bar{X}_{10,000} \approx 0.5002</m>. As <m>n</m> increases, <m>\bar{X}_n</m> gets closer to <m>\mu = 0.5</m>, illustrating the LLN. The Strong LLN guarantees this convergence occurs almost surely.
        </p>
        <p>
            <alert>Visualization:</alert> The following figure shows the sample mean of rolls of a fair six-sided die (with true mean <m>\mu = (1+2+3+4+5+6)/6 = 3.5</m>) for increasing <m>n</m>, converging to <m>3.5</m>.
            <figure xml:id="fig-die-rolls-sample-mean-vs-true-mean">
                <caption>Illustration of the Law of Large Numbers: Sample means of fair six-sided die rolls converge to the true mean <m>\mu = 3.5</m> as <m>n</m> increases.</caption>
                <image source="./images/essential-probability-and-statistics/die-rolls-sample-mean-vs-true-mean.png">
                    <shortdescription>Sample means of die rolls converging to 3.5.</shortdescription>
                </image>
            </figure>
        </p>
    </subsection>

    <subsection xml:id="subsec-Central-Limit-Theorem">
        <title>Central Limit Theorem</title>
        <p>
            The CLT states that for i.i.d. random variables <m>X_1, X_2, \dots, X_n</m> with finite mean <m>\mu</m> and variance <m>\sigma^2</m>, the distribution of the sample mean <m>\bar{X}_n</m> becomes approximately normal as <m>n</m> increases:
            <men xml:id="eqn-CLT">
                \bar{X}_n \xrightarrow{d} \mathcal{N}\left( \mu, \frac{\sigma^2}{n} \right),
            </men>
            where <m>\xrightarrow{d}</m> denotes convergence in distribution. Equivalently, the standardized sample mean:
            <men xml:id="eqn-the-Zn-variable">
                \bar{Z}_n = \frac{\sqrt{n} (\bar{X}_n - \mu)}{\sigma},
            </men>
            converges to a standard normal distribution:
            <men xml:id="eqn-the-normalized-CLT">
                \bar{Z}_n \xrightarrow{d} \mathcal{N}(0, 1).
            </men>
            The probability density of <m>\bar{X}_n</m> for large <m>n</m> is approximately:
            <me>
                f_{\bar{X}_n}(x) \approx \frac{1}{\sqrt{2\pi \sigma^2 / n}} \exp\left( -\frac{(x - \mu)^2}{2 \sigma^2 / n} \right).
            </me>
            The cumulative distribution function (CDF) of <m>\bar{Z}_n</m> is:
            <me>
                P(\bar{Z}_n \leq z) \approx \Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{t^2}{2} \right) dt.
            </me>
        </p>
        <p>
            The CLT is remarkable because it holds regardless of the underlying distribution of <m>X_i</m> (e.g., Bernoulli, exponential, or normal), as long as <m>\mu</m> and <m>\sigma^2</m> are finite. This explains why normal distributions appear in phenomena like measurement errors, test scores, or heights, which are aggregates of many small random effects.
        </p>
        <p>
            <alert>Example:</alert> For an exponential distribution with rate <m>\lambda = 1</m> (mean <m>\mu = 1</m>, variance <m>\sigma^2 = 1</m>), the sample mean of <m>n = 100</m> observations is approximately <m>\mathcal{N}(1, 1/100)</m>. The CLT allows us to compute probabilities, such as <m>P(\bar{X}_{100} &lt; 1.1)</m>, using the normal distribution.
        </p>
        <p>
            <alert>Visualization:</alert> <xref ref="fig-clt-convergence"/> shows histograms of <m>\bar{X}_n</m> for a fair six-sided die for different sample sizes, illustrating convergence to a normal distribution.
            <figure xml:id="fig-clt-convergence">
                <caption>Histograms of sample means of fair six-sided die rolls for <m>n = 10, 100, 500</m>, visually showing convergence to a normal distribution per the CLT.</caption>
                <image source="./images/essential-probability-and-statistics/clt-convergence.png">
                    <shortdescription>Histograms showing CLT convergence for die rolls.</shortdescription>
                </image>
            </figure>
        </p>
    </subsection>

    <subsection xml:id="subsec-LLN-vs-CLT">
        <title>LLN vs. CLT</title>
        <p>
            The LLN and CLT are complementary:
            <ul>
                <li><p><alert>LLN:</alert> Ensures that <m>\bar{X}_n \to \mu</m> (in probability or almost surely), describing the convergence of the sample mean to the true mean.</p></li>
                <li><p><alert>CLT:</alert> Describes the distribution of fluctuations around <m>\mu</m>, stating that <m>\sqrt{n} (\bar{X}_n - \mu) / \sigma \xrightarrow{d} \mathcal{N}(0, 1)</m>, with deviations of order <m>1/\sqrt{n}</m>.</p></li>
            </ul>
            In essence, the LLN tells us where the sample mean goes, while the CLT tells us how it fluctuates around that value.
        </p>
    </subsection>

    <subsection xml:id="subsec-Berry-Esseen">
        <title>Berry-Esseen Theorem</title>
        <p>
            The CLT states that <m>\bar{X}_n</m> is approximately normal for large <m>n</m>, but how large must <m>n</m> be? The Berry-Esseen Theorem quantifies the rate of convergence. Let <m>S_n = \frac{X_1 + \cdots + X_n - n\mu}{\sigma \sqrt{n}}</m> be the standardized sum, with CDF <m>F_n(x) = P(S_n \leq x)</m>. The theorem states:
            <men xml:id="eqn-Berry-Esseen-Theorem">
                \sup_{x \in \mathbb{R}} |F_n(x) - \Phi(x)| \leq \frac{C \rho}{\sigma^3 \sqrt{n}},
            </men>
            where <m>\rho = E[|X_i - \mu|^3]</m> is the third absolute moment, <m>\Phi(x)</m> is the standard normal CDF, and <m>C</m> is a constant (<m>C \leq 0.4748</m>, Shevtsova 2011). This implies that the error in the normal approximation decreases as <m>1/\sqrt{n}</m>, modulated by the “tail-heaviness” factor <m>\rho / \sigma^3</m>.
        </p>
        <p>
            For example, for a fair six-sided die, <m>\rho</m> is finite, and for <m>n = 100</m>, the approximation error is small, ensuring reliable use of the CLT in practice.
        </p>
    </subsection>

    <subsection xml:id="subsec-Why-Large-n-Matters">
        <title>Why LLN and CLT Matter</title>
        <p>
            <alert>LLN Applications:</alert> The LLN justifies using sample means to estimate population means. For example, in polling, we survey a sample to estimate voter preferences. The LLN ensures that with a large enough sample, the sample mean is close to the true population mean.
        </p>
        <p>
            Consider rolling a fair six-sided die with true mean <m>\mu = (1+2+3+4+5+6)/6 = 3.5</m> and variance <m>\sigma^2 = \frac{1}{6} \sum_{i=1}^6 (i - 3.5)^2 = \frac{35}{12} \approx 2.9167</m>. For <m>n = 100</m> rolls, suppose we compute <m>\bar{X}_{100} = 3.6</m>. The LLN suggests <m>\bar{X}_{100} \approx 3.5</m>. <xref ref="fig-die-rolls-sample-mean-vs-true-mean"/>illustrates this convergence.
        </p>
        <p>
            In statistical mechanics, the LLN applies to time averages in ergodic systems, ensuring that long-term observations of a particle’s behavior approximate the population average.
        </p>
        <p>
            <alert>CLT and Confidence Intervals:</alert> The CLT enables us to quantify uncertainty in sample means via confidence intervals. For the die example, suppose we roll <m>n = 100</m> times and compute <m>\bar{X}_{100} = 3.6</m> and sample standard deviation <m>s = \sqrt{\frac{1}{99} \sum_{i=1}^{100} (X_i - 3.6)^2}</m>. The CLT implies:
            <men xml:id="eqn-clt-gaussian-approx">
                \bar{X}_n \approx \mathcal{N}\left( \mu, \frac{\sigma^2}{n} \right).
            </men>
            Using the sample standard deviation <m>s \approx \sqrt{2.9167} \approx 1.7078</m>, the standard error is <m>s / \sqrt{n} \approx 1.7078 / \sqrt{100} = 0.17078</m>. A 95% confidence interval is:
            <me>
                \bar{X}_n \pm z_{0.975} \cdot \frac{s}{\sqrt{n}} \approx 3.6 \pm 1.96 \cdot 0.17078 \approx [3.265, 3.935].
            </me>
            This interval suggests that we are 95% confident that <m>\mu</m> lies between 3.265 and 3.935, consistent with the true mean <m>\mu = 3.5</m>.
        </p>
        <p>
            <alert>Simulation:</alert> The following Python code simulates 10,000 trials of 100 die rolls, computes sample means, and plots a histogram with a 95% confidence interval:
            <program language="python">
                <title>Simulating Dice Rolls and Confidence Interval</title>
                <code>
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Step 1: Simulate dice rolls
np.random.seed(42)
n_trials = 10000
n_rolls = 100
data = np.random.randint(1, 7, size=(n_trials, n_rolls))
sample_means = data.mean(axis=1)

# Step 2: Compute 95% CI
mean_est = np.mean(sample_means)
std_est = np.std(sample_means, ddof=1)
z_975 = norm.ppf(0.975)
margin = z_975 * std_est
ci_lower, ci_upper = mean_est - margin, mean_est + margin

print(f"Estimated mean = {mean_est:.3f}")
print(f"95% CI ≈ [{ci_lower:.3f}, {ci_upper:.3f}]")

# Step 3: Plot histogram with CI
fig, ax = plt.subplots(figsize=(8,5))
counts, bins, patches = ax.hist(sample_means, bins=50, density=True, 
                                alpha=0.6, color='skyblue', edgecolor='black')
plt.axvline(mean_est, color='red', linestyle='--', label="Mean of sample means")
plt.axvline(ci_lower, color='black', linestyle='dashed', label="95% CI")
plt.axvline(ci_upper, color='black', linestyle='dashed')
y_arrow = counts.max() / 3
plt.annotate('', xy=(ci_lower, y_arrow), xytext=(ci_upper, y_arrow),
             arrowprops=dict(arrowstyle='&lt;-&gt;', color='black', lw=2))
plt.text(mean_est, y_arrow * 1.1, "95% CI", ha='center', fontsize=12)
plt.title("Sampling Distribution of Dice Means (100 rolls, 10,000 trials)")
plt.xlabel("Sample Mean")
plt.ylabel("Density")
plt.legend()
plt.show()
                </code>
            </program>
            <figure xml:id="fig-confidence-interval-die-roll">
                <caption>Histogram of sample means from 10,000 trials of 100 fair six-sided die rolls, with a 95% confidence interval (dashed lines and arrow) around the estimated mean.</caption>
                <image source="./images/essential-probability-and-statistics/confidence-interval-die-roll.png">
                    <shortdescription>Histogram of die roll sample means with 95% CI.</shortdescription>
                </image>
            </figure>
        </p>
        <p>
            <alert>Other Applications:</alert> The CLT is crucial for hypothesis testing (e.g., z-tests) and approximating probabilities for sums of random variables, such as total customer purchases in a store.
        </p>
    </subsection>
</section>