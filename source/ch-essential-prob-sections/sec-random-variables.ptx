<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-Random-Variables-and-Probabilities">
    <title>Random Variables and Probabilities</title>
    <introduction>
        <p>
            In this section we will present foundations of the calculational tools necessary for analytical work. First we will define random variables and then follow up with probability distributions of a single and then two random variables. The generalization of more than two random variables will be left for future sections.
        </p>
    </introduction>

    <!--  Random Variables -->
    <subsection xml:id="subsec-Random-Variables">
        <title>Random Variables, Probabilities, and Expectations</title>
        <p>
            We will think of variables as something the is observed or measured by experiments. The outcome in any experiment is a real value of the variable. A variable whose value is uncertain or unpredictable or varies from trial to trial, even though measurement conditions haven't changed, is called a <term>random variable</term>.
        </p>
        <p>
            We tend to use <alert>capital letter for the name</alert> of the variable and small letters for its values. Thus, for a variable <m>X</m>, the values will be denoted by <m>x_1, x_2, \cdots,</m> etc., or a generic value by just <m>x</m>. Sometimes we will use superscripts to denote values, <m>x^{(1)},x^{(2)}, \cdots </m>.
        </p>
        <p>
            An <alert>event</alert> will now refer to the outcome that in a particular trial, variable <m>X</m> has some value or a set of the possible values. Thus, <m>X=\{x_1\}</m> would be an event and so would be <m>X=\{x_1, x_2\}</m>,  etc. In case of continuous values for <m>X</m>, and event may even be written as <m>x_1 \lt X \le x_2</m>, etc.
        </p>
        <p>
            A random variable can either <term>continuous variable</term> or a <term>discrete variable</term>. A continuous random variable takes values either in a finite segment of the real line or on an entire real line. For example, the price of a house (say, denoted by <m>X</m>) in the US dollars could be between <m>10^4</m> and <m>10^8</m>. We can say <m>X \in [10^4, 10^8]</m>.
        </p>
        <p>
            A <term>discrete random variable</term> takes values in a countable set e.g., die outcomes <m>\in \{1, 2, \dots, 6\}</m>, colors <m> \in \{ \text{red}, \text{blue}, \text{green}, \text{magenta} \}</m>. When a discrete random variable has abstract symbols or wrods as values, we seek numerical embeddings of them using real-valued vectors so that they can be fed into machine learning algorithms for processing.
        </p>
    </subsection>


    <subsection xml:id="subsec-Probability-Mass-Function">
        <title>Probability Mass Function</title>
        
        <p>
            For a discrete random variable <m>X</m> with values <m>x_i</m> (<m>i=1,\dots,N</m>), the <alert>probability mass function (PMF)</alert> assigns appropriate probabilities to events of all the unique values that the random variable can take. Thus, PMF of our example variable <m>X</m> will give all the <m>p_i</m> values in:
            <men xml:id="eqn-pmf">
                P(X=x_i) = p_i, \quad i=1,\dots,N, \quad \sum_{i=1}^N p_i = 1,
            </men>
            where the last condition is important since it makes sure that probability values are properly normalized and all the unique events are included.

        </p>

        <figure xml:id="fig-die-pmf">
            <caption>Probability mass function (PMF) of a fair six-sided die, showing equal probabilities (<m>P(X=k) = \frac{1}{6}</m>) for outcomes <m>k=1,\dots,6</m>. This bar chart visualizes the discrete distribution, useful for understanding expected values in games of chance.</caption>
            <image source="./images/essential-probability-and-statistics/die_pmf.png">
                <shortdescription>Bar chart of die PMF.</shortdescription>
            </image>
        </figure>
        <program language="python" line-numbers="yes">
            <title>PMF of a fair die</title>
            <code>
            # === CODE: PMF of a fair die ===
            import matplotlib.pyplot as plt

            x = [1, 2, 3, 4, 5, 6]
            p = [1/6] * 6
            plt.figure()
            plt.bar(x, p)
            plt.xlabel("Die Outcome")
            plt.ylabel("Probability")
            plt.title("PMF of a Fair Die")
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig("die_pmf.png", dpi=300)
            plt.show()
            </code>
        </program>

    </subsection>
    <subsection xml:id="subsec-Expectation-Values">
        <title>Expectation Values</title>
        
        <p>
            The <alert>expectation</alert> value of a function <m>f(X)</m>, which is also simply called expectation of <m>f</m>, is the weighted value of the function according to the probailities of each value of <m>X</m>.
            <men xml:id="eqn-expectation">
                \mathbb{E}[f(X)] = \sum_{i=1}^N f(x_i) p_i.
            </men>
            The <term>mean</term> is just when the function is the variable itself.
            <men xml:id="eqn-mean">
                \mathbb{E}[X] = \sum_{i=1}^N x_i p_i.
            </men>
            In Physics, the expectation is commonly written as <m>\langle f(X) \rangle</m>, but in probability and statistics, the notation is <m>\mathbb{E}[f(X)]</m>, with <m>f(X)</m> in the bracket.
        </p>

        <p>
            The <term>variance</term> measures spread of distribution about the mean value. It is defined by
            <men xml:id="eqn-variance">
                \mathrm{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
            </men>
        </p>
        <proof><title>Proof of the last step</title>
        
        
            <p>
                Here, the second equality is easy to see by an explicit work, noting that <m>\mathbb{E}[X]</m> is just a number. Let's denote it by letter <m>\mu</m>.
            <md>
                <mrow> \mathbb{E}[(X - \mathbb{E}[X])^2]\amp = \sum_{i=1}^N (x_i - \mu)^2 p_i </mrow>
                <mrow> \amp = \sum_{i=1}^N \,x_i^2 p_i + 2\mu \sum_{i=1}^N \,x_i^2 p_i  - \mu^2 \sum_{i=1}^N \, p_i </mrow>
                <mrow> \amp = \mathbb{E}[X^2] - 2\mu \times \mu  +\mu^2 \times 1 </mrow>
                <mrow> \amp =  \mathbb{E}[X^2] - \mu^2 = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.\quad \blacksquare</mrow>
            </md>
            </p>
        </proof>
            

        <p>  
            The <term>standard deviation</term> is just square root of variance.
            <me>
                \sigma_X = \sqrt{\mathrm{Var}(X)}

            </me>.
        </p>
        <example>
            <title>Die roll expectation and variance</title>
            <p>
                For a fair six-sided die, <m>P(X=k) = \frac{1}{6}</m> for <m>k=1,\dots,6</m>. The mean is:
                <me>
                    \mathbb{E}[X] = \sum_{k=1}^6 k \cdot \frac{1}{6} = \frac{1+2+3+4+5+6}{6} = 3.5.
                </me>
                The second moment is:
                <me>
                    \mathbb{E}[X^2] = \sum_{k=1}^6 k^2 \cdot \frac{1}{6} = \frac{1^2 + 2^2 + \dots + 6^2}{6} = \frac{91}{6} \approx 15.1667.
                </me>
                The variance is:
                <me>
                    \mathrm{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = \frac{91}{6} - (3.5)^2 = \frac{35}{12} \approx 2.9167.
                </me>
            </p>
        </example>

    </subsection>

    <subsection xml:id="subsec-Probability-Distribution-of-Continuous-Variable">
        <title>Probability Density of Continuous Variables</title>
        <p>
            Probability distribution of a continuous variable cannot be given by a PMF since there are uncountably infinitely many unique values in the sample space <m>\Omega</m>, which is taken to be the entire real line.
            <me>
                \Omega = \mathbb{R}.
            </me>
            If we have an <m>X</m> that is non-zero only in an interval, we can extend this <m>X</m> to the entire real line by just stating that the probability for all other values be zero.
        </p>
        <p>
            Instead of PMF, it turns out that the best we can do in the case of continuous variables is <em>to define probability within an infinitesimal interval around some value</em> <m>x</m> when we want to know about the distribution of probabilities <q>at</q> <m>x</m>. Thus, we nead probability per unit interval, i.e., <term>probability density function</term> (<term>PDF</term>) at <m>x</m>, which we will denote by <m>\rho(x)</m>.
            <men xml:id="eqn-Probaility-Density">
                P(x - dx/2 \le X \le x + dx/2 ) = \rho(x)\, dx \equiv P(x \le X \le x + dx ).
            </men>
            Events are then defined by an interval in which you will find the value of the random variable <m>X</m>. Thus, the probability of the <alert>event that <m>X \in [a, b]</m></alert> will be
            <me>
                P(a\le X \le b ) = \int_a^b\, \rho(x)\,dx.
            </me>
            Normalization requires that this integral over all values of <m>x</m> be <m>1</m>.
            <men xml:id="eqn-probability-density-normalization">
                P(x \in \mathbb{R}) = \int_{-\infty}^\infty\, \rho(x)\,dx = 1.
            </men>
            A very useful probability density is a uniform value between, say <m>x=a</m> and <m>x = b \gt a</m>. Due to normalization, the constant value of <m>\rho(x) = \frac{1}{b-a}</m>.
        </p> 
            <sidebyside widths="50% 50%">
                <p> 
                    <me>
                        \rho(x) = \begin{cases}
                            0 \amp x \lt \\
                            \frac{1}{b-a} \amp a \le x \le b\\
                            0 \amp x \gt b
                        \end{cases}
                    </me>
                </p>  
                <image source="./images/essential-probability-and-statistics/uniform-distribution-a-to-b.png">
                    <shortdescription>(for accessibility)</shortdescription>
                </image>
            </sidebyside>
        <p>  
            You can easily check that it is properly normalized.
            <me>
                \int_0^{\infty}\rho(x) dx = 0 + \int_a^b \frac{1}{b-a}\,dx + 0 = 1.
            </me>
        </p> 
            <sidebyside widths="50% 50%">
                <p> 
                    Another probability density of extreme importance is the Gaussian density function centered about zero and having unit variance. This Gaussian is called normal density.
                    <me>
                        \rho = \frac{1}{ \sqrt{2\pi} }\,e^{-x^2/2}.
                    </me>
                </p>  
                <image source="./images/essential-probability-and-statistics/standard-normal.png">
                    <shortdescription>(for accessibility)</shortdescription>
                </image>
            </sidebyside>
        <p>  
            Although, it is more difficult to show but the integral of a Gaussian with the factor as shown does give you the correct normaliztion.
            <me>
                \frac{1}{ \sqrt{2\pi} }\,\int_{-\infty}^{\infty}\,e^{-x^2/2}\, dx = 1.
            </me>            
        </p>
        <proof>
            <p>
                Proof: Let <m>I</m> denote the following integral.
            <me>
                I = \int_{-\infty}^{\infty}\,e^{-x^2/2}\, dx.
            </me>
            We need to show that <m>I = \sqrt{2\pi}</m>. We look at <m>I^2</m> and write the second <m>I</m> using another dummy symbol <m>y</m>.
            <me>
                I \times I = \left[ \int_{-\infty}^{\infty}\,e^{-x^2/2}\, dx \right] \times \left[ \int_{-\infty}^{\infty}\,e^{-y^2/2}\, dy \right]
            </me>
            Now, combine the two integrals and think of them as integral in the <m>xy</m>-plane. 
            <me>
                I^2 = \int_{-\infty}^{\infty}\,e^{-(x^2 + y^2)/2}\, dxdy.
            </me>
            Now, change variable to polar coordinates <m>r,\theta</m> with
            <me>
                dx dy = r dr d\theta. \quad 
            </me>
            With <m>x^2 + y^2 = r^2</m>, the integral over <m>\theta</m> separates out and gives <m>2\pi</m> leaving only integral over <m>r</m>, which goes from <m>0</m> to <m>\infty</m>.
            <me>
                I^2 = 2\pi \int_{0}^{\infty}\,e^{-r/2}\, r dr.
            </me>
            The integral over <m>r</m> is easily done by substitution <m>u=r^2</m>. Show that the value of this integral is just <m>1</m>.
            <me>
                I ^2 = 2\pi.
            </me>
            Since <m>I \gt 0</m>, only positive root is applicable. Hence
            <me>
                I = \sqrt{2\pi}\qquad \blacksquare
            </me>
            </p>
        </proof>
        <p>
            <alert>Expectation values</alert> of functions continuous random variables are also weighted values of the functions. But, here you need to do the integral.
            <men xml:id="eqn-Expectation-Values-Continuous-Variables">
                \mathbb{E}[f(X)] = \int_{-\infty}^{\infty}\,f(x)\,\rho(x)\,dx.
            </men>
            Accordingly, mean, variance, and standard deviations will be
            <mdn>
                <mrow> \amp \mu = \int_{-\infty}^{\infty}\,x\,\rho(x)\,dx. </mrow>
                <mrow> \amp \mathrm{Var}(X) =  \int_{-\infty}^{\infty}\,x^2\,\rho(x)\,dx - \mu^2 \equiv \sigma^2.</mrow>
                <mrow> \amp\sigma = \sqrt{ \mathrm{Var}(X) } </mrow>
            </mdn>
            We will study examples of various distribution functions in the next section.    
        </p>
        
    </subsection>

    <subsection xml:id="subsec-Cumulative-Distribution-Function">
        <title>Cumulative Distribution Function (CDF)</title>
        <p>
            For continuous distribution functions, we can find the probability of an event that the outcome of an observation on the random variable <m>X</m> will be less than or equal to some real number <m>x</m> by the following integral.
            <men xml:id="eqn-probability-that-defines-cdf">
                P( X \le x) = \int_{-\infty}^x\, \rho(x')\, dx'.
            </men>
            It's important to note that on the right side the <m>x</m> is some definite fixed value and that is why I am using <m>x'</m> as the dummy variable for the integral. You could use <m>t</m> or <m>y</m> or any other symbol for the dummy variable. Try not to confuse the dummy variable with the value variable. The value <m>x</m> can be any value on the real line.
        </p>
        <p>
            Probability in Eq. <xref ref="eqn-probability-that-defines-cdf"/> is called <term>cumulative distribution function</term> (<term>CDF</term>) corresponding to that probability density <m>\rho</m>. We typically use symbol <m>F</m> for the CDF. It is clearly a function of the value variable <m>x</m>.
            <men xml:id="eqn-cdf-definition">
                F (x) = \int_{-\infty}^x\, \rho(x')\, dx' = P( X \le x).
            </men>
            The integral-derivative relation in the fundamental theorem of Calculus immediately gives us the following relation between <m>F</m> and <m>\rho</m>.
            <men xml:id="eqn-cdf-pdf-relation">
                \rho(x) = \frac{dF}{dx}.
            </men>
            From the definition in Eq. <xref ref="eqn-cdf-definition"/> it is clear that when <m>x=\infty</m>, then we would be integrating the PDF over the entire real line, which by normalization should be <m>1</m>.
            <me>
                F(+\infty) = 1.
            </me>
            Similarly, when <m>x=-\infty</m>, the integral should be zero.
            <me>
                F(-\infty) = 0.
            </me>
            Thus over the real line, CDF goes from <m>0</m> at <m>x=-\infty</m> to <m>1</m> at <m>x=+\infty</m>. <xref ref="fig-cdf-compared-normal-uniform"/> shows this behavior of CDF of two distributions. Notice that the shape of the CDF's are different for different distribution functions and carry the information of the probabilities of events in different intervals.
            
        </p>


        <figure xml:id="fig-cdf-compared-normal-uniform">
            <caption>Comparing cumulative distribution functions (CDF) of a standard normal distribution and a uniform distribution which is non-zero between  <m>1</m> and <m>3</m>. </caption>
            <image source="./images/essential-probability-and-statistics/cdf-compared-normal-uniform.png">
                <shortdescription>Comparing cumulative distribution functions (CDF) of a standard normal distribution and a uniform distribution.</shortdescription>
            </image>
        </figure>


    </subsection>


</section>