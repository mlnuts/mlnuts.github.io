<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-Inferential-Statistics">
    <title>Inferential Statistics</title>
    <introduction>
        <p>
            Inferential statistics is the branch of statistics that enables us to draw conclusions about a population based on data from a sample. Unlike descriptive statistics, which summarize observed data, inferential statistics use probability theory to make generalizations about the population from which the sample is drawn. This is essential in fields such as medicine, economics, and social sciences, where measuring an entire population is often impractical. For example, a survey of 1,000 voters can estimate the preferences of millions. This section focuses on frequentist methods, which rely on hypothesis testing and confidence intervals, but Bayesian methods, which incorporate prior knowledge to compute probabilities of hypotheses, are also used in statistical inference.
        </p>
        <p>
            The core concepts of inferential statistics include:
        </p>
        <ul>
            <li><p><term>Population</term>: The entire group of interest.</p></li>
            <li><p><term>Sample</term>: A subset of the population that is observed.</p></li>
            <li><p><term>Parameter</term>: A numerical characteristic of the population (e.g., population mean <m>\mu</m>, variance <m>\sigma^2</m>).</p></li>
            <li><p><term>Statistic</term>: A numerical characteristic computed from the sample (e.g., sample mean <m>\bar{x}</m>, sample proportion <m>\hat{p}</m>).</p></li>
        </ul>
    </introduction>

    <subsection xml:id="subsec-Point-Estimation">
        <title>Point Estimation</title>
        <p>
            Point estimation involves using a sample statistic to estimate a population parameter. For example, the sample mean <m>\bar{x}</m> is used to estimate the population mean <m>\mu</m>, and the sample proportion <m>\hat{p}</m> estimates the population proportion <m>p</m>. A good point estimator should be unbiased (its expected value equals the true parameter) and have low variance (it is precise).
        </p>
        <p>
            For instance, if we measure the heights of 50 randomly selected adults and find a sample mean of <m>\bar{x} = 170 \, \text{cm}</m>, we use <m>170 \, \text{cm}</m> as a point estimate for the population mean height <m>\mu</m>. However, point estimates do not convey uncertainty, which is why confidence intervals are often used to provide a range of plausible values.
        </p>
    </subsection>

    <subsection xml:id="subsec-Sampling-Distributions">
        <title>Sampling Distributions</title>
        <p>
            A sampling distribution is the probability distribution of a statistic over repeated samples of the same size from a population. For example, if we repeatedly draw samples of size <m>n</m> and compute the sample mean <m>\bar{x}</m>, the distribution of <m>\bar{x}</m> values is the sampling distribution of the mean.
        </p>
        <p>
            The Central Limit Theorem (CLT) states that for large sample sizes, the sampling distribution of the sample mean or proportion is approximately normal, regardless of the population distribution, provided certain conditions are met (e.g., <m>np \geq 5</m> and <m>n(1-p) \geq 5</m> for proportions). This property underpins hypothesis testing and confidence intervals, allowing us to use the normal distribution for inference.
        </p>
    </subsection>

    <subsection xml:id="subsec-Hypothesis-Testing">
        <title>Hypothesis Testing</title>
        <p>
            Hypothesis testing is a cornerstone of inferential statistics, allowing us to test claims about population parameters using sample data. It involves a structured process to determine whether the data provide sufficient evidence to reject a default claim, called the null hypothesis.
        </p>
        <p>
            The steps of hypothesis testing are:
        </p>
        <ol>
            <li><p><term>Null hypothesis</term> (<m>H_0</m>): The default claim, often stating no effect or no difference (e.g., <m>p = 0.5</m>).</p></li>
            <li><p><term>Alternative hypothesis</term> (<m>H_a</m>): The claim we seek evidence for (e.g., <m>p \neq 0.5</m>).</p></li>
            <li><p>Choose a significance level <m>\alpha</m> (e.g., 0.05), the probability of rejecting <m>H_0</m> when it is true.</p></li>
            <li><p>Compute a test statistic from the sample data.</p></li>
            <li><p>Find the <term>p-value</term> or critical region.</p></li>
            <li><p>Decision: Reject <m>H_0</m> if the p-value is less than <m>\alpha</m>; otherwise, fail to reject <m>H_0</m>. Failing to reject <m>H_0</m> does not prove it is true, only that the evidence is insufficient to reject it.</p></li>
        </ol>
        <p>
            A common misconception is that the p-value represents the probability that the null hypothesis is true. Instead, it is the probability of observing data as extreme as the sample, assuming <m>H_0</m> is true. A small p-value (e.g., <m>p &lt; 0.05</m>) suggests strong evidence against <m>H_0</m>, but it does not quantify the probability of <m>H_0</m> or <m>H_a</m>.
        </p>
        <p>
            <alert>Worked Example: Coin Toss</alert>
        </p>
        <p>
            Suppose we suspect a coin is biased because it lands on heads more often. To test if it is fair, we define:
            <md>
                <mrow> \amp H_0: p = 0.5 \quad (\text{coin is fair})</mrow>
                <mrow> \amp H_a: p \neq 0.5 \quad (\text{coin is biased})</mrow>
            </md>
            where <m>p</m> is the probability of heads. We toss the coin 100 times and observe 52 heads, so the sample proportion is:
            <me>
                \hat{p} = \frac{52}{100} = 0.52.
            </me>
        </p>
        <p>
            We set the significance level at <m>\alpha = 0.02</m> (98% confidence). The Central Limit Theorem applies because <m>np_0 = 100 \cdot 0.5 = 50 \geq 5</m> and <m>n(1-p_0) = 50 \geq 5</m>, ensuring the sample proportion is approximately normal. The standard error under <m>H_0</m> is:
            <me>
                \sigma_\text{samp} = \sqrt{\frac{p_0(1-p_0)}{n}} = \sqrt{\frac{0.5 \cdot 0.5}{100}} = 0.05.
            </me>
            The test statistic is:
            <me>
                Z = \frac{\hat{p} - p_0}{\sigma_\text{samp}} = \frac{0.52 - 0.5}{0.05} = 0.4.
            </me>
        </p>
        <p>
            For a two-tailed test, the p-value is:
            <me>
                \text{p-value} = 2 \cdot (1 - \Phi(0.4)) \approx 2 \cdot (1 - 0.6554) = 0.6892.
            </me>
            Since <m>0.6892 &gt; \alpha = 0.02</m>, we fail to reject <m>H_0</m>. The significance level <m>\alpha</m> must be chosen before the experiment to avoid bias (e.g., adjusting <m>\alpha</m> to 0.7 after seeing the p-value). There is insufficient evidence to conclude the coin is biased, but this does not prove the coin is fair.
        </p>
        <p>
            <alert>Visualization</alert>
        </p>
        <p>
            The following Python code generates a plot of the standard normal distribution with the test statistic <m>Z = 0.4</m> marked and the two-tailed p-value regions shaded:
        </p>
        <program language="python">
            <code>
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Define the range for the x-axis
x = np.linspace(-4, 4, 1000)
# Standard normal distribution
y = norm.pdf(x, 0, 1)
# Test statistic
z = 0.4

# Create the plot
plt.figure(figsize=(8, 5))
plt.plot(x, y, 'b-', label='Standard Normal Distribution')
# Shade the right tail (z &gt; 0.4)
x_right = np.linspace(z, 4, 100)
plt.fill_between(x_right, norm.pdf(x_right, 0, 1), color='red', alpha=0.3, label='p-value region (two-tailed)')
# Shade the left tail (z &lt; -0.4)
x_left = np.linspace(-4, -z, 100)
plt.fill_between(x_left, norm.pdf(x_left, 0, 1), color='red', alpha=0.3)
# Mark the test statistic
plt.axvline(z, color='black', linestyle='--', label=f'Z = {z}')
plt.axvline(-z, color='black', linestyle='--')
plt.title('Standard Normal Distribution with Test Statistic and p-value')
plt.xlabel('Z')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.show()
            </code>
        </program>
        <figure xml:id="fig-p-value">
            <caption>Plot of the standard normal distribution with the test statistic <m>Z = 0.4</m> marked by dashed lines and the two-tailed p-value regions shaded in red.</caption>
            <image source="./images/essential-probability-and-statistics/p-value.png">
                <shortdescription>Plot of the standard normal distribution with the test statistic and p-value regions shaded.</shortdescription>
            </image>
        </figure>
        <p>
            The plot in <xref ref="fig-p-value"/> shows the test statistic <m>Z = 0.4</m> close to the mean, indicating a large p-value and weak evidence against <m>H_0</m>.
        </p>
        <p>
            Alternatively, we can construct a 98% confidence interval for <m>p</m> using the standard error of the proportion, <m>\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} = \sqrt{\frac{0.52 \cdot 0.48}{100}} \approx 0.05</m>:
            <md>
                <mrow> \text{CI} \amp = \left[ 0.52 \pm 2.326 \cdot 0.05 \right] \approx [0.4037, 0.6363].</mrow>
            </md>
            Since <m>0.5</m> lies within this interval, we fail to reject <m>H_0</m>, consistent with the p-value approach.
        </p>
        <p>
            <alert>Real-World Application: Drug Efficacy</alert>
        </p>
        <p>
            In medicine, hypothesis testing is used to evaluate drug efficacy. Suppose a new drug claims to reduce blood pressure by at least 10 mmHg. We test <m>H_0: \mu = 0</m> (no reduction) versus <m>H_a: \mu &gt; 0</m> (reduction). Data from 30 patients show a mean reduction of <m>\bar{x} = 12 \, \text{mmHg}</m> with a sample standard deviation <m>s = 15 \, \text{mmHg}</m>. Using a t-test (since <m>\sigma</m> is unknown and <m>n = 30</m> is moderate), we compute the t-statistic and p-value to determine if the evidence supports the drug’s efficacy.
        </p>
    </subsection>

    <subsection xml:id="subsec-Confidence-Intervals">
        <title>Confidence Intervals</title>
        <p>
            A confidence interval (CI) estimates a population parameter with a range of values, associated with a confidence level (e.g., 98%). For the coin toss example, the 98% CI for the true proportion <m>p</m> is <m>[0.4037, 0.6363]</m>, meaning we are 98% confident that <m>p</m> lies within this range. The confidence level indicates that if we repeated the sampling process many times, 98% of such intervals would contain the true parameter. It does not mean there is a 98% probability that <m>p</m> lies in this specific interval.
        </p>
        <p>
            For a population mean, the CI is:
            <me>
                \left[ \bar{x} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}} \right],
            </me>
            where <m>\bar{x}</m> is the sample mean, <m>\sigma</m> is the population standard deviation, <m>n</m> is the sample size, and <m>z_{\alpha/2}</m> is the critical value (e.g., 2.326 for 98% confidence). If <m>\sigma</m> is unknown, the sample standard deviation <m>s</m> is used, and for small samples (<m>n &lt; 30</m>), a t-distribution is applied instead of the normal distribution.
        </p>
        <p>
            CIs complement hypothesis testing by providing a range of plausible values for the parameter. For example, in the drug efficacy case, a 95% CI for the mean blood pressure reduction might be <m>[8, 16] \, \text{mmHg}</m>, suggesting the true effect is likely positive.
        </p>
    </subsection>

    <subsection xml:id="subsec-Types-of-Errors">
        <title>Types of Errors</title>
        <p>
            Hypothesis testing carries risks of incorrect decisions. A <term>Type I error</term> occurs when we reject <m>H_0</m> when it is true (false positive), with probability <m>\alpha</m> (e.g., 0.02 for a 2% chance). A <term>Type II error</term> occurs when we fail to reject <m>H_0</m> when <m>H_a</m> is true (false negative), with probability <m>\beta</m>. Reducing <m>\alpha</m> increases <m>\beta</m> unless the sample size or effect size is increased.
        </p>
        <p>
            In the coin toss example, a Type I error would be concluding the coin is biased when it is fair. A Type II error would be failing to detect a bias when the coin is biased.
        </p>
        <p>
            <alert>Visualization: Types of Errors</alert>
        </p>
        <p>
            The following table illustrates the possible outcomes of a hypothesis test:
        </p>
        <table xml:id="table-error-types">
            <title>Hypothesis Testing Outcomes</title>
            <tabular>
                <row header="yes">
                    <cell></cell>
                    <cell><m>H_0</m> True</cell>
                    <cell><m>H_0</m> False</cell>
                </row>
                <row>
                    <cell>Reject <m>H_0</m></cell>
                    <cell>Type I Error (<m>\alpha</m>)</cell>
                    <cell>Correct (Power: <m>1-\beta</m>)</cell>
                </row>
                <row>
                    <cell>Fail to Reject <m>H_0</m></cell>
                    <cell>Correct (<m>1-\alpha</m>)</cell>
                    <cell>Type II Error (<m>\beta</m>)</cell>
                </row>
            </tabular>
        </table>
    </subsection>

    <subsection xml:id="subsec-Power-Effect-Size">
        <title>Statistical Power and Effect Size</title>
        <p>
            The <term>power</term> of a test is the probability of correctly rejecting a false null hypothesis (<m>1 - \beta</m>). Power depends on the sample size, effect size, and significance level <m>\alpha</m>. The <term>effect size</term> measures the magnitude of the difference or relationship, such as Cohen’s d for the difference in means or the odds ratio for proportions.
        </p>
        <p>
            For example, in the drug efficacy example, the effect size might be the mean blood pressure reduction (e.g., 10 mmHg). A larger effect size or sample size increases power, making it easier to detect a true effect. Power analysis determines the sample size needed to achieve a desired power (e.g., 80%) for a given effect size and <m>\alpha</m>.
        </p>
    </subsection>

    <subsection xml:id="subsec-Multiple-Testing">
        <title>Multiple Testing</title>
        <p>
            When conducting multiple hypothesis tests, the probability of at least one Type I error increases. For example, if 20 tests are performed at <m>\alpha = 0.05</m>, the chance of at least one false positive is approximately <m>1 - (1 - 0.05)^{20} \approx 0.64</m>. Methods like the Bonferroni correction adjust the significance level (e.g., <m>\alpha / m</m> for <m>m</m> tests) to control the overall Type I error rate.
        </p>
        <p>
            For instance, in a study testing 10 drugs, using <m>\alpha = 0.005</m> per test ensures the overall Type I error rate remains near 0.05. However, this reduces power, so researchers must balance error control and test sensitivity.
        </p>
    </subsection>

    <subsection xml:id="subsec-Other-Tests">
        <title>Common Statistical Tests</title>
        <p>
            Beyond the z-test, other common tests include:
            <ul>
                <li><p><term>t-test</term>: Used for small samples or when the population variance is unknown. For example, to test if a new teaching method improves test scores, we collect scores from 20 students, compute the sample mean difference <m>\bar{x} = 5</m> points and standard deviation <m>s = 10</m>, and use a t-test to compare against <m>H_0: \mu = 0</m>.</p></li>
                <li><p><term>Chi-square test</term>: Used for categorical data. For example, a survey of 200 people tests if political affiliation (Party A, Party B, Independent) is independent of age group (young, middle-aged, older) by comparing observed and expected frequencies.</p></li>
                <li><p><term>ANOVA</term>: Used to compare means across multiple groups, e.g., testing if different diets affect weight loss.</p></li>
                <li><p><term>Non-parametric tests</term>: Used when assumptions like normality are violated, such as the Mann-Whitney U test for comparing two groups or the Kruskal-Wallis test for multiple groups.</p></li>
            </ul>
        </p>
        <p>
            Each test relies on specific assumptions. For example, z-tests and t-tests assume approximately normal data (or large samples for the CLT) and independent observations. The chi-square test requires expected frequencies of at least 5 per category, and ANOVA assumes homogeneity of variances across groups. Violating these assumptions may necessitate non-parametric tests.
        </p>
    </subsection>
</section>

<!--Revised by Grok-->
<!--
<section xml:id="sec-Inferential-Statistics">
    <title>Inferential Statistics</title>
    <introduction>
        <p>
            Inferential statistics is the branch of statistics that allows us to make conclusions about a population based on data sampled from it. While descriptive statistics summarize data, inferential statistics use probability theory to make inferences about the population based on sample data. This is crucial in fields like medicine, economics, and social sciences, where we often cannot measure an entire population but need to make informed decisions. For example, a poll of 1,000 voters can estimate the preferences of millions.
        </p>
        <p>
            The core ideas of inferential statistics are:
        </p>
        <ul>
            <li><p><term>Population</term>: The entire group we want to learn about.</p></li>
            <li><p><term>Sample</term>: A subset of the population that we actually observe.</p></li>
            <li><p><term>Parameter</term>: A numerical property of the population (e.g., true mean, variance).</p></li>
            <li><p><term>Statistic</term>: A numerical property computed from the sample (e.g., sample mean).</p></li>
        </ul>
    </introduction>
    <subsection xml:id="subsec-Hypothesis-Testing">
        <title>Hypothesis Testing</title>
        <p>
            Perhaps the most important part of inferential statistics is hypothesis testing. As the name implies, it’s a test of whether a particular claim (i.e., hypothesis) can be refuted based on the data collected.
        </p>
        <p>
            Hypothesis testing is a formal procedure for testing claims about population parameters using sample data. It follows a structured framework, which we’ll list here for reference and illustrate with an example.
        </p>
        <ol>
            <li><p><term>Null hypothesis</term> (<m>H_0</m>): The default claim (e.g., no difference).</p></li>
            <li><p><term>Alternative hypothesis</term> (<m>H_a</m>): What we seek evidence for.</p></li>
            <li><p>Choose a significance level <m>\alpha</m> (e.g., 0.05).</p></li>
            <li><p>Compute a test statistic from data.</p></li>
            <li><p>Find the <term>p-value</term> or critical region.</p></li>
            <li><p>Decision: Reject <m>H_0</m> if p-value <m>&lt; \alpha</m>. You can only reject a hypothesis.</p></li>
        </ol>
        <p>
            <alert>Worked Example: Coin Toss</alert>
        </p>
        <p>
            Suppose we suspect a coin is biased because it seems to land on heads more often. To test if it’s fair, we define:
            <md>
                <mrow> \amp H_0: p = 0.5 \quad (\text{coin is fair})</mrow>
                <mrow> \amp H_a: p \neq 0.5 \quad (\text{coin is biased})</mrow>
            </md>
            where <m>p</m> is the probability of heads. We toss the coin 100 times and observe 52 heads, so the sample proportion is:
            <me>
                \hat{p} = \frac{52}{100} = 0.52.
            </me>
        </p>
        <p>
            We set the significance level at <m>\alpha = 0.02</m> (98% confidence). Using the Central Limit Theorem (CLT), the sample proportion <m>\hat{p}</m> is approximately normal with mean <m>p_0 = 0.5</m> (under <m>H_0</m>) and standard error:
            <me>
                \sigma_\text{samp} = \sqrt{\frac{p_0(1-p_0)}{n}} = \sqrt{\frac{0.5 \cdot 0.5}{100}} = 0.05.
            </me>
            The test statistic is:
            <me>
                Z = \frac{\hat{p} - p_0}{\sigma_\text{samp}} = \frac{0.52 - 0.5}{0.05} = 0.4.
            </me>
        </p>
        <p>
            For a two-tailed test, the p-value is:
            <me>
                \text{p-value} = 2 \cdot (1 - \Phi(0.4)) \approx 2 \cdot (1 - 0.6554) = 0.6892.
            </me>
            Since <m>0.6892 \gt \alpha = 0.02</m>, we fail to reject <m>H_0</m> at the significane level, which was chosen BEFORE the experiment was conducted. (Not choosing the <m>\alpha</m> before the experiment has the danger that once you see your p-value, you might say let's set, say <m>\alpha = 0.7</m> here, just to be able to reject the null hypothesis, <m>H_0</m>, thus introducin your bias in the test.) There is insufficient evidence to conclude the coin is biased. This does not prove the coin is fair, only that our data does not provide strong evidence of bias.
        </p>
        <p>
            <alert>Visualization</alert>
        </p>
        <p>
            To visualize the test statistic, consider the standard normal distribution, where <m>Z = 0.4</m> lies close to the mean, indicating a high p-value. A plot of the distribution, the test-statistic and p-value regions are shown in <xref ref="fig-p-value"/> which was generated by the following Python code:
        </p>



        <program language="python">
            <code> 
            import numpy as np
            import matplotlib.pyplot as plt
            from scipy.stats import norm

            # Define the range for the x-axis
            x = np.linspace(-4, 4, 1000)
            # Standard normal distribution
            y = norm.pdf(x, 0, 1)
            # Test statistic
            z = 0.4

            # Create the plot
            plt.figure(figsize=(8, 5))
            plt.plot(x, y, 'b-', label='Standard Normal Distribution')
            # Shade the right tail (z &gt; 0.4)
            x_right = np.linspace(z, 4, 100)
            plt.fill_between(x_right, norm.pdf(x_right, 0, 1), color='red', alpha=0.3, label='p-value region (two-tailed)')
            # Shade the left tail (z &lt; -0.4)
            x_left = np.linspace(-4, -z, 100)
            plt.fill_between(x_left, norm.pdf(x_left, 0, 1), color='red', alpha=0.3)
            # Mark the test statistic
            plt.axvline(z, color='black', linestyle='\-\-', label=f'Z = {z}')
            plt.axvline(-z, color='black', linestyle='\-\-')
            plt.title('Standard Normal Distribution with Test Statistic and p-value')
            plt.xlabel('Z')
            plt.ylabel('Density')
            plt.legend()
            plt.grid(True)
            plt.show()
            </code>
        </program>

        <figure xml:id="fig-p-value">
            <caption> Plot of the standard normal distribution, with the test statistic and p-value regions shaded. </caption>
            <image source="./images/essential-probability-and-statistics/p-value.png">
                <shortdescription>Plot of the standard normal distribution, with the test statistic and p-value regions shaded. </shortdescription>
            </image>
        </figure>
        <p>
            The plot in <xref ref="fig-p-value"/> shows the standard normal distribution with the test statistic <m>Z = 0.4</m> marked by dashed lines and the p-value regions (both tails) shaded in red.
        </p>
        <p>
            Alternatively, we can construct a 98% confidence interval using the sample standard deviation <m>\sigma_1 = \sqrt{\hat{p}(1-\hat{p})} = \sqrt{0.52 \cdot 0.48} \approx 0.4996</m>:
            <md>
                <mrow> \text{CI} \amp = \left[ 0.52 \pm 2.326 \cdot \frac{0.4996}{\sqrt{100}} \right] \approx [0.4038, 0.6362].</mrow>
            </md>
            Since <m>0.5</m> lies within this interval, we again fail to reject <m>H_0</m>, consistent with the p-value approach.
        </p>
        <p>
            <alert>Real-World Application: Drug Efficacy</alert>
        </p>
        <p>
            Hypothesis testing is widely used in medicine. Suppose a new drug claims to reduce blood pressure by at least 10 mmHg. We could set up a null hypothesis <m>H_0</m>: the drug has no effect (mean reduction = 0 mmHg) versus <m>H_a</m>: the drug reduces blood pressure (mean reduction <m>> 0</m> mmHg). By collecting data from a sample of patients and applying the hypothesis testing steps, we can determine if there’s sufficient evidence to support the drug’s efficacy.
        </p>
    </subsection>
    <subsection xml:id="subsec-Confidence-Intervals">
        <title>Confidence Intervals</title>
        <p>
            A confidence interval (CI) estimates a population parameter with a range of values, associated with a confidence level (e.g., 98%). For our coin toss example, the 98% CI for the true probability of heads is <m>[0.4038, 0.6362]</m>, meaning we are 98% confident that the true <m>p</m> lies within this range. The CI is constructed using the sample statistic (e.g., <m>\hat{p} = 0.52</m>) and a margin of error based on the standard error and critical values from the normal distribution.
        </p>
        <p>
            For a population mean, the CI is typically:
            <me>
                \left[ \bar{x} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}} \right],
            </me>
            where <m>\bar{x}</m> is the sample mean, <m>\sigma</m> is the standard deviation, <m>n</m> is the sample size, and <m>z_{\alpha/2}</m> is the critical value for the desired confidence level. CIs provide a range of plausible values for the parameter, complementing hypothesis testing.
        </p>
    </subsection>
    <subsection xml:id="subsec-Types-of-Errors">
        <title>Types of Errors</title>
        <p>
            Hypothesis testing involves risks of incorrect decisions. The significance level <m>\alpha</m> represents the probability of a <term>Type I error</term> (rejecting <m>H_0</m> when it is true), also called the “false positive rate.” For <m>\alpha = 0.02</m>, we accept a 2% chance of this error. A <term>Type II error</term> occurs when we fail to reject <m>H_0</m> when <m>H_a</m> is true (a “false negative”). The probability of a Type II error, denoted <m>\beta</m>, depends on the sample size and effect size.
        </p>
        <p>
            In our coin toss example, a Type I error would be concluding the coin is biased when it is actually fair. A Type II error would be failing to detect a bias when the coin is indeed biased.
        </p>
    </subsection>
    <subsection xml:id="subsec-Other-Tests">
        <title>Common Statistical Tests</title>
        <p>
            Beyond the z-test used in our coin toss example, other tests include:
            <ul>
                <li><p><term>t-test</term>: Used for small samples or when the population variance is unknown.</p></li>
                <li><p><term>Chi-square test</term>: Used for categorical data, e.g., testing if observed frequencies match expected ones.</p></li>
                <li><p><term>ANOVA</term>: Used to compare means across multiple groups.</p></li>
            </ul>
            Each test follows a similar hypothesis testing framework but applies to different scenarios, depending on the data type and research question.
        </p>
    </subsection>
</section>
-->

<!---
<section xml:id="sec-Inferential-Statistics">
    <title>Inferential-Statistics</title>
    <introduction>
        <p>
            Inferential statistics is the branch of statistics that allows us to make 
            conclusions about a population based on data sampled from it. 
            While descriptive statistics summarize data, inferential statistics 
            generalize beyond the data at hand using probability theory.
        </p>
        <p>
            The <alert>core ideas</alert> of inferential statistics are:
                <ul>
                    <li><p><term>Population</term>: The entire group we want to learn about.</p></li>
                    <li><p><term>Sample</term>: A subset of the population that we actually observe.</p></li>
                    <li><p><term>Parameter</term>: A numerical property of the population (e.g., true mean, variance).</p></li>
                    <li><p><term>Statistic</term>: A numerical property computed from the sample (e.g., sample mean).</p></li>
                </ul>
        </p>
    </introduction>
    <subsection xml:id="subsec-Hypothesis-Testing">
        <title>Hypothesis Testing</title>
        <p>
            Perhaps the most important part of inferential testing is the Hypothesis Testing. As the name implies, it a test of whether a particular claim (i.e., hypothesis) can be refuted based on the contents of the data collected. 
        </p>

        <p>
            Hypothesis testing is the formal procedure for testing claims about population parameters using sample data. It proceeds in structured steps and looks too stodgy. Let's just write them down here, only for reference. We will illustrate by example how we do these steps.
        </p>

        <ol>
            <li><p><term>Null hypothesis</term> (<m>H_0</m>): the default claim (e.g., no difference).</p></li>
            <li><p><term>Alternative hypothesis</term> (<m>H_a</m>): what we seek evidence for.</p></li>
            <li><p>Choose a significance level <m>\alpha</m> (e.g., 0.05).</p></li>
            <li><p>Compute a test statistic from data.</p></li>
            <li><p>Find the <term>p-value</term> or critical region.</p></li>
            <li><p>Decision: reject <m>H_0</m> if p-value &lt; <m>\alpha</m>. You can only reject a hypothesis.</p></li>
        </ol>
        <p>
            <alert>Worked Example: Coin Toss:</alert>
        </p>
        <p>
            Suppose we want to test whether a coin is fair by looking at the probability <m>p</m> of getting a head when tossed. The hypotheses will be:
            <md>
                <mrow> \amp H_0: p = p_0\qquad (\text{coin is fair})</mrow>
                <mrow> \amp H_a: p \neq p_0\qquad (\text{coin is NOT fair}, i.e., biased)</mrow>
            </md>
            where I am using symbol <m>p_0</m> for what is expected in the hypothesis <m>h_0</m>, which is, of course <m>0.05</m> for a fair coin.
            <me>
                p_0 = 0.05.
            </me>
            
        </p>
        <p>
            To decide which is the case, we conduct an experiment and collect data. Here, let's toss the said coin <m>n</m> times and try to use the mean of the tosses in conjunction with the Central Limit Theorem (CLT).
        </p>
        <p>
            Suppose, when we toss the coin <m>100</m> times, we get <m>52</m> heads and <m>48</m> tails. From this, we get an estimate of the probability of heads to be
            <me>
                p_1 = \frac{52}{100} = 0.52,
            </me>
            and the variance of these <m>100</m> numbers, treating heads as <m>1</m> and tails as <m>0</m>, is calculated as follows. The sample mean is <m>\bar{X}_n = 0.52</m>. The sample variance is:
            <md>
                <mrow>\sigma_1^2 \amp = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2</mrow>
                <mrow> \amp = \frac{52 \cdot (1 - 0.52)^2 + 48 \cdot (0 - 0.52)^2}{99}</mrow>
                <mrow> \amp = \frac{52 \cdot 0.48^2 + 48 \cdot 0.52^2}{99} \approx 0.2496.</mrow>
            </md>
            Clearly, <m>0.52 \ne 0.50</m>. So, it may be tempting to say that the coin is not fair, having tossed so many times! But this turns out to be a premature conclusion. What we need is a level of confidence in our conclusion, not just a yes or no answer. For that, we turn to probability theory and the CLT.
        </p>
        <p>
            <alert>First, we decide a level of confidence we desire!</alert> In hypothesis testing, we use the letter <m>\alpha</m> to denote the significance level, the probability of rejecting the null hypothesis when it is true. Here, if we want to be <m>98\%</m> sure, we set <m>\alpha = 0.02</m>. We do not use percentage notation for <m>\alpha</m>.
        </p>
        <p>
            We can consider each toss to be an independent random variable: for <m>n</m> tosses, we have <m>n</m> identical random variables, <m>X_i,\ i=1,2,\cdots,n</m>. Each of these variables produces either <m>1</m> (when heads) or <m>0</m> (when tails). To make use of the CLT, we introduce the sample mean variable by
            <me>
                \bar{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n}.
            </me>
            For large enough <m>n</m>, its distribution is approximately Gaussian with the mean approximating the <m>p</m> we are looking for and a standard deviation, which we can use to set confidence intervals at a particular significance level. <alert>If the <m>p = 0.5</m>, required by hypothesis <m>H_0</m>, is outside of the confidence interval, then we can rule out this hypothesis.</alert> This is the crux of the decision process.
        </p>
        <p>
            According to the CLT, the standardized sample mean is:
            <me>
                Z = \sqrt{n} \frac{\bar{X}_n - p}{\sigma_1} \sim \mathcal{N}(0,1),
            </me>
            where <m>\sigma_1</m> is the sample standard deviation. Since, we will use critical values of <m>Z</m> to make our decision, it is the <term>text statistic</term> here.
        </p>  
        <p>  
            To find the right end of the confidence interval at <m>\alpha = 0.02</m>, we solve
            <me>
                \Phi(z) = 1 - \frac{\alpha}{2} = 1 - \frac{0.02}{2} = 0.99.
            </me>
            This gives
            <me>
                z_{0.99} = 2.326.
            </me>
            Similarly, to find the left end of the confidence interval, we solve
            <me>
                \Phi(z) = \frac{\alpha}{2} = 0.01
            </me>
            This gives
            <me>
                z_{0.01} = -2.326.
            </me>
            Therefore, at this confidence level, the true <m>p</m> must lie somewhere in the confidence interval (CI):
            <md>
                <mrow> \text{CI} \amp = \left[ p_1 + z_{0.01} \times \frac{\sigma_1}{\sqrt{100}},\quad p_1 + z_{0.99} \times \frac{\sigma_1}{\sqrt{100}} \right]</mrow>
                <mrow> \amp = \left[ 0.52 - 2.326 \times \frac{\sqrt{0.2496}}{10},\quad 0.52 + 2.326 \times \frac{\sqrt{0.2496}}{10} \right] </mrow>
                <mrow> \amp  \approx [0.52 - 0.1162,\quad 0.52 + 0.1162] \approx [0.4038,\quad 0.6362].</mrow>
            </md>
            Since, according to hypothesis <m>H_0</m>, <m>p = p_0 = 0.5</m> and this value is within the confidence interval, we say that at this level of confidence, this hypothesis cannot be ruled out.
        </p>
        <p>
            <alert>The p-value:</alert>
        </p>
        <p>
            The p-value represents the probability of observing a result at least as extreme as the one obtained, assuming the null hypothesis is true. This p is not the <m>p=0.5</m> in our example above. 
        </p>  
        <p>  
            We will use the sample data of <m>100</m> tosses with <m>52</m> heads and <m>48</m> tails and the Central Limit Theorem (CLT) to compute the <m>p</m>-value. <alert>Since we must assume that <m>H_0</m> is true</alert>, the variance of individual tosses, which are Bernoulli variables, will be
            <me>
                \sigma_0^2 = p_0(1-p_0) = 0.5(1-0.5) = 0.25.
            </me>
        </p>  
        <p>
            From, this we get the sample variance to be
            <me>
                \sigma_\text{samp}^2 = \frac{\sigma_0^2}{n} = \frac{0.25}{100} = 0.0025\quad\Rightarrow \sigma_\text{samp} = \sqrt{0.0025} = 0.05.
            </me>
        </p>  
        <p>  
            The test statistic now under the <m>H_0</m> assumption will be the scaled deviation of the observed mean from the presumed mean of asumed to be correct hypothesis <m>H_0</m>.
            <me>
                Z = \frac{ \left( \bar{X}_{n}  - p_0 \right)  }{\sigma_\text{samp}},
            </me>
            which has standard normal distribution according to CLT.
            <me>
                Z \sim \mathcal{N}(0,1).
            </me>
        </p>  
        <p>  
            In our <m>100</m>-toss experiment, we found <m>\bar{X}_{n} = 0.52</m> as found in the experiment. Thus, the value of the statistic is
            <me>
                z_\text{data} = \frac{0.52 - 0.50}{0.05} = 0.4. 
            </me>
            Now, from the CDF of standard normal distribution, we need to address how extreme is this value by looking at the following probability of seeing this and larger value of <m>Z</m> on either side of the mean zero of <m>Z \sim \mathcal{N}(0,1)</m>.
            <me>
                P( |Z| \gt z_\text{data} ).
            </me>
            This is called the p-value of the data. The CDF gives probability less than or equal to some <m>z</m>.
            <me>
                \Phi(z) = P(Z \le z).
            </me>
            This will give us
            <me>
                P( Z \gt z_\text{data} ) = 1- \Phi(z_\text{data}).
            </me>
            But, we want <m>P( |Z| \gt z_\text{data} )</m> for <m>Z</m> to go both sides, the so-called two-tail test. That will mean
            <men xml:id="eqn-two-tail-p-value">
                P( |Z| \gt z_\text{data} ) = 2\times \left( 1- \Phi(z_\text{data}) \right).
            </men>
            For <m>z_\text{data}</m>, 
            <me>
                \Phi(0.4) \approx 0.6554.
            </me>
            Therefore, p-value of the given data will be
            <me>
                \text{p-value} = 2( 1 - 0.6554) = 2\times 0.3446 = 0.6892.
            </me>
            A p-value of <m>0.6892</m> suggests there is a <m>68.92\%</m> chance of observing a sample proportion at least as extreme as <m>0.52</m>, i.e, <m>52</m> heads in 100 tosses. So we <alert>fail to reject</alert> <m>H_0</m>, consistent with the conclusion above based on <m>\alpha</m>.
            
        </p>
    </subsection>
</section>
-->