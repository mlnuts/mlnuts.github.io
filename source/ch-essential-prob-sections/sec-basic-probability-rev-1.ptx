<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-Basic-Probability">
    <title>Basic Probability for Machine Learning</title>
    <introduction>
        <p>
            Probability is the backbone of machine learning, helping us model uncertainty in data, predictions, and outcomes. In machine learning, probability underpins tasks like classification (e.g., predicting labels), evaluating model confidence, and handling noisy data. This section introduces probability concepts such as sample spaces, events, and axioms—and connects them to practical machine learning applications using Python. We will use the student dataset from <xref ref="sec-data-types-for-machine-learning"/> to illustrate ideas.
        </p>
        <p>
            An <alert>event</alert> is a specific outcome or set of outcomes from an experiment, represented as a set. For a coin toss, "heads" is <m>\{H\}</m>, "tails" is <m>\{T\}</m>, and "heads or tails" is <m>\{H, T\}</m>. Each trial answers whether an event occurred (yes/no). For a die roll yielding 2, events like <m>\{2\}</m> or <m>\{1,2,3\}</m> occur if they include 2. Sets allow combining events via union (<m>\cup</m>) or intersection (<m>\cap</m>), such as <m>\{H\} \cup \{T\} = \{H, T\}</m>.
        </p>
    </introduction>
    <subsection xml:id="subsec-Axiomatic-View-of-Probability">
        <title>Axiomatic View of Probability</title>
        <p>
            In 1933, Andrey Kolmogorov formalized probability with three axioms, providing a mathematical framework. Think of these as rules that ensure probabilities make sense, like ensuring a weather forecast never predicts negative rain or more than 100% chance.
        </p>
        <p>
            <alert>Sample Space <m>\Omega</m></alert>: The set of all possible outcomes. For a six-sided die, <m>\Omega = \{1, 2, 3, 4, 5, 6\}</m>. For a student passing an exam, <m>\Omega = \{\text{Pass}, \text{Fail}\}</m>.
        </p>
        <p>
            <alert>Event Space <m>F</m></alert>: All possible subsets of <m>\Omega</m>, including the empty set <m>\varnothing</m> (impossible event) and <m>\Omega</m> (event certain to happen). For a coin toss (<m>\Omega = \{H, T\}</m>), <m>F = \{\varnothing, \{H\}, \{T\}, \{H, T\}\}</m>. With <m>N</m> outcomes, <m>F</m> has <m>2^N</m> events.
        </p>
        <p>
            <alert>Probability Measure <m>P</m></alert>: Assigns a number <m>P(E)</m> to each event <m>E \in F</m>, representing its likelihood. For example, for a fair die, <m>P(\{1\}) = 1/6</m>. You only need <m>P</m> for elementary events, which are the elements of <m>\Omega</m>, each taken as one event since you can use the Aditivity law below to get probability of any event in the entire event space <m>F</m>.
        </p>
        <p>
            The probability space is the triplet <m>(\Omega, F, P)</m>. Kolmogorov’s axioms are:
            <ol marker="1">
                <li><alert>Non-negativity</alert>: <men xml:id="eqn-first-axiom-non-negativity">P(E) \ge 0</men> for all <m>E \in F</m>.</li>
                <li><alert>Normalization</alert>: <men xml:id="eqn-second-axiom-normalization">P(\Omega) = 1</men>, ensuring total certainty for event <m>\Omega</m>.</li>
                <li><alert>Additivity</alert>: For disjoint events (<m>E_1 \cap E_2 = \varnothing</m>), <men xml:id="eqn-third-axiom-additivity">P(E_1 \cup E_2) = P(E_1) + P(E_2)</men>.</li>
            </ol>
        </p>
        <p>
            Derived results:
            <mdn>
                <mrow> \amp P(\varnothing) = 0. </mrow>
                <mrow> \amp P(E^c) = 1 - P(E), \text{ where } E^c \text{ is the complement of } E. </mrow>
                <mrow> \amp P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2). </mrow>
            </mdn>
        </p>
        <p>
            The union formula accounts for overlap, as shown in <xref ref="fig-venn-diagram-E1-E2"/>. For a die, if <m>E_1 = \{1,3,5\}</m> (odd numbers), <m>E_2 = \{3,5,6\}</m>, then <m>P(E_1 \cup E_2) = P(\{1,3,5,6\}) = 4/6 = 2/3</m>.
        </p>
        <figure xml:id="fig-venn-diagram-E1-E2">
            <caption>Venn diagram illustrating events <m>E_1 = \{1,3,5\}</m> (odd numbers) and <m>E_2 = \{3,5,6\}</m> for 1,000 simulated rolls of a fair six-sided die. The left region (181) counts rolls of 1 (in <m>E_1</m> only), the right region (326) counts rolls of 6 (in <m>E_2</m> only), and the overlap (155) counts rolls of 3 or 5 (in <m>E_1 \cap E_2</m>). These counts estimate probabilities, with theoretical values <m>P(E_1) = 3/6</m>, <m>P(E_2) = 3/6</m>, <m>P(E_1 \cap E_2) = 2/6</m>, and <m>P(E_1 \cup E_2) = 4/6</m>. Deviations (e.g., 326 vs. expected 167 for <m>E_2</m> only) reflect random variation. This visualization supports the union formula <m>P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)</m>, used in machine learning for feature probability calculations.</caption>
            <image source="./images/essential-probability-and-statistics/venn-diagram-E1-E2.png">
                <shortdescription>Venn diagram of intersecting events.</shortdescription>
            </image>
        </figure>
        <program language="python" line-numbers="yes">
            <title>Simulating die roll for union probability</title>
            <code>
            # --- DIE ROLL VENN DIAGRAM ---
            import numpy as np
            from matplotlib_venn import venn2
            import matplotlib.pyplot as plt

            np.random.seed(42)
            n_trials = 1000
            rolls = np.random.randint(1, 7, n_trials)

            # Events
            e1 = np.isin(rolls, [1, 3, 5])  # Odd numbers
            e2 = np.isin(rolls, [3, 5, 6])  # 3,5,6
            e1_only = np.sum(e1 \amp; ~e2)
            e2_only = np.sum(e2 \amp; ~e1)
            both = np.sum(e1 \amp; e2)

            # Venn diagram
            plt.figure(figsize=(6, 4))
            venn2(subsets=(e1_only, e2_only, both), set_labels=('E1 (Odd)', 'E2 (3,5,6)'))
            plt.title('Venn Diagram of Die Roll Events')
            plt.savefig('venn-diagram-E1-E2.png', dpi=300)
            plt.show()

            # Probabilities
            p_e1 = np.mean(e1)
            p_e2 = np.mean(e2)
            p_inter = np.mean(e1 \amp; e2)
            p_union = np.mean(e1 | e2)
            print(f"P(E1): {p_e1:.3f}, P(E2): {p_e2:.3f}, P(E1 ∩ E2): {p_inter:.3f}, P(E1 ∪ E2): {p_union:.3f}")
            # --- END CODE ---
            </code>
        </program>
    </subsection>

    <subsection xml:id="subsec-sum-product-rules">
    <title>Sum and Product Rules for Probability</title>
    <p>
        The sum and product rules are two cornerstones of probability theory. They allow us to compute probabilities of unions and intersections of events, which are essential in many machine learning applications, from estimating marginal distributions to building classifiers.
    </p>

    <p>
        <alert>Sum Rule</alert>: The probability of the union of two events <m>A</m> and <m>B</m> is given by:
        <men xml:id="eqn-sum-rule">P(A \cup B) = P(A) + P(B) - P(A \cap B).</men>
        The subtraction of <m>P(A \cap B)</m> avoids double-counting the overlap between the two events. This can be clearly understood using a Venn diagram.
    </p>

    <figure xml:id="fig-venn-sum-rule">
        <caption>Illustration of the sum rule. The probability of <m>A \cup B</m> is the sum of the shaded areas of <m>A</m> and <m>B</m>, minus the overlapping region <m>A \cap B</m> that would otherwise be counted twice.</caption>
        <image source="./images/essential-probability-and-statistics/sum-rule-venn.png">
        <shortdescription>Venn diagram showing the sum rule for two overlapping events.</shortdescription>
        </image>
    </figure>

    <p>
        <alert>Product Rule</alert>: The joint probability of two events <m>A</m> and <m>B</m> is:
        <men xml:id="eqn-product-rule">P(A \cap B) = P(A|B)P(B),</men>
        where <m>P(A|B)</m> is the conditional probability of <m>A</m> given <m>B</m>. This factorization is the basis for probabilistic models such as Naive Bayes.
    </p>

    <p>
        Example (Student Study Data): Suppose we record whether students studied more than 20 hours (<m>High Study</m>) and whether they passed an exam (<m>Pass</m>). Using the dataset below, we can estimate probabilities empirically:
        <ul>
        <li><m>P(\text{Pass})</m>: overall fraction of students who passed,</li>
        <li><m>P(\text{High Study})</m>: fraction who studied more than 20 hours,</li>
        <li><m>P(\text{Pass} \cap \text{High Study})</m>: fraction who both passed and studied more than 20 hours.</li>
        </ul>
        The sum rule gives <m>P(\text{Pass} \cup \text{High Study}) = P(\text{Pass}) + P(\text{High Study}) - P(\text{Pass} \cap \text{High Study})</m>.  
        The product rule verifies that <m>P(\text{Pass} \cap \text{High Study}) = P(\text{Pass}|\text{High Study})P(\text{High Study})</m>.
    </p>

    <program language="python" line-numbers="yes">
        <title>Sum and product rules with student data</title>
        <code>
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns

        np.random.seed(42)
        data = pd.DataFrame({
            "Hours_Studied": np.random.normal(20, 5, 200).clip(0, 40),
            "Passed": np.random.binomial(1, 0.7, 200)
        })
        data["High_Study"] = data["Hours_Studied"] > 20

        # Probabilities
        p_pass = data["Passed"].mean()
        p_high = data["High_Study"].mean()
        p_both = ((data["Passed"] == 1) \amp; (data["High_Study"] == True)).mean()
        p_union = p_pass + p_high - p_both
        p_conditional = p_both / p_high

        print(f"P(Pass) = {p_pass:.3f}")
        print(f"P(High Study) = {p_high:.3f}")
        print(f"P(Pass ∩ High Study) = {p_both:.3f}")
        print(f"P(Pass ∪ High Study) = {p_union:.3f}")
        print(f"P(Pass|High Study) * P(High Study) = {p_conditional:.3f} * {p_high:.3f} = {p_conditional * p_high:.3f}")

        # Visualize joint distribution
        joint_table = pd.crosstab(data["Passed"], data["High_Study"], normalize="all")
        sns.heatmap(joint_table, annot=True, cmap="Blues", fmt=".3f")
        plt.xlabel("High Study Hours (>20)")
        plt.ylabel("Pass (0=No, 1=Yes)")
        plt.title("Joint Probability of Pass and High Study Hours")
        plt.savefig("joint-probability-heatmap.png", dpi=300)
        plt.show()
        </code>
    </program>

    <figure xml:id="fig-joint-probability-heatmap">
        <caption>Joint probability table of passing and high study hours. Each cell represents <m>P(\text{Pass}, \text{High Study})</m>. Row and column sums recover marginals (<m>P(\text{Pass})</m> and <m>P(\text{High Study})</m>), illustrating the sum rule. The product rule is verified by comparing the joint probability with <m>P(\text{Pass}|\text{High Study})P(\text{High Study})</m>.</caption>
        <image source="./images/essential-probability-and-statistics/joint-probability-heatmap.png">
        <shortdescription>Heatmap of joint probabilities for pass and study hours.</shortdescription>
        </image>
    </figure>
    </subsection>



    <subsection xml:id="subsec-conditional-probability-independence">
        <title>Conditional Probability and Independence</title>

        <p>
            <alert>Conditional Probability</alert>: The probability of an event <m>A</m> given that event <m>B</m> has occurred is written as
            <men xml:id="eqn-conditional-prob-defn">
            P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \text{ provided that } P(B) \gt; 0
            </men>.  
            Intuitively, this represents <em>updating our belief about <m>A</m></em> once we know that <m>B</m> is true.  
            For instance, the probability that a student passes an exam may be different depending on whether we already know they studied more than 20 hours.
        </p>
        <example><title>Probability of King if the Card is known to be Spade</title>
            <p>
                Imagine we have a standard deck of 52 playing cards.  
                Event <m>A</m> = "the card is a King".  
                Event <m>B</m> = "the card is a Spade".  
            </p>
            <p>
                The unconditional probability of drawing a King is <m>P(A) = 4/52 = 1/13</m>.  
                But suppose we are told that the card drawn is a Spade. Now, the sample space is only 13 Spade cards.  
                Out of these, exactly one is a King (the King of Spades).  
            </p>
            <p>
                Thus the conditional probability is
                <m>P(A \mid B) = \tfrac{1}{13}</m>, which differs from <m>P(A)</m>.  
                This illustrates how new information (that the card is a Spade) changes the probability of <m>A</m>.
            </p>
        </example>

        <p>
            <alert>Independence</alert>: Two events <m>A</m> and <m>B</m> are independent if the occurrence of one does not affect the probability of the other.  
            Formally, the eventa <m>A</m> and <m>B</m> are independent if
            <men xml:id="eqn-independent-events-def">
                P(A \cap B) = P(A)P(B).\quad \Leftrightarrow\quad  A \text{ and } B \text{ independent}.
            </men>. 
            Equivalently, 
            <men xml:id="eqn-independent-events-def-2">
                P(A \mid B) = P(A)\text{ when } P(B) &gt; 0.\quad \Leftrightarrow\quad  A \text{ and } B \text{ independent}.
            </men>
            Independence means that knowing whether <m>B</m> occurred does not provide any new information about <m>A</m>.
        </p>

        <example><title>Tosses of a fair coin are independent</title>
            <p>
                Suppose we toss a fair coin twice. Let event <m>A</m> be "the first toss is Heads" and event <m>B</m> be "the second toss is Heads."  
                The sample space is <m>\{HH, HT, TH, TT\}</m>.
            </p>

            <p>
                We have <m>P(A) = 1/2</m>, <m>P(B) = 1/2</m>, and <m>P(A \cap B) = 1/4</m>.  
                Therefore, <m>P(A \cap B) = P(A)P(B)</m>, which means the events are independent.  
            </p>

            <p>
                Now let event <m>C</m> be "at least one toss is Heads."  
                Then <m>P(C) = 3/4</m>, <m>P(A \cap C) = 1/2</m>, and <m>P(A \mid C) = (1/2) / (3/4) = 2/3</m>.  
                But <m>P(A) = 1/2</m>, so <m>P(A \mid C) \neq P(A)</m>.  
                Thus, <m>A</m> and <m>C</m> are not independent.
            </p>
        </example>
    </subsection>
    
    <subsection xml:id="subsec-probability-distributions">
        <title>Probability Distributions</title>
        <p>
            Probability distributions describe how probabilities are distributed over outcomes. In machine learning, distributions model data or predictions.
        </p>
        <p>
            <alert>Bernoulli Distribution</alert>: Models a binary outcome (e.g., pass/fail) with probability <m>p</m>. For passing an exam, <m>P(\text{Pass}) = p</m>, <m>P(\text{Fail}) = 1-p</m>.
        </p>
        <p>
            <alert>Binomial Distribution</alert>: Counts successes in <m>n</m> independent Bernoulli trials. For 10 students, the number who pass follows a binomial distribution.
        </p>
        <program language="python" line-numbers="yes">
            <title>Binomial distribution for student passes</title>
            <code>
# --- BINOMIAL DISTRIBUTION ---
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

n, p = 10, 0.7  # 10 students, P(Pass) = 0.7
k = np.arange(0, 11)
pmf = binom.pmf(k, n, p)

plt.bar(k, pmf)
plt.xlabel('Number of Passes')
plt.ylabel('Probability')
plt.title('Binomial Distribution (n=10, p=0.7)')
plt.grid(True, alpha=0.3)
plt.savefig('./images/essential-probability-and-statistics/binomial-dist.png', dpi=300)
plt.show()
# --- END CODE ---
            </code>
        </program>
        <figure xml:id="fig-binomial-dist">
            <caption>Bar plot of the binomial probability mass function (PMF) for the number of students passing an exam out of 10, with a pass probability <m>p=0.7</m>. Each bar represents the probability of <m>k</m> students passing, calculated as <m>P(k) = \binom{n}{k} p^k (1-p)^{n-k}</m>. The peak around 7 passes reflects the high likelihood of most students passing given <m>p=0.7</m>. This distribution is critical in machine learning for modeling binary outcomes, such as predicting the number of successful predictions in a classification task.</caption>
            <image source="./images/essential-probability-and-statistics/binomial-dist.png">
                <shortdescription>Bar plot of binomial distribution.</shortdescription>
            </image>
        </figure>
    </subsection>
    <subsection xml:id="subsec-three-types-of-probabilities">
        <title>Three Types of Probabilities</title>
        <p>
            Probability can be approached theoretically, empirically (frequentist), or subjectively (Bayesian).
        </p>
        <ol>
            <li>
                <p>
                    <alert>Theoretical Probability</alert>: Uses symmetry. For a fair die, <m>P(\{1\}) = 1/6</m>. For even numbers, <m>P(\{2,4,6\}) = 3/6 = 0.5</m>.
                </p>
            </li>
            <li>
                <p>
                    <alert>Frequentist Probability</alert>: Estimates probability from trial frequencies: <men xml:id="eqn-frequentist-probability-definition">p = \lim_{N \to \infty} \frac{n}{N}</men>.
                </p>
                <program language="python" line-numbers="yes">
                    <title>Frequentist estimation for fair and biased dice</title>
                    <code>
                    # --- FREQUENTIST SIMULATION ---
                    import numpy as np
                    import matplotlib.pyplot as plt

                    np.random.seed(42)
                    n_trials = 1000
                    fair_rolls = np.random.randint(1, 7, n_trials)
                    biased_rolls = np.random.choice([1, 2, 3, 4, 5, 6], n_trials, 
                                                    p=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1])

                    # Cumulative probabilities
                    cum_fair = np.cumsum(fair_rolls == 1) / np.arange(1, n_trials + 1)
                    cum_biased = np.cumsum(biased_rolls == 1) / np.arange(1, n_trials + 1)

                    plt.plot(cum_fair, label='Fair Die (P=1/6)')
                    plt.plot(cum_biased, label='Biased Die (P=0.2)')
                    plt.axhline(1/6, color='red', linestyle='--', label='Theoretical P=1/6')
                    plt.xlabel('Trials')
                    plt.ylabel('Estimated P(1)')
                    plt.title('Frequentist Estimates: Fair vs. Biased Die')
                    plt.legend()
                    plt.grid(True, alpha=0.3)
                    plt.savefig('frequentist-convergence.png', dpi=300)
                    plt.show()
                    # --- END CODE ---
                    </code>
                </program>
                <figure xml:id="fig-frequentist-convergence">
                    <caption>Plot showing the convergence of frequentist probability estimates for rolling a 1 on a fair die (<m>P(1)=1/6 \approx 0.167</m>) and a biased die (<m>P(1)=0.2</m>) over 1,000 trials. The fair die’s estimate (blue) fluctuates but approaches 1/6 (red dashed line), while the biased die’s estimate (orange) converges to 0.2, reflecting the higher probability of rolling a 1. This visualization demonstrates how empirical frequencies approximate true probabilities in large samples, a technique used in machine learning to estimate probabilities from training data.</caption>
                    <image source="./images/essential-probability-and-statistics/frequentist-convergence.png">
                        <shortdescription>Convergence plot for frequentist estimates.</shortdescription>
                    </image>
                </figure>
            </li>
            <li>
                <p>
                    <alert>Bayesian Probability</alert>: Updates prior beliefs with data using Bayes’ theorem: <m>P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}</m>. For die face 1, use a Beta prior, updated to Beta(<m>\alpha + n_1, \beta + (N - n_1)</m>).
                </p>
                <p>
                    Example: Estimate <m>P(\text{Pass})</m> for students using the dataset, starting with a Beta(1,1) prior.
                </p>
                <program language="python" line-numbers="yes">
                    <title>Bayesian update for student pass probability</title>
                    <code>
                    # --- BAYESIAN UPDATE ---
                    import numpy as np
                    import matplotlib.pyplot as plt
                    from scipy.stats import beta
                    import pandas as pd

                    # Student data
                    np.random.seed(42)
                    data = pd.DataFrame({
                        'Passed': np.random.binomial(1, 0.7, 10)
                    })

                    # Prior: Beta(1,1)
                    a, b = 1, 1
                    n, n1 = len(data), data['Passed'].sum()
                    a_post, b_post = a + n1, b + n - n1

                    # Plot prior and posterior
                    x = np.linspace(0, 1, 1000)
                    plt.plot(x, beta.pdf(x, a, b), label='Prior Beta(1,1)', color='blue')
                    plt.plot(x, beta.pdf(x, a + 1, b + 1), label='After 1 Pass', color='orange')
                    plt.plot(x, beta.pdf(x, a_post, b_post), label=f'Posterior Beta({a_post},{b_post})', color='green')
                    plt.axvline(n1/n, color='red', linestyle='--', label='Frequentist Est.')
                    plt.xlabel('P(Pass)')
                    plt.ylabel('Density')
                    plt.title('Bayesian Update for P(Pass)')
                    plt.legend()
                    plt.grid(True, alpha=0.3)
                    plt.savefig('./images/essential-probability-and-statistics/bayesian-update.png', dpi=300)
                    plt.show()
                    # --- END CODE ---
                    </code>
                </program>
                <figure xml:id="fig-bayesian-update">

                    <caption>Plot showing the Bayesian update of the probability of a student passing an exam, starting with a uniform Beta(1,1) prior (blue). After observing one pass (orange) and 10 trials with 3 passes (green, posterior Beta(4,8)), the distribution shifts, with the posterior mean at 4/12 ≈ 0.333. The frequentist estimate (red dashed line, 3/10 = 0.3) is shown for comparison. This visualization illustrates how Bayesian methods incorporate prior beliefs and data to refine probability estimates, a technique used in machine learning for probabilistic models and uncertainty quantification.</caption>
                    
                    <image source="./images/essential-probability-and-statistics/bayesian-update.png">
                        <shortdescription>Bayesian update plot.</shortdescription>
                    </image>
                </figure>
            </li>
        </ol>
    </subsection>

    <conclusion>
        <p>
            Probability provides the foundation for modeling uncertainty in machine learning. Axioms define the rules, while theoretical, frequentist, and Bayesian approaches offer different perspectives. Conditional probability and distributions like binomial are key for models like Naive Bayes. Practice with datasets from <xref ref="sec-data-types-for-machine-learning"/> and libraries from <xref ref="sec-useful-descriptive-statistics-tools"/> to apply these concepts. Explore <url href="https://www.probabilitycourse.com/" visual="probabilitycourse.com">Probability Course</url> for further learning.
        </p>
    </conclusion>
</section>