<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-LLN-and-CLT">
    <title>Law of Large Numbers and Central Limit Theorem</title>
    <introduction>
        <p>
            The Law of Large Numbers (LLN) is often hand-waved as <q>averages go to the mean,</q> but there are really two distinct theorems (weak vs strong), different modes of convergence, and subtle conditions. Similary, the Central Limit Theorem (CLT) is one of the deepest ideas in probability, and it's often glossed over as <q>averages look normal.</q> 
        </p>  
        <p>  
            Let's take a slow, detailed walk through them. Although I do not intend our treatment here to be exhaustive, but there will be enough detail for you to get a fuller understanding of what they represent and how to use them.
        </p>
        <p>
            <alert>The Setup:</alert>
        </p>
        <p>
            The set up for both LLN and CLT is the same. Suppose you have a number of independent random variables <m>X_1, X_2, \cdots, X_n</m>, all with identical distribution functions <m>P(X)</m>, same for every <m>X_i</m>. Let <m>\mu</m> be the man and <m>\sigma^2</m> the variance of the common distribution.
            <mdn>
                <mrow xml:id="eqn-true-mean"> \amp \mu  = \langle X_i\rangle,\qquad \text{same for every }X_i </mrow>
                <mrow xml:id="eqn-true-var">\amp \sigma^2  = \langle X_i^2\rangle - \mu^2,\qquad \text{same for every }X_i </mrow>
            </mdn>
            We will call these values <alert>true mean</alert> and <alert>true variance</alert>.   
            Now, we introduce a new random variable, called the <alert>sample mean</alert> by 
            <men xml:id="eqn-sample-mean-random-variable">
                \bar{X}_n = \frac{1}{n}\,\left( X_1 + X_2 + \cdots + X_n  \right).
            </men>
            The bar over <m>X</m> just distinguishes it from the <m>n^\text{th}\, X</m> in the collection of random variables. More importantly, <m>\bar{X}_n</m> is not one number, but a random variable, which will take values in the range given by the components <m>X_i</m>. For instance, if <m>X_i \in \{ 0, 1 \}</m>, a Bernoulli random variable, then, <m>\bar{X}_n</m> would actually take many values, here  <m>\bar{X}_n \in \{ 0, 1/n, 2/n, \cdots, (n-1)/n, 1\}</m>.
        </p>  
        <p>
            The random variable <m>\bar{X}_n</m> will have it's own probability distribution, <m>P(\bar{X}_n)</m>. We call the mean and variance of this dstribution <alert>sample mean</alert> and <alert>sample variance</alert>.
            <men xml:id="eqn-">
                \bar{\mu}_n = \langle \bar{X}_n \rangle,\qquad \bar{\sigma}_n^2 =  \langle \bar{X}_n^2 \rangle - \bar{\mu}^2.
            </men>
            
        </p>

    </introduction>
    <subsection xml:id="subsec-Law-of-Large-Numbers">
        <title>Law of Large Numbers (LLN) </title>
        <p>
            The LLN addresses the question: <alert>What can we say about the mean of the sample mean <m>\bar{X}_n</m></alert> as <m>n</m> grows large? Clearly, the probability distribution of <m>\bar{X}_n</m> varies if we change <m>n</m>. 
        </p>
        <p>
            <alert>The weak law of LLN</alert> asserts that probablity that <m>|\bar{X}_n - \mu|</m>, where <m>\mu</m> is the true mean defined above in Eq. <xref ref="eqn-true-mean"/>, can be made smaller than any positive real number as long s you go sufficiently large <m>n</m>. Mathematically, we express it as the complement of this probility.
            <me>
                \lim_{n\rightarrow\infty}P(|\bar{X}_n - \mu| \gt \epsilon ) = 0,\qquad \text{for all } \epsilon\gt 0.
            </me>

            That is, the probability that <m>\bar{X}_n</m> strays outside of <m>\mu \pm \epsilon</m> goes to zero as <n>n \rightarrow \infty</n>. 
            This result is also expressed in a more compact notation, with a letter <m>P</m> above arrow implying <q>in probability</q>.
            <me>
                \bar{X}_n \xrightarrow{P}\, \mu.
            </me>
        </p>
        <p>
            <alert>The stron LLN</alert> uses a different criteria. You look at the sequency of sample random variables <m>\bar{X}_n</m> with increasing value of <m>n</m> and ask if these random variables themselves ted to <m>\mu</m>. The claim is
            <me>
                P(\lim_{n\rightarrow \infty} = \mu) = 1.
            </me>
            We say that <m>\bar{X}_n</m> <alert>almost surely</alert> converges to <m>\mu</m>, the true mean. 
        </p>
        <p>
            Although nt that hard, the proofs of these results are beyond these notes.
        </p>
        <p>
            For an <alert>intuitive feel</alert>, imagine tossing a fair coin. Suppose you toss it <m>n</m> times. For varying values of <m>n</m> you might find:
            <ul>
                <li>
                    <p>
                        <m>n=1</m>: <m>X_1</m> = 1 if heads and 0 if tails, with <m>\mu = \langle X_1 \rangle = 0.5</m>. <m>\bar{X}_1 = X_1</m>, hence, <m>\langle \bar{X}_1 \rangle = \mu</m>.
                    </p>
                </li>
                <li>
                    <p>
                        Suppose you flip the coin 10 times, now you have 10 variables, since outcome of each flip is a random variable in its own right. We usually, think of the tosses same experiment done 10 times. With this, <m>n = 10</m>. Suppose, you got <m>(1,1,0,0, 1, 1, 1, 1, 0, 1)</m>. That would mean, for this experiment, you got the following value of the sample mean, <m>\bar{X}_{10} = 0.7</m>.
                    </p>
                </li>
                <li>
                    <p>
                        f you flip 100 times, the value of the sample mean variable in any one set of 100 tosses will usually be close to <m>0.5</m>, say <m>0.501</m> 
                    </p>
                </li>
                <li>
                    <p>
                        Now, if you tossed the coin then <m>10,000</m>, you would get even closer to <m>o.5</m>. 
                    </p>
                </li>
            </ul>
            This is LLN in action. The strong law says: with probability 1, the sequence of averages converges to 0.5.
        </p>
 
        
    </subsection>
    <subsection xml:id="subsec-Central-Limit-Theorem">
        <title>Central Limit Theorem</title>
        <p>
            The Central Limit Theorem says that regardless of the distribution <m>P(X_i)</m> of <m>X_i</m>'s, which may be Bernouli, Binomial, Exponential, or whatever, the distribution of a <m>\bar{X}_n</m> tends towards a bell-shaped Gaussian wil larger and larger values of <m>n</m>. Specifically,
            <men xml:id="eqn-CLT">
                \lim_{n\rightarrow \infty}\, \bar{X}_n \xrightarrow{d} \, \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right),          
            </men>
            where <m>d</m> above arrow just indicates the result is to be understood as converging <q>in distribution</q> and <m>\sigma^2</m> is the true variance of the component <m>X_i</m> variables. In terms ofprobability this means 
            <me>
                P(x \le \bar{X}_n \le x+dx) = \frac{1}{\sqrt{2\pi\sigma^2}}\, \exp\left( - \frac{( x - \mu )^2}{2 \sigma^2/n}  \right)\, dx.
            </me>
        </p> 
        <p> 
            In applications of this formula, it is convenient to express it in another form by shifting the random variable <m>\bar{X}_n</m> and scaling the result.
            <men xml:id="eqn-the-Zn-variable">
                \bar{Z}_n = \frac{ \sqrt{n}\left( \bar{X}_n -\mu \right) }{\sigma}.
            </men>
            Then, you can show that <m>\bar{Z}_n</m> has a standard normal distribution, which lends itself to looking up standard tables and standard computations.
            <men xml:id="eqn-the-normalized-CLT">
                \bar{Z}_n \sim \mathcal{N}(0, 1).
            </men>
            the CDF of standard normal is denoted by <m>\Phi</m> instead of <m>F</m>.
            <me>
                \Phi(z) = P( \bar{Z}_n \le z ) = \int_{-\infty}^z\frac{1}{\sqrt{2\pi}}\, \exp\left( - \frac{z^2}{2}  \right)\, dz.
            </me>
        </p> 
        <p>     
            Let's emphasize once again the fact that the convergence to a Gaussian happens regarless of the original distribution of <m>X_i</m>'s. This is why we see Gaussian curves everywhere in nature â€” measurement errors, heights, test scores, etc. aren't inherently normal, but they are aggregates of many small random effects.
        </p>
        
    </subsection>
    <!-- <subsection xml:id="subsec-LLN-vs-CLT">
        <title>LLN vs CLT</title>
        <p>
            Let's summarize the basic features of the Law of Large Numbers and Central Limit Theorem.
            <dl>
                <li>
                    <title>Law of Large Numbers (LLN)</title>
                    <ul>
                        <li>
                            <p>
                                <m>\bar{X}_n \rightarrow \mu</m> as <m>n\rightarrow \infty</m> in the context of either in probability or almost surely.
                            </p>
                        </li>
                        <li>
                            <p>
                                Says the sample mean stabilizes around the true mean.
                            </p>
                        </li>
                    </ul>

                </li>
                <li>
                    <title>Central Limit Theorem (CLT)</title>
                    <ul>
                        <li>
                            <p>
                                Tells you how the deviations of <m>\bar{X}_n</m> from <m>\mu</m> are distributed.
                            </p>
                        </li>
                        <li>
                            <p>
                                Specifically, deviations are of order <m>1/\sqrt{n}</m>, and tend towards a Gaussian.
                            </p>
                        </li>
                    </ul>

                </li>
            </dl>
        So, LLN = consistency (where averages go) and CLT = fluctuations (how theyâ€™re spread around).
 

        </p>
        <p>
            <alert>How large is <m>n</m> when we say n large?</alert> A rule of thumb is that often <m>n \ge 30 </m> is enough. Why? <m>1/\sqrt{30} = 5.5</m> and when <m>\sigma = 1</m>, then probability of deviation from true <m>\mu</m> larger than this value is about <m>1\times 10^{-7}</m>, which can be safely ignored in most cases. There is actually a mathematical result that quantifies the rate of convergence. Berry-Esseen Theorem gives rates of convergence.
        </p>  
        <p>  
            Let <m>Y_1, Y_2, \cdots, Y_n</m> be i.i.d. (independent identically distributed) random variables as above. But, we now have subtracted away the true mean so that these have zero true mean. We state the conditions on <m>Y_1</m> since all others are same.
            <me>
                \langle Y_1 \rangle = 0.
            </me>
            Furthermore,
            <me>
                \text{Var}(Y_1) = \sigma^2 \gt 0,\quad \text{finite},
            </me>
            and third absolute moment is also finite.
            <me>
                \rho \equiv \langle Y_1^3 \rangle \lt \infty.
            </me>
            In place of sample mean random variable, let us define <alert>normalized sum </alert> random variable:
            <me>
                S_n = \frac{Y_1 + Y_2 + \cdots + Y_n}{ \sigma\,\sqrt{n} }.
            </me>
            Let <m>F_n</m> be the Cumulative Distribution Function (CDF) of the distribution of <m>S_n</m>, meaning
            <me>
                F_n(x) = P(S_n \le x),
            </me>
            and let us denote the CDF of standard normal, <m>\mathcal{N}(0,1)</m> by the Greek letter, <m>\Phi</m>. This is a standard notation. We keep <m>F</m> for other distributions. Then, Berry-Essen Theorem states that
            
            <men xml:id="eqn-Berry-Esseen-Theorem">
                \underset{x \in \R}{\sup}\, \left| F_n(x) - \Phi(x)  \right| \le \frac{C\, \rho}{ \sigma^3\,\sqrt{n}},
                
            </men>
            where <m>C</m> is a universal constant, independent of the distribution of the <m>Y_i</m>'s, so that the result holds for all i.i.d. variables satisfying the conditions above. Shevtsova (2011) showed <m>C \le 0.4748</m> and the best known lower bound is <m>C \ge 0.4097</m>. IExact value is not that important since in most applications <m>C \approx 1</m> is enough.
            
        </p>
        <p>
            The CLT says that as <m>n \rightarrow \infty</m>, <m>F_n \rightarrow \Phi</m>, and Berry-Esseen theorem tells us the rate at which this approach occurs. It says that the difference shrinks as <m>1/\sqrt{n}</m> with a "tail-heaviness" factor <m>\rho/\sigma^3</m>.
        </p>
        
    </subsection> -->
    <subsection xml:id="subsec-Why-Large-n-Matters">
        <title>Why LLN and CLT Matter?</title>
        <p>
            <alert>Why LLN Matter?</alert>
        </p>
        <p>
            In statistics, we distinguish between a sample mean and a population mean. The population mean, also called true mean <m>\mu</m>, is usually not known since we either do not have access to every point of interest, we only look at a small part. For instance, when we poll the population on some issue, we do not ask everybody, we just take <alert>sample</alert>. The LLN gives us confidence that if <m>n</m> is <alert>large enough</alert>, sample mean will be close to the population mean.
        </p>
        <p>
            As a numerical illustration of how sample mean <m>\bar{X}_n</m> <q>hugs</q> the true mean as <m>n</m> gets large, consider rolls of a fair six-sided die, whose faces are labeled <m>1, 2, 3, 4, 5, 6</m>. The true mean of these rolls will be just 
            <me>
                \text{True Mean}, \mu = 3.5,\qquad \text{Estimated Mean}, \bar{X}_n \approx 3.5.
            </me>
            Can you see how this can be shown? Anyway, let's roll this die <m>n</m> times and plot the sample means of the results obtained in rolls for various <m>n</m> values. This is shown in <xref ref="fig-die-rolls-sample-mean-vs-true-mean-x"/>. You can see that As the number of rolls increases, the sample mean stabilizes around <m>3.5</m>

            <figure xml:id="fig-die-rolls-sample-mean-vs-true-mean-x">
                <caption>Illustration of the Law of Large Numbers with rolls of a fair six-sided die.</caption>
                <image source="./images/essential-probability-and-statistics/die-rolls-sample-mean-vs-true-mean.png">
                    <shortdescription>Illustration of the Law of Large Numbers with rolls of a fair six-sided die.</shortdescription>
                </image>
            </figure>

            
        </p>
        <!--
        <p>
            In <alert>Supervised Machine Learning</alert> we want to find the function <m>f</m> of the independent variables, called <q>features</q>, that minimizes the expected loss <m>L</m> which is a function of <m>f</m> and independent variable, also called <q>target</q> <m>Y</m>.
            <me>
                R(f) = \langle \mathcal{L}(f(X), Y)  \rangle
            </me>
            Since, we do not know the true distribution, we can find the <q>true minimum</q>. The only thing we have are <m>(x,y)</m> pairs of values in each trial, i.e., each data point. Say you have <m>n</m> data points. So, we build the expectation on the right based on these <m>n</m> data points and get <q>an approximation</q> of <m>R(f)</m>, which we can designate as <m>R_n(f)</m>.
            <me>
                R_n(f) = \frac{1}{n} \sum_{i=1}^n\mathcal{L}(f(X_i), Y_i).
            </me>
            The LLN gives us confidence that
            <me>
                R_n(f) \rightarrow R(f)\quad \text{as long as }n \text{ is large enough}.
            </me>
            Hence, more the data, better the approximation!
            
            
        </p>
        -->
        <p>
            Maybe an example from Physics as well: In statistical mechanics, we often can't observe all particles in a macroscopic system. Instead, suppose we observe one particle over a long time. LLN guarantees that if the system is <alert>ergodic</alert>, the time average converges to the ensemble (population) average.
        </p>

        <p>
            <alert>Why CLT Matter? The Confidence Interval</alert>
        </p>
        <p>
            Although LLN tells us that in the large <m>n</m> limit sample mean will be close to the population or true mean <m>\mu</m>. But, we don't only want to know the approximate value of the true mean. We also would like to know the <alert>uncertainty</alert> in this knowledge, especially since we often do not know the value of true mean. This is where we <alert>need the Central Limit Theorem</alert>. The CLT says that <m>\bar{X}_n</m> is approximately Gaussian, which gives as tools to build <alert>confidence intervals</alert>.
        </p>
        
        <!-- <p>
            Suppose we have the sample mean <m>\bar{X}_n</m> that gives us and estimate of the true mean <m>\mu</m> by Law of Large Numbers as above. If we knew the true mean <m>\mu</m>, just the absolute difference or percentage difference would suffice to tell us how different is the estimate from the true value. But, if we knew the true mean, we wouldn't need the estimated mean.
        </p>   -->
        <!-- <p> 
            In the absence of any knowledge of the true mean, how do we make any conclusion about how good a estimate we have. That's where we can use Central Limit Theorem. According to the CLT, even though we do not know the true distribution of our original problem of random variables <m>X</m>, by repeatedly performing the experiment that measures <m>X</m> in an identical manner, say <m>n</m> times, we can claim that the mean of these <m>n</m> experimental outcomes has a Gaussian distribution whose mean and standard deviation are related to the mean and standard deviation of individual <m>X</m> themselves.
        </p> -->
        <p>
            Let us look at the fair six-sided die rolls. Suppose, want to answer the question: what is the mean of face value of a roll of a single die? The result of a signle roll is a random variable, which are calling <m>X</m>. Of course, each time you roll the die, you will get a single value from <m>1, 2, 3, 4, 5, \text{or}, 6</m> in a equiprobable way. From this you can calculate the true mean to be <m>\mu = 3.5</m>, and standard deviation <m>\sigma = \sqrt{35/12}</m>, which is left as an exercise for you. (Just use the mean and variance formulas.)
            <me>
                \mu = 3.5,\quad \sigma = \sqrt{35/12}.
            </me>
            Now, we are going to pretend that we didn't know these values and want to figure out the mean by doing experiments and generating data. 
        </p>
        <p>
            To get the estimates of mean and standard deviation, we can just roll the die <m>n</m> times, say for <m>n=100</m>. The outcome of this experiment will be a sequence of face values, <m>\{X_1 = 2, X_2 = 5, X_3 = 1, X_4 = 3, \cdots, X_{100}=4 \}</m>. The average, denoted by <m>\bar{X}_{n}</m> and standard deviation of this data can be an estimate of the corresponding true quantities. Since <m>n=100</m> is fairly large number, the law of large numbers tells us that the average will be a good estimate of <m>\mu</m>. 
        </p>
        <p>
            Suppose you got a value of <m>\bar{X}_{n} = 3.6</m> in our first try of a 100 rolls. Now, when you repeat another <m>100</m> rolls, you will get another set of data, <m>\{X_1 = 5, X_2 = 1, X_3 = 6, X_4 = 1, \cdots, X_{100}=2 \}</m>. Their average and standard deviation will be different from what you got the first time. Say, for average you got this time <m> \bar{X}_{n}= 3.55</m> this time. 
        </p>  
        <!-- <p>  
            Clearly, <m>\bar{X}_{n}</m> is a random variable. The good news is that Central Limit Theorem tells us that the distribution function of <m>\bar{X}_{n}</m> is a Gausian, whose mean and standard deviation are related to the true mean <m>\mu</m>,  true standard deviation <m>\sigma</m> of the individual rolls, say roll number <m>1</m>, whose random variable is <m>X_1</m> with all rolls being identical, and the number of rolls in one experiment, here <m>n=100</m>. For clarity, I will state the CLT result here a little differently making use the probability of values of the random variable <m>\bar{X}_n</m>.
            <me>
                P(x \le \bar{X}_n \le x+dx) = \frac{1}{\sqrt{2\pi\sigma^2}}\, \exp\left( - \frac{( x - \mu )^2}{2 \sigma^2/n}  \right)\, dx
            </me>
            Of course writing in such details is not necessary as long as we keep the prpbablity part in the back of our ming and instead, use the following notation, which means the same thing.  
            <men xml:id="eqn-clt-gaussian">
                \bar{X}_n \sim \mathcal{N}\left( \mu, \frac{\sigma^2}{n}\right).
            </men>
            OF course, we still do not know the true mean and true standard deviation. So, we can estomate them by repeating sets of <m>n</m> rolls, each set of 100 rolls, being one trial. 
        </p> -->
        <p>
            A simulation of <m>N=10000</m> repeats of this <m>100</m>-roll experimets is presented in <xref ref="fig-confidence-interval-die-roll"/> as a histogram. The theoretical true mean is near the peak of the Gaussian as expected. But, if we didn't know the value of the true mean, we will need to guess where it is. That guess can be quantified with a degree of confidence in the guess. The figure shows a range of values of the sample mean (the abscissa in the plot) within which there is <m>95\%</m> chance of finding the true <m>\mu</m>.
        </p>
        <p>
            Here is the breakdown of how the confidence interval was obtained. Let's recall that as per CLT for a large <m>n</m>.
            
            <!-- First we re-write the Gaussian result as a standard Gaussian by shifting and scaling the random variable to get a new random variable, which is to the left of the following equation.
            <men xml:id="eqn-clt-standard-gaussian">
                \frac{\sqrt{n}\left( \bar{X}_n -\mu \right)}{\sigma} \sim \mathcal{N}\left( 0, 1\right).
            </men>
            Since we do not know the true <m>\mu</m> and <m>\sigma</m>, we replace them with the one we can estimate from our rolls, which I will denote by <m>\mu_\text{est}</m> and <m>\sigma_\text{est}</m>.
            <men xml:id="eqn-clt-standard-gaussian-approx">
                \frac{\sqrt{n}\left( \bar{X}_n -\mu_\text{est} \right)}{\sigma_\text{est}} \sim \mathcal{N}\left( 0, 1\right).
            </men>           
            Or, we can equivalently do the following. -->
            <men xml:id="eqn-clt-gaussian-approx">
                \bar{X}_n  \sim  \mathcal{N}\left( \mu, \frac{ \sigma^2 }{n}\right),
            </men>
            where <m>\mu</m> and <m>\sigma</m> are the true mean and standard deviation of the random variabel <m>X_1</m>. The last quantity is sample variance
            <men xml:id="eqn-sample-variance">
                \text{Sample Variance} = \frac{ \sigma^2 }{n}.
            </men>
            This is often referred to as theoretical result. When we simulate the experiment as done in <xref ref="fig-confidence-interval-die-roll"/>, you can get estimated sample mean as well the estimated sample variance directly from the experiment and don't need to divide it further by <m>\sqrt{n}</m>. Thus, using the quatities from the simulation, we will write the CLT as follows.
            <men xml:id="eqn-clt-gaussian-approx-simulation">
                \bar{X}_n  \sim  \mathcal{N}\left( \mu_\text{sim}, \sigma_\text{sim}^2 \right),
            </men>
            where I am using the subscript <m>\text{sim}</m> to indicate the simulation quantities.
        </p>  
        <p>              
            
            Here, we want a <m>95\%</m> confidence interval (in other applications you may want <m>99\%</m> CI or whatever), which is just the range of values around the mean that covers the <m>95\%</m> of the area under the curve in the Gaussian function. That will leave <m>5\%</m> area outside, which will be <m>2.5\%</m> on the right and <m>2.5\%</m> on the left. That's where we can use the inverse of the CDF <m>F(x)</m>. If we solve
            <me>
                F(x) = 0.95 + 0.025 = 0.975, 
            </me>
            we will get the value of <m>x</m> that is the right edge. By symmetry, we will go to the left edge, or for the left edge solve
            <me>
                F(x) = 0.025.
            </me>
            These points will be
            <me>
                x_\text{right} = F^{-1}(0.975),\quad x_\text{left} = F^{-1}(0.025).
            </me>
            Altough plot in <xref ref="fig-confidence-interval-die-roll"/> does not show the normalized Gaussian, in practice it is common to shift and scale to the <m>Z</m> vartiable as defined above.
            <me>
                \bar{Z}_n \equiv \frac{\left( \bar{X}_n -\mu_\text{sim} \right)}{\sigma_\text{sim}} \sim \mathcal{N}\left( 0, 1\right).
            </me> 
            Then, it is easy to get the left and right values of <m>\bar{Z}_n</m> by using a standard library function, such as <code>ppf()</code> shown in the program listed below. This gives
            <me>
                 x_\text{right} =  \mu_\text{est} + z_\text{right} \times \sigma_\text{sim},
            </me>
            and 
            <me>
                 x_\text{left} =  \mu_\text{est} + z_\text{left} \times \sigma_\text{sim}.
            </me>
            The range <m>[x_\text{left}, x_\text{right}]</m> gives the range of <m>\bar{X}_n</m> values within which at the chosen confidence level, here illustrated for <m>95\%</m> that the true <m>\mu</m> is expected to reside.
            <men xml:id="eqn-confidence-interval">
                \mu \in [x_\text{left}, x_\text{right}] = [\mu_\text{est} + z_\text{right} \times \sigma_\text{sim},  \mu_\text{est} + z_\text{left} \times \sigma_\text{sim} ].
            </men>
                        
            
        </p>

        <!-- <p>
            Suppose you N=10000 repeats of n=100 rolls. In each roll you compute the average of the n face up values and the standard deviation of the face up values from this average. In each trial, you will get the values of sample mean. That will give us the distribution of values of smaple mean from all <m>N</m> trials. We can then compute mean of these sample means and their standard deviation or variance as we learned in basic statistics. Then, because of the claim of the CLI, we can assert that
            <me>
                \bar{X}_{100} \sim \mathcal{N}\left( \text{Mean of Sample Means},  \frac{ \text{Variance of Sample Means} }{N} \right).
            </me>
            Let us use the following symbols:
            <me>
                \text{Mean of Sample Means} = \bar{m},\qquad \text{Variance of Sample Means} = \bar{s}^2.
            </me>
            
            Where will the true mean be present at say <m>95%</m> interval? We will study the details in another section,but I want just state here the results that the following range will contain the true mean of <m>X</m>.
            <me>
                95\% \text{Confindene Interval (CL)} = \left[  \bar{m} - z_{0.975}\, \frac{\bar{s}^2}{N}, \bar{m} + z_{0.975}\, \frac{\bar{s}^2}{N} \right],
            </me>
            where <m>z_a</m> is the notation for the inverse of the standard normal CDF, i.e.,
            <me>
                z_{0.975} = \Phi^{-1}(0.975),
            </me>
            and <m>0.975</m> came from the <m>95\%</m> confidence for finding <m>\mu</m> meant <m>5\%</m> being outside, i.e, <m>2.5\%</m> on either side. Thus, it gave us probability value 
            <me>
                0.95 + 0.025 = 0.975,
            </me>
            for the argument of the inverse <m>\Phi^{-1}</m>, which gave us how far from the estimated mean we need to go on either side of the estimated mean as as to be able to find the true mean. The true mean, of course, usually not known in a real situation, although it is known here simply by the equal probability of each face.
             
        </p> -->
        <!-- <p>
            A simulation of tossing of die is presented in <xref ref="fig-confidence-interval-die-roll"/>. Runs 10,000 experiments of rolling 100 dice each. The simulation (1) Collects sample means from each trial of 100 tosses; (2) Uses those means to estimate the 95% CI; (3) Draws a double-headed arrow under the histogram showing the CI. The program in Python that does all of this is listed after the figure.
        </p> -->

        <figure xml:id="fig-confidence-interval-die-roll">
            <caption>Confidence interval for 10000 trials of the computation of the distribution of mean of face values in 100 rolls of a siz-sided fair die with marking 1, 2, 3, 4, 5, 6. The true mean is just <m>(1+ 2+ 3+ 4+ 5+ 6)/6 = 3.5</m>. </caption>
            <image source="./images/essential-probability-and-statistics/confidence-interval-die-roll.png">
                <shortdescription>Confidence interval for 10000 trials of the computation of the distribution of mean of face values in 100 rolls of a siz-sided fair die with marking 1, 2, 3, 4, 5, 6.</shortdescription>
            </image>
        </figure>
        <program language="python">
            <title>Program for simulating dice rolls and plotting the result</title>
            <code>
                import numpy as np
                import matplotlib.pyplot as plt
                from scipy.stats import norm

                # --- Step 1: Simulate dice rolls ---
                np.random.seed(42)
                n_trials = 10000   # number of experiments
                n_rolls = 100      # dice per experiment

                # Each experiment = 100 dice rolls
                data = np.random.randint(1, 7, size=(n_trials, n_rolls))
                sample_means = data.mean(axis=1)

                # --- Step 2: Compute 95% CI using sample statistics ---
                mean_est = np.mean(sample_means)
                std_est = np.std(sample_means, ddof=1)
                z_975 = norm.ppf(0.975)

                margin = z_975 * std_est
                ci_lower, ci_upper = mean_est - margin, mean_est + margin

                print(f"Estimated mean = {mean_est:.3f}")
                print(f"95% CI â‰ˆ [{ci_lower:.3f}, {ci_upper:.3f}]")

                # --- Step 3: Plot histogram + CI arrow ---
                fig, ax = plt.subplots(figsize=(8,5))
                counts, bins, patches = ax.hist(sample_means, bins=50, density=True, 
                                                alpha=0.6, color='skyblue', edgecolor='black')

                # Vertical line for mean
                plt.axvline(mean_est, color='red', linestyle='--', label="Mean of sample means")

                # Place CI arrow at 1/3 of max histogram height
                y_arrow = counts.max() / 3  

                # Horizontal CI arrow
                plt.annotate(
                    '', 
                    xy=(ci_lower, y_arrow), xycoords='data',
                    xytext=(ci_upper, y_arrow), textcoords='data',
                    arrowprops=dict(arrowstyle='&lt;-&gt;', color='black', lw=2)
                )
                plt.text(mean_est, y_arrow * 1.1, "95% CI", ha='center', fontsize=12)

                # --- New: Vertical dashed lines at CI endpoints ---
                plt.axvline(ci_lower, color='black', linestyle='dashed', ymax=0.9, label="CI lower/upper")
                plt.axvline(ci_upper, color='black', linestyle='dashed', ymax=0.9)

                plt.title("Sampling Distribution of Dice Means (100 rolls, 10,000 trials)")
                plt.xlabel("Sample Mean")
                plt.ylabel("Density")
                plt.legend()
                plt.show()        
            </code>
        </program>
    </subsection>

</section>