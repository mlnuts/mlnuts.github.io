<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-LLN-and-CLT">
    <title>Law of Large Numbers and Central Limit Theorem</title>
    <introduction>
        <p>
            The Law of Large Numbers (LLN) is often hand-waved as <q>averages go to the mean,</q> but there are really two distinct theorems (weak vs. strong), different modes of convergence, and subtle conditions. Similarly, the Central Limit Theorem (CLT) is one of the deepest ideas in probability, and it's often glossed over as <q>averages look normal.</q> 
        </p>  
        <p>  
            Let's take a slow, detailed walk through them. Although our treatment here is not exhaustive, there will be enough detail for you to gain a fuller understanding of what they represent and how to use them.
        </p>
        <p>
            <alert>The Setup:</alert>
        </p>
        <p>
            The setup for both LLN and CLT is the same. Suppose you have a sequence of independent random variables <m>X_1, X_2, \ldots, X_n</m>, all with identical distribution functions <m>P(X)</m>, the same for every <m>X_i</m>. Let <m>\mu</m> be the mean and <m>\sigma^2</m> the variance of the common distribution.
            <mdn>
                <mrow xml:id="eqn-true-mean"> \amp \mu = \langle X_i \rangle, \qquad \text{same for every } X_i </mrow>
                <mrow xml:id="eqn-true-var"> \amp \sigma^2 = \langle X_i^2 \rangle - \mu^2, \qquad \text{same for every } X_i </mrow>
            </mdn>
            We will call these values the <alert>true mean</alert> and <alert>true variance</alert>.   
            Now, we introduce a new random variable, called the <alert>sample mean</alert>, defined by 
            <men xml:id="eqn-sample-mean-random-variable">
                \bar{X}_n = \frac{1}{n} \left( X_1 + X_2 + \cdots + X_n \right).
            </men>
            The bar over <m>X</m> distinguishes it from the <m>n^\text{th}</m> <m>X</m> in the collection of random variables. More importantly, <m>\bar{X}_n</m> is not a single number but a random variable, which takes values in the range determined by the components <m>X_i</m>. For instance, if <m>X_i \in \{ 0, 1 \}</m>, a Bernoulli random variable, then <m>\bar{X}_n</m> would take values in <m>\{ 0, 1/n, 2/n, \ldots, (n-1)/n, 1 \}</m>.
        </p>  
        <p>
            The random variable <m>\bar{X}_n</m> has its own probability distribution, <m>P(\bar{X}_n)</m>. The mean and variance of this distribution are given by
            <mdn>
                <mrow xml:id="eqn-sample-mean-expectation"> \amp \langle \bar{X}_n \rangle = \mu, </mrow>
                <mrow xml:id="eqn-sample-variance"> \amp \Var(\bar{X}_n) = \frac{\sigma^2}{n}. </mrow>
            </mdn>
            Note that the sample mean <m>\bar{X}_n</m> is an unbiased estimator of the true mean <m>\mu</m>, and its variance decreases as <m>1/n</m>.
        </p>
    </introduction>
    <subsection xml:id="subsec-Law-of-Large-Numbers">
        <title>Law of Large Numbers (LLN)</title>
        <p>
            The LLN addresses the question: <alert>What can we say about the sample mean <m>\bar{X}_n</m></alert> as <m>n</m> grows large? Clearly, the probability distribution of <m>\bar{X}_n</m> varies as we change <m>n</m>. 
        </p>
        <p>
            <alert>The Weak Law of LLN</alert> asserts that the probability that <m>|\bar{X}_n - \mu|</m>, where <m>\mu</m> is the true mean defined in Eq. <xref ref="eqn-true-mean"/>, exceeds any positive real number <m>\epsilon</m> can be made arbitrarily small as <m>n</m> becomes sufficiently large. Mathematically, we express this as
            <me>
                \lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0, \qquad \text{for all } \epsilon > 0.
            </me>
            That is, the probability that <m>\bar{X}_n</m> strays outside of <m>\mu \pm \epsilon</m> goes to zero as <m>n \to \infty</m>. 
            This result is also expressed in compact notation, with a superscript <m>P</m> implying <q>convergence in probability</q>:
            <me>
                \bar{X}_n \xrightarrow{P} \mu.
            </me>
        </p>
        <p>
            <alert>The Strong Law of LLN</alert> uses a different criterion. It examines the sequence of sample mean random variables <m>\bar{X}_n</m> as <m>n</m> increases and asks whether the sequence itself converges to <m>\mu</m>. The claim is
            <me>
                P\left( \lim_{n \to \infty} \bar{X}_n = \mu \right) = 1.
            </me>
            We say that <m>\bar{X}_n</m> converges <alert>almost surely</alert> to the true mean <m>\mu</m>. 
        </p>
        <p>
            Although not overly complex, the proofs of these results are beyond the scope of these notes.
        </p>
        <p>
            For an <alert>intuitive feel</alert>, imagine tossing a fair coin. Suppose you toss it <m>n</m> times. For varying values of <m>n</m>, you might observe:
            <ul>
                <li>
                    <p>
                        <m>n=1</m>: <m>X_1 = 1</m> if heads and <m>0</m> if tails, with <m>\mu = \langle X_1 \rangle = 0.5</m>. Here, <m>\bar{X}_1 = X_1</m>, so <m>\langle \bar{X}_1 \rangle = \mu</m>.
                    </p>
                </li>
                <li>
                    <p>
                        For <m>n=10</m>, you have 10 random variables, as each flip’s outcome is a random variable. We often think of these as the same experiment repeated 10 times. Suppose you get <m>(1,1,0,0,1,1,1,1,0,1)</m>. The sample mean is <m>\bar{X}_{10} = 0.7</m>.
                    </p>
                </li>
                <li>
                    <p>
                        For <m>n=100</m>, the sample mean in any one set of 100 tosses will usually be close to <m>0.5</m>, say <m>0.501</m>. 
                    </p>
                </li>
                <li>
                    <p>
                        For <m>n=10,000</m>, the sample mean gets even closer to <m>0.5</m>. 
                    </p>
                </li>
            </ul>
            This is the LLN in action. The strong law states that, with probability 1, the sequence of averages converges to 0.5.
        </p>
        <p>
            To make this more concrete, consider a Poisson distribution, which models events like the number of emails received per hour. Suppose <m>X_i \sim \Poisson(\lambda=5)</m>, so <m>\mu = 5</m>, <m>\sigma^2 = 5</m>. If you simulate sample means for increasing <m>n</m>, you’ll see <m>\bar{X}_n</m> approaching 5. For small <m>n</m> (e.g., 10), the sample mean might vary between 3 and 7; for <m>n=1000</m>, it hugs tightly around 5. This illustrates the LLN beyond simple Bernoulli trials.
        </p>
    </subsection>
    <subsection xml:id="subsec-Central-Limit-Theorem">
        <title>Central Limit Theorem</title>
        <p>
            The Central Limit Theorem states that, regardless of the distribution <m>P(X_i)</m> of the <m>X_i</m>’s (e.g., Bernoulli, Binomial, Exponential), the distribution of <m>\bar{X}_n</m> tends toward a bell-shaped Gaussian as <m>n</m> becomes large, provided the <m>X_i</m>’s have finite mean and variance. Specifically,
            <men xml:id="eqn-CLT">
                \lim_{n \to \infty} \bar{X}_n \xrightarrow{d} \mathcal{N}\left( \mu, \frac{\sigma^2}{n} \right),          
            </men>
            where the superscript <m>d</m> indicates convergence <q>in distribution</q>, and <m>\sigma^2</m> is the true variance of the <m>X_i</m> variables. In terms of probability, this means 
            <me>
                P(x \le \bar{X}_n \le x+dx) = \frac{1}{\sqrt{2\pi (\sigma^2/n)}} \exp\left( - \frac{(x - \mu)^2}{2 (\sigma^2/n)} \right) dx.
            </me>
        </p> 
        <p> 
            In applications, it’s often convenient to standardize <m>\bar{X}_n</m> by shifting and scaling:
            <men xml:id="eqn-the-Zn-variable">
                Z_n = \frac{\sqrt{n} (\bar{X}_n - \mu)}{\sigma}.
            </men>
            Then, <m>Z_n</m> follows a standard normal distribution, which simplifies computations using standard tables:
            <men xml:id="eqn-the-normalized-CLT">
                Z_n \sim \mathcal{N}(0, 1).
            </men>
            The cumulative distribution function (CDF) of the standard normal is denoted by <m>\Phi</m>:
            <me>
                \Phi(z) = P( Z_n \le z ) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{z^2}{2} \right) dz.
            </me>
        </p> 
        <p>     
            The convergence to a Gaussian occurs regardless of the original distribution of the <m>X_i</m>’s, as long as they have finite mean and variance. This explains why Gaussian curves appear in nature—measurement errors, heights, test scores, etc., aren’t inherently normal but are aggregates of many small random effects.
        </p>
        <p>
            For a clarifying example, consider an exponential distribution, which is skewed and models waiting times (e.g., time between earthquakes). Let <m>X_i \sim \Exp(\lambda=1)</m>, so <m>\mu=1</m>, <m>\sigma^2=1</m>. For small <m>n</m> (e.g., 5), the distribution of <m>\bar{X}_n</m> is still skewed. For <m>n=30</m>, it starts looking bell-shaped, and by <m>n=100</m>, it’s very close to Gaussian. This demonstrates the CLT’s power on non-symmetric distributions, unlike the symmetric die or coin examples.
        </p>
    </subsection>
    <subsection xml:id="subsec-LLN-vs-CLT">
        <title>LLN vs. CLT</title>
        <p>
            Let’s summarize the key features of the Law of Large Numbers and Central Limit Theorem:
            <dl>
                <description xml:id="dl-lln">
                    <title>Law of Large Numbers (LLN)</title>
                    <ul>
                        <li>
                            <p>
                                <m>\bar{X}_n \to \mu</m> as <m>n \to \infty</m>, either in probability (weak LLN) or almost surely (strong LLN).
                            </p>
                        </li>
                        <li>
                            <p>
                                States that the sample mean stabilizes around the true mean.
                            </p>
                        </li>
                    </ul>
                </description>
                <description xml:id="dl-clt">
                    <title>Central Limit Theorem (CLT)</title>
                    <ul>
                        <li>
                            <p>
                                Describes how the deviations of <m>\bar{X}_n</m> from <m>\mu</m> are distributed.
                            </p>
                        </li>
                        <li>
                            <p>
                                Specifically, deviations are of order <m>1/\sqrt{n}</m> and tend toward a Gaussian distribution.
                            </p>
                        </li>
                    </ul>
                </description>
            </dl>
            In short, LLN ensures consistency (where averages go), while CLT describes fluctuations (how they’re spread around).
        </p>
        <p>
            <alert>How large is large <m>n</m>?</alert> A rule of thumb is that <m>n \ge 30</m> is often sufficient for the CLT to provide a good approximation. Why? For <m>n=30</m>, <m>1/\sqrt{30} \approx 0.183</m>, and for <m>\sigma=1</m>, the probability of a deviation larger than three standard errors (<m>3 \cdot \sigma/\sqrt{n}</m>) is approximately <m>0.0027</m>, which is negligible in many applications. The Berry-Esseen Theorem quantifies this convergence rate more precisely.
        </p>  
        <p>  
            Let <m>Y_1, Y_2, \ldots, Y_n</m> be i.i.d. random variables with zero mean (i.e., <m>\langle Y_1 \rangle = 0</m>), finite variance <m>\Var(Y_1) = \sigma^2 > 0</m>, and finite third absolute moment <m>\rho = \langle |Y_1|^3 \rangle < \infty</m>. Define the <alert>normalized sum</alert> random variable:
            <me>
                S_n = \frac{Y_1 + Y_2 + \cdots + Y_n}{\sigma \sqrt{n}}.
            </me>
            Let <m>F_n</m> be the CDF of <m>S_n</m>, i.e., <m>F_n(x) = P(S_n \le x)</m>, and let <m>\Phi</m> denote the CDF of the standard normal <m>\mathcal{N}(0,1)</m>. The Berry-Esseen Theorem states:
            <men xml:id="eqn-Berry-Esseen-Theorem">
                \sup_{x \in \mathbb{R}} \left| F_n(x) - \Phi(x) \right| \le \frac{C \rho}{\sigma^3 \sqrt{n}},
            </men>
            where <m>C</m> is a universal constant, independent of the distribution of the <m>Y_i</m>’s. Shevtsova (2011) showed <m>C \le 0.4748</m>, with a lower bound of <m>C \ge 0.4097</m>. In practice, <m>C \approx 1</m> is often sufficient.
        </p>
        <p>
            The CLT states that as <m>n \to \infty</m>, <m>F_n \to \Phi</m>, and the Berry-Esseen Theorem quantifies the rate of this convergence, showing the error shrinks as <m>1/\sqrt{n}</m>, modulated by the tail-heaviness factor <m>\rho/\sigma^3</m>.
        </p>
    </subsection>
    <subsection xml:id="subsec-Why-Large-n-Matters">
        <title>Why LLN and CLT Matter</title>
        <p>
            <alert>Why Does LLN Matter?</alert>
        </p>
        <p>
            In statistics, we distinguish between the sample mean and the population mean. The population mean, or true mean <m>\mu</m>, is usually unknown since we typically only have access to a sample, not the entire population. For instance, in a poll, we don’t ask everyone, just a <alert>sample</alert>. The LLN assures us that if <m>n</m> is <alert>large enough</alert>, the sample mean will be close to the population mean.
        </p>
        <p>
            As a numerical illustration, consider rolls of a fair six-sided die with faces labeled <m>1, 2, 3, 4, 5, 6</m>. The true mean is
            <me>
                \mu = \frac{1+2+3+4+5+6}{6} = 3.5,
            </me>
            and the estimated mean is <m>\bar{X}_n \approx 3.5</m>. Roll the die <m>n</m> times and compute the sample mean for various <m>n</m>. This is shown in <xref ref="fig-die-rolls-sample-mean-vs-true-mean"/>. As <m>n</m> increases, the sample mean stabilizes around <m>3.5</m>.
            <figure xml:id="fig-die-rolls-sample-mean-vs-true-mean">
                <caption>Illustration of the Law of Large Numbers with rolls of a fair six-sided die.</caption>
                <image source="./images/essential-probability-and-statistics/die-rolls-sample-mean-vs-true-mean.png">
                    <shortdescription>Illustration of the Law of Large Numbers with rolls of a fair six-sided die.</shortdescription>
                </image>
            </figure>
        </p>
        <p>
            In <alert>Supervised Machine Learning</alert>, we aim to find a function <m>f</m> of the independent variables (features) that minimizes the expected loss <m>L</m>, which depends on <m>f</m> and the dependent variable (target) <m>Y</m>:
            <me>
                R(f) = \langle \mathcal{L}(f(X), Y) \rangle.
            </me>
            Since the true distribution is unknown, we approximate <m>R(f)</m> using <m>n</m> data points <m>(x_i, y_i)</m>:
            <me>
                R_n(f) = \frac{1}{n} \sum_{i=1}^n \mathcal{L}(f(X_i), Y_i).
            </me>
            The LLN ensures that <m>R_n(f) \to R(f)</m> as <m>n</m> becomes large, so more data improves the approximation.
        </p>
        <p>
            In physics, consider statistical mechanics, where we can’t observe all particles in a macroscopic system. If we observe one particle over a long time in an <alert>ergodic</alert> system, the LLN guarantees that the time average converges to the ensemble (population) average.
        </p>
        <p>
            <alert>Why Does CLT Matter? The Confidence Interval</alert>
        </p>
        <p>
            The LLN assures us that the sample mean approaches the true mean <m>\mu</m> as <m>n</m> grows, but we also want to quantify the <alert>uncertainty</alert> in this estimate, especially since <m>\mu</m> is often unknown. The CLT provides tools to build <alert>confidence intervals</alert> by stating that <m>\bar{X}_n</m> is approximately Gaussian.
        </p>
        <p>
            Consider the fair six-sided die again. The result of a single roll is a random variable <m>X</m> taking values <m>1, 2, 3, 4, 5, \text{or } 6</m> with equal probability. The true mean is <m>\mu = 3.5</m>, and the true standard deviation is <m>\sigma = \sqrt{35/12} \approx 1.708</m>. To estimate these, roll the die <m>n=100</m> times, obtaining a sequence like <m>\{X_1 = 2, X_2 = 5, X_3 = 1, \ldots, X_{100}=4 \}</m>. The sample mean <m>\bar{X}_n</m> and sample standard deviation provide estimates of <m>\mu</m> and <m>\sigma</m>. Since <m>n=100</m> is large, the LLN suggests <m>\bar{X}_n</m> is close to <m>\mu</m>.
        </p>
        <p>
            Suppose one set of 100 rolls yields <m>\bar{X}_n = 3.6</m>. Another set might give <m>\bar{X}_n = 3.55</m>. The CLT tells us that the distribution of <m>\bar{X}_n</m> is approximately
            <men xml:id="eqn-clt-gaussian-approx">
                \bar{X}_n \sim \mathcal{N}\left( \mu, \frac{\sigma^2}{n} \right),
            </men>
            where <m>\sigma^2/n</m> is the variance of <m>\bar{X}_n</m> (the standard error squared).
        </p>
        <p>
            A simulation of <m>N=10,000</m> sets of 100-roll experiments is shown in <xref ref="fig-confidence-interval-die-roll"/> as a histogram. The true mean lies near the peak, but if <m>\mu</m> were unknown, we’d estimate it and quantify uncertainty. The figure shows the <m>95\%</m> confidence interval for <m>\mu</m>.
            <figure xml:id="fig-confidence-interval-die-roll">
                <caption>Confidence interval for 10,000 trials computing the mean of face values in 100 rolls of a six-sided fair die with markings 1, 2, 3, 4, 5, 6. The true mean is <m>(1+2+3+4+5+6)/6 = 3.5</m>.</caption>
                <image source="./images/essential-probability-and-statistics/confidence-interval-die-roll.png">
                    <shortdescription>Confidence interval for 10,000 trials of the mean of face values in 100 rolls of a six-sided fair die.</shortdescription>
                </image>
            </figure>
        </p>
        <p>
            To compute the <m>95\%</m> confidence interval, note that the CLT gives
            <men xml:id="eqn-clt-standard-gaussian">
                Z_n = \frac{\sqrt{n} (\bar{X}_n - \mu)}{\sigma} \sim \mathcal{N}(0, 1).
            </men>
            For a <m>95\%</m> confidence interval, we want the range covering <m>95\%</m> of the area under the standard normal curve, leaving <m>2.5\%</m> in each tail. Using the standard normal CDF <m>\Phi</m>, we solve
            <me>
                \Phi(z) = 0.975,
            </me>
            yielding <m>z \approx 1.96</m>. By symmetry, the left tail is at <m>z \approx -1.96</m>. In practice, we estimate <m>\mu</m> and <m>\sigma</m> from the sample, so
            <me>
                x_\text{left} = \bar{X}_n - 1.96 \cdot \frac{s}{\sqrt{n}}, \quad x_\text{right} = \bar{X}_n + 1.96 \cdot \frac{s}{\sqrt{n}},
            </me>
            where <m>s = \sqrt{\frac{1}{n-1} \sum (X_i - \bar{X}_n)^2}</m> is the sample standard deviation. The interval <m>[x_\text{left}, x_\text{right}]</m> is the <m>95\%</m> confidence interval for <m>\mu</m>.
        </p>
        <p>
            The following Python program simulates the die rolls and computes the confidence interval:
            <program language="python">
                <title>Program for simulating dice rolls and plotting the confidence interval</title>
                <code>
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Simulate dice rolls
np.random.seed(42)
n_trials = 10000  # Number of experiments
n_rolls = 100     # Dice per experiment

# Each experiment = 100 dice rolls
data = np.random.randint(1, 7, size=(n_trials, n_rolls))
sample_means = data.mean(axis=1)

# Compute 95% CI using sample statistics
mean_est = np.mean(sample_means)
std_est = np.std(sample_means, ddof=1)
z_975 = norm.ppf(0.975)

margin = z_975 * std_est
ci_lower, ci_upper = mean_est - margin, mean_est + margin

print(f"Estimated mean = {mean_est:.3f}")
print(f"95% CI ≈ [{ci_lower:.3f}, {ci_upper:.3f}]")

# Plot histogram + CI arrow
fig, ax = plt.subplots(figsize=(8,5))
counts, bins, patches = ax.hist(sample_means, bins=50, density=True, 
                                alpha=0.6, color='skyblue', edgecolor='black')

# Vertical line for mean
plt.axvline(mean_est, color='red', linestyle='--', label="Mean of sample means")

# Place CI arrow at 1/3 of max histogram height
y_arrow = counts.max() / 3  

# Horizontal CI arrow
plt.annotate(
    '', 
    xy=(ci_lower, y_arrow), xycoords='data',
    xytext=(ci_upper, y_arrow), textcoords='data',
    arrowprops=dict(arrowstyle='&lt;-&gt;', color='black', lw=2)
)
plt.text(mean_est, y_arrow * 1.1, "95% CI", ha='center', fontsize=12)

# Vertical dashed lines at CI endpoints
plt.axvline(ci_lower, color='black', linestyle='dashed', ymax=0.9, label="CI lower/upper")
plt.axvline(ci_upper, color='black', linestyle='dashed', ymax=0.9)

plt.title("Sampling Distribution of Dice Means (100 rolls, 10,000 trials)")
plt.xlabel("Sample Mean")
plt.ylabel("Density")
plt.legend()
plt.show()
                </code>
            </program>
        </p>
        <p>
            To illustrate the CLT with a skewed distribution, consider exponential random variables with <m>X_i \sim \Exp(\lambda=1)</m>, so <m>\mu=1</m>, <m>\sigma=1</m>. The following program simulates sample means for different <m>n</m> and plots histograms to show convergence to a normal distribution:
            <program language="python">
                <title>Program for simulating CLT with exponential distribution</title>
                <code>
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

np.random.seed(42)
lambda_param = 1.0  # Rate for exponential
mu = 1 / lambda_param
sigma = 1 / lambda_param
n_trials = 10000
ns = [1, 5, 30, 100]

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.ravel()

for i, n in enumerate(ns):
    # Generate sample means
    samples = np.random.exponential(1/lambda_param, size=(n_trials, n))
    sample_means = samples.mean(axis=1)
    
    # Normalize for standard normal comparison
    normalized = np.sqrt(n) * (sample_means - mu) / sigma
    
    # Plot histogram
    axes[i].hist(normalized, bins=50, density=True, alpha=0.6, color='green', edgecolor='black')
    
    # Overlay standard normal
    x = np.linspace(-4, 4, 1000)
    axes[i].plot(x, norm.pdf(x), 'r--', label='Standard Normal')
    
    axes[i].set_title(f'n = {n}')
    axes[i].set_xlabel('Normalized Sample Mean')
    axes[i].set_ylabel('Density')
    axes[i].legend()

plt.tight_layout()
plt.show()
                </code>
            </program>
            <figure xml:id="fig-clt-exponential">
                <caption>Illustration of CLT with exponential distribution: Histograms of normalized sample means for <m>n=1,5,30,100</m>.</caption>
                <image source="./images/essential-probability-and-statistics/clt-exponential.png">
                    <shortdescription>Histograms showing CLT convergence for exponential samples.</shortdescription>
                </image>
            </figure>
        </p>
    </subsection>
</section>